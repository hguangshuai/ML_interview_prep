#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾ç‰‡ç”Ÿæˆå·¥å…· - ä¸ºmarkdownæ–‡ä»¶ç”Ÿæˆç¤ºä¾‹å›¾ç‰‡
"""

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from matplotlib.patches import Rectangle
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def create_bias_variance_diagram():
    """åˆ›å»ºåå·®-æ–¹å·®æƒè¡¡å›¾"""
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # æ¨¡å‹å¤æ‚åº¦
    complexity = np.linspace(0, 10, 100)
    
    # åå·®ï¼ˆéšå¤æ‚åº¦å¢åŠ è€Œå‡å°‘ï¼‰
    bias_squared = 2 * np.exp(-complexity/3) + 0.1
    
    # æ–¹å·®ï¼ˆéšå¤æ‚åº¦å¢åŠ è€Œå¢åŠ ï¼‰
    variance = 0.1 * complexity + 0.05
    
    # æ€»è¯¯å·®
    total_error = bias_squared + variance + 0.05
    
    ax.plot(complexity, bias_squared, 'r-', linewidth=2, label='åå·®Â²')
    ax.plot(complexity, variance, 'b-', linewidth=2, label='æ–¹å·®')
    ax.plot(complexity, total_error, 'g-', linewidth=3, label='æ€»è¯¯å·®')
    
    # æ‰¾åˆ°æœ€ä¼˜ç‚¹
    optimal_idx = np.argmin(total_error)
    optimal_complexity = complexity[optimal_idx]
    optimal_error = total_error[optimal_idx]
    
    ax.axvline(optimal_complexity, color='orange', linestyle='--', alpha=0.7)
    ax.plot(optimal_complexity, optimal_error, 'ro', markersize=10)
    
    ax.set_xlabel('æ¨¡å‹å¤æ‚åº¦')
    ax.set_ylabel('è¯¯å·®')
    ax.set_title('åå·®-æ–¹å·®æƒè¡¡')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # æ·»åŠ åŒºåŸŸæ ‡æ³¨
    ax.text(2, 1.5, 'æ¬ æ‹ŸåˆåŒºåŸŸ\n(é«˜åå·®ï¼Œä½æ–¹å·®)', fontsize=10, 
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))
    ax.text(7, 1.5, 'è¿‡æ‹ŸåˆåŒºåŸŸ\n(ä½åå·®ï¼Œé«˜æ–¹å·®)', fontsize=10,
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral", alpha=0.7))
    
    plt.tight_layout()
    return fig

def create_overfitting_diagram():
    """åˆ›å»ºè¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆå›¾"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # è®­ç»ƒæ•°æ®
    np.random.seed(42)
    x_train = np.linspace(0, 10, 20)
    y_train = 0.5 * x_train + 2 + np.random.normal(0, 1, 20)
    
    # æµ‹è¯•æ•°æ®
    x_test = np.linspace(0, 10, 100)
    y_test = 0.5 * x_test + 2
    
    # æ¬ æ‹Ÿåˆæ¨¡å‹ï¼ˆçº¿æ€§ï¼‰
    ax1.scatter(x_train, y_train, color='blue', alpha=0.7, label='è®­ç»ƒæ•°æ®')
    ax1.plot(x_test, y_test, 'g-', linewidth=2, label='çœŸå®å‡½æ•°')
    
    # ç®€å•çš„çº¿æ€§æ‹Ÿåˆ
    poly_coeffs = np.polyfit(x_train, y_train, 1)
    y_pred_simple = np.polyval(poly_coeffs, x_test)
    ax1.plot(x_test, y_pred_simple, 'r--', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
    
    ax1.set_title('æ¬ æ‹Ÿåˆ (Underfitting)')
    ax1.set_xlabel('ç‰¹å¾')
    ax1.set_ylabel('ç›®æ ‡å€¼')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # è¿‡æ‹Ÿåˆæ¨¡å‹ï¼ˆé«˜æ¬¡å¤šé¡¹å¼ï¼‰
    ax2.scatter(x_train, y_train, color='blue', alpha=0.7, label='è®­ç»ƒæ•°æ®')
    ax2.plot(x_test, y_test, 'g-', linewidth=2, label='çœŸå®å‡½æ•°')
    
    # é«˜æ¬¡å¤šé¡¹å¼æ‹Ÿåˆ
    poly_coeffs_complex = np.polyfit(x_train, y_train, 15)
    y_pred_complex = np.polyval(poly_coeffs_complex, x_test)
    ax2.plot(x_test, y_pred_complex, 'r--', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
    
    ax2.set_title('è¿‡æ‹Ÿåˆ (Overfitting)')
    ax2.set_xlabel('ç‰¹å¾')
    ax2.set_ylabel('ç›®æ ‡å€¼')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def create_bayes_theorem_diagram():
    """åˆ›å»ºè´å¶æ–¯å®šç†å¯è§†åŒ–å›¾"""
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # åˆ›å»ºéŸ¦æ©å›¾é£æ ¼çš„å›¾
    circle1 = plt.Circle((0.3, 0.5), 0.25, color='lightblue', alpha=0.7)
    circle2 = plt.Circle((0.7, 0.5), 0.25, color='lightcoral', alpha=0.7)
    
    ax.add_patch(circle1)
    ax.add_patch(circle2)
    
    # æ·»åŠ æ ‡ç­¾
    ax.text(0.3, 0.5, 'P(Y)', fontsize=16, ha='center', va='center', weight='bold')
    ax.text(0.7, 0.5, 'P(X)', fontsize=16, ha='center', va='center', weight='bold')
    ax.text(0.5, 0.5, 'P(X,Y)', fontsize=14, ha='center', va='center', weight='bold')
    
    # æ·»åŠ å…¬å¼
    ax.text(0.5, 0.2, 'P(Y|X) = P(X|Y) Ã— P(Y) / P(X)', 
            fontsize=14, ha='center', va='center',
            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))
    
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.set_aspect('equal')
    ax.axis('off')
    ax.set_title('è´å¶æ–¯å®šç†å¯è§†åŒ–', fontsize=16, weight='bold')
    
    return fig

def create_generative_vs_discriminative_diagram():
    """åˆ›å»ºç”Ÿæˆå¼vsåˆ¤åˆ«å¼æ¨¡å‹å¯¹æ¯”å›¾"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
    
    # ç”Ÿæˆå¼æ¨¡å‹
    ax1.set_title('ç”Ÿæˆå¼æ¨¡å‹ (Generative)', fontsize=14, weight='bold')
    ax1.text(0.5, 0.8, 'å­¦ä¹  P(X,Y)', fontsize=12, ha='center', va='center')
    ax1.text(0.5, 0.6, 'è´å¶æ–¯å®šç†', fontsize=12, ha='center', va='center')
    ax1.text(0.5, 0.4, 'P(Y|X) = P(X|Y) Ã— P(Y) / P(X)', fontsize=10, ha='center', va='center')
    ax1.text(0.5, 0.2, 'å¯ä»¥ç”Ÿæˆæ–°æ ·æœ¬', fontsize=12, ha='center', va='center')
    
    # æ·»åŠ ç®­å¤´
    ax1.arrow(0.2, 0.7, 0.1, 0, head_width=0.02, head_length=0.02, fc='blue', ec='blue')
    ax1.arrow(0.8, 0.7, -0.1, 0, head_width=0.02, head_length=0.02, fc='blue', ec='blue')
    
    ax1.set_xlim(0, 1)
    ax1.set_ylim(0, 1)
    ax1.axis('off')
    
    # åˆ¤åˆ«å¼æ¨¡å‹
    ax2.set_title('åˆ¤åˆ«å¼æ¨¡å‹ (Discriminative)', fontsize=14, weight='bold')
    ax2.text(0.5, 0.8, 'ç›´æ¥å­¦ä¹  P(Y|X)', fontsize=12, ha='center', va='center')
    ax2.text(0.5, 0.6, 'å­¦ä¹ å†³ç­–è¾¹ç•Œ', fontsize=12, ha='center', va='center')
    ax2.text(0.5, 0.4, 'ä¸“æ³¨äºåˆ†ç±»ä»»åŠ¡', fontsize=12, ha='center', va='center')
    ax2.text(0.5, 0.2, 'ä¸èƒ½ç”Ÿæˆæ–°æ ·æœ¬', fontsize=12, ha='center', va='center')
    
    # æ·»åŠ ç®­å¤´
    ax2.arrow(0.2, 0.7, 0.1, 0, head_width=0.02, head_length=0.02, fc='red', ec='red')
    ax2.arrow(0.8, 0.7, -0.1, 0, head_width=0.02, head_length=0.02, fc='red', ec='red')
    
    ax2.set_xlim(0, 1)
    ax2.set_ylim(0, 1)
    ax2.axis('off')
    
    plt.tight_layout()
    return fig

def main():
    """ç”Ÿæˆæ‰€æœ‰å›¾ç‰‡"""
    # ç¡®ä¿å›¾ç‰‡ç›®å½•å­˜åœ¨
    os.makedirs('images/basic_ml', exist_ok=True)
    os.makedirs('images/generative_models', exist_ok=True)
    
    print("ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾ç‰‡...")
    
    # ç”ŸæˆåŸºç¡€MLå›¾ç‰‡
    print("ğŸ“Š ç”Ÿæˆåå·®-æ–¹å·®æƒè¡¡å›¾...")
    fig1 = create_bias_variance_diagram()
    fig1.savefig('images/basic_ml/bias_variance_tradeoff.png', dpi=300, bbox_inches='tight')
    plt.close(fig1)
    
    print("ğŸ“Š ç”Ÿæˆè¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆå›¾...")
    fig2 = create_overfitting_diagram()
    fig2.savefig('images/basic_ml/overfitting_underfitting_analysis.png', dpi=300, bbox_inches='tight')
    plt.close(fig2)
    
    # ç”Ÿæˆç”Ÿæˆæ¨¡å‹å›¾ç‰‡
    print("ğŸ¤– ç”Ÿæˆè´å¶æ–¯å®šç†å›¾...")
    fig3 = create_bayes_theorem_diagram()
    fig3.savefig('images/generative_models/bayes_theorem_visualization.png', dpi=300, bbox_inches='tight')
    plt.close(fig3)
    
    print("ğŸ¤– ç”Ÿæˆç”Ÿæˆå¼vsåˆ¤åˆ«å¼å¯¹æ¯”å›¾...")
    fig4 = create_generative_vs_discriminative_diagram()
    fig4.savefig('images/generative_models/generative_vs_discriminative_comparison.png', dpi=300, bbox_inches='tight')
    plt.close(fig4)
    
    print("âœ… æ‰€æœ‰å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼")
    print("ğŸ“ å›¾ç‰‡ä¿å­˜åœ¨ images/ æ–‡ä»¶å¤¹ä¸­")

if __name__ == "__main__":
    main()
