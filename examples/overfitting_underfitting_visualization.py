"""
è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆå¯è§†åŒ–ç¤ºä¾‹
å¸®åŠ©ç†è§£è¿™ä¸¤ä¸ªé‡è¦æ¦‚å¿µ
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def generate_sample_data():
    """ç”Ÿæˆç¤ºä¾‹æ•°æ®"""
    np.random.seed(42)
    X = np.linspace(0, 10, 100).reshape(-1, 1)
    # çœŸå®å‡½æ•°: y = 0.5x + sin(x) + noise
    y = 0.5 * X.flatten() + np.sin(X.flatten()) + np.random.normal(0, 0.3, 100)
    return X, y

def create_models():
    """åˆ›å»ºä¸åŒå¤æ‚åº¦çš„æ¨¡å‹"""
    models = {}
    
    # æ¬ æ‹Ÿåˆ: çº¿æ€§æ¨¡å‹
    models['underfitting'] = LinearRegression()
    
    # åˆé€‚æ‹Ÿåˆ: 3æ¬¡å¤šé¡¹å¼
    models['good_fit'] = LinearRegression()
    
    # è¿‡æ‹Ÿåˆ: 15æ¬¡å¤šé¡¹å¼
    models['overfitting'] = LinearRegression()
    
    return models

def plot_overfitting_underfitting():
    """ç»˜åˆ¶è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆçš„å¯¹æ¯”å›¾"""
    X, y = generate_sample_data()
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    models = create_models()
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('è¿‡æ‹Ÿåˆ vs æ¬ æ‹Ÿåˆ å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
    
    # 1. æ¬ æ‹Ÿåˆæ¨¡å‹
    ax1 = axes[0, 0]
    model = models['underfitting']
    model.fit(X_train, y_train)
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    train_mse = mean_squared_error(y_train, y_pred_train)
    test_mse = mean_squared_error(y_test, y_pred_test)
    
    ax1.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®', color='blue')
    ax1.scatter(X_test, y_test, alpha=0.6, label='æµ‹è¯•æ•°æ®', color='red')
    
    X_plot = np.linspace(0, 10, 100).reshape(-1, 1)
    y_plot = model.predict(X_plot)
    ax1.plot(X_plot, y_plot, 'g-', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
    
    ax1.set_title(f'æ¬ æ‹Ÿåˆ (Underfitting)\nè®­ç»ƒMSE: {train_mse:.3f}, æµ‹è¯•MSE: {test_mse:.3f}')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. åˆé€‚æ‹Ÿåˆæ¨¡å‹
    ax2 = axes[0, 1]
    poly_features = PolynomialFeatures(degree=3)
    X_train_poly = poly_features.fit_transform(X_train)
    X_test_poly = poly_features.transform(X_test)
    
    model = models['good_fit']
    model.fit(X_train_poly, y_train)
    y_pred_train = model.predict(X_train_poly)
    y_pred_test = model.predict(X_test_poly)
    
    train_mse = mean_squared_error(y_train, y_pred_train)
    test_mse = mean_squared_error(y_test, y_pred_test)
    
    ax2.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®', color='blue')
    ax2.scatter(X_test, y_test, alpha=0.6, label='æµ‹è¯•æ•°æ®', color='red')
    
    X_plot_poly = poly_features.transform(X_plot)
    y_plot = model.predict(X_plot_poly)
    ax2.plot(X_plot, y_plot, 'g-', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
    
    ax2.set_title(f'åˆé€‚æ‹Ÿåˆ (Good Fit)\nè®­ç»ƒMSE: {train_mse:.3f}, æµ‹è¯•MSE: {test_mse:.3f}')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. è¿‡æ‹Ÿåˆæ¨¡å‹
    ax3 = axes[1, 0]
    poly_features = PolynomialFeatures(degree=15)
    X_train_poly = poly_features.fit_transform(X_train)
    X_test_poly = poly_features.transform(X_test)
    
    model = models['overfitting']
    model.fit(X_train_poly, y_train)
    y_pred_train = model.predict(X_train_poly)
    y_pred_test = model.predict(X_test_poly)
    
    train_mse = mean_squared_error(y_train, y_pred_train)
    test_mse = mean_squared_error(y_test, y_pred_test)
    
    ax3.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®', color='blue')
    ax3.scatter(X_test, y_test, alpha=0.6, label='æµ‹è¯•æ•°æ®', color='red')
    
    X_plot_poly = poly_features.transform(X_plot)
    y_plot = model.predict(X_plot_poly)
    ax3.plot(X_plot, y_plot, 'g-', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
    
    ax3.set_title(f'è¿‡æ‹Ÿåˆ (Overfitting)\nè®­ç»ƒMSE: {train_mse:.3f}, æµ‹è¯•MSE: {test_mse:.3f}')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. å­¦ä¹ æ›²çº¿å¯¹æ¯”
    ax4 = axes[1, 1]
    
    # ç”Ÿæˆä¸åŒè®­ç»ƒé›†å¤§å°çš„å­¦ä¹ æ›²çº¿
    train_sizes = np.linspace(0.1, 1.0, 10)
    train_scores = []
    test_scores = []
    
    for size in train_sizes:
        n_samples = int(size * len(X_train))
        X_subset = X_train[:n_samples]
        y_subset = y_train[:n_samples]
        
        # ç®€å•çº¿æ€§æ¨¡å‹
        model = LinearRegression()
        model.fit(X_subset, y_subset)
        
        train_score = model.score(X_subset, y_subset)
        test_score = model.score(X_test, y_test)
        
        train_scores.append(train_score)
        test_scores.append(test_score)
    
    ax4.plot(train_sizes, train_scores, 'o-', label='è®­ç»ƒåˆ†æ•°', color='blue')
    ax4.plot(train_sizes, test_scores, 'o-', label='æµ‹è¯•åˆ†æ•°', color='red')
    ax4.set_title('å­¦ä¹ æ›²çº¿ (Learning Curve)')
    ax4.set_xlabel('è®­ç»ƒé›†å¤§å°æ¯”ä¾‹')
    ax4.set_ylabel('RÂ² åˆ†æ•°')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('images/basic_ml/overfitting_underfitting_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return {
        'underfitting': {'train_mse': train_mse, 'test_mse': test_mse},
        'good_fit': {'train_mse': train_mse, 'test_mse': test_mse},
        'overfitting': {'train_mse': train_mse, 'test_mse': test_mse}
    }

def plot_bias_variance_tradeoff():
    """ç»˜åˆ¶åå·®-æ–¹å·®æƒè¡¡å›¾"""
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    
    # æ¨¡å‹å¤æ‚åº¦
    complexity = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    
    # åå·® (Bias) - éšç€å¤æ‚åº¦å¢åŠ è€Œå‡å°‘
    bias = np.exp(-complexity/3) + 0.1
    
    # æ–¹å·® (Variance) - éšç€å¤æ‚åº¦å¢åŠ è€Œå¢åŠ 
    variance = complexity * 0.1 + 0.05
    
    # æ€»è¯¯å·® = åå·®Â² + æ–¹å·® + å™ªå£°
    noise = 0.1
    total_error = bias**2 + variance + noise
    
    ax.plot(complexity, bias**2, 'b-', linewidth=2, label='åå·®Â² (BiasÂ²)')
    ax.plot(complexity, variance, 'r-', linewidth=2, label='æ–¹å·® (Variance)')
    ax.plot(complexity, total_error, 'g-', linewidth=3, label='æ€»è¯¯å·® (Total Error)')
    ax.axhline(y=noise, color='gray', linestyle='--', alpha=0.7, label='å™ªå£° (Noise)')
    
    # æ ‡è®°æœ€ä¼˜å¤æ‚åº¦
    optimal_idx = np.argmin(total_error)
    ax.axvline(x=complexity[optimal_idx], color='orange', linestyle='--', alpha=0.7)
    ax.plot(complexity[optimal_idx], total_error[optimal_idx], 'o', color='orange', markersize=8)
    
    ax.set_xlabel('æ¨¡å‹å¤æ‚åº¦ (Model Complexity)')
    ax.set_ylabel('è¯¯å·® (Error)')
    ax.set_title('åå·®-æ–¹å·®æƒè¡¡ (Bias-Variance Tradeoff)')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('images/basic_ml/bias_variance_tradeoff.png', dpi=300, bbox_inches='tight')
    plt.show()

def create_summary_table():
    """åˆ›å»ºæ€»ç»“è¡¨æ ¼"""
    import pandas as pd
    
    summary_data = {
        'ç‰¹å¾': ['è®­ç»ƒè¯¯å·®', 'æµ‹è¯•è¯¯å·®', 'æ³›åŒ–èƒ½åŠ›', 'æ¨¡å‹å¤æ‚åº¦', 'è§£å†³æ–¹æ¡ˆ'],
        'æ¬ æ‹Ÿåˆ (Underfitting)': [
            'é«˜', 'é«˜', 'å·®', 'è¿‡ä½', 'å¢åŠ å¤æ‚åº¦ã€ç‰¹å¾å·¥ç¨‹'
        ],
        'åˆé€‚æ‹Ÿåˆ (Good Fit)': [
            'ä½', 'ä½', 'å¥½', 'é€‚ä¸­', 'ä¿æŒç°çŠ¶'
        ],
        'è¿‡æ‹Ÿåˆ (Overfitting)': [
            'ä½', 'é«˜', 'å·®', 'è¿‡é«˜', 'æ­£åˆ™åŒ–ã€å¢åŠ æ•°æ®'
        ]
    }
    
    df = pd.DataFrame(summary_data)
    print("\nğŸ“Š è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆå¯¹æ¯”æ€»ç»“:")
    print("=" * 60)
    print(df.to_string(index=False))
    
    return df

if __name__ == "__main__":
    print("ğŸ¯ è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆå¯è§†åŒ–åˆ†æ")
    print("=" * 50)
    
    # ç”Ÿæˆå¯¹æ¯”å›¾
    results = plot_overfitting_underfitting()
    
    # ç”Ÿæˆåå·®-æ–¹å·®æƒè¡¡å›¾
    plot_bias_variance_tradeoff()
    
    # åˆ›å»ºæ€»ç»“è¡¨æ ¼
    summary_df = create_summary_table()
    
    print("\nâœ… åˆ†æå®Œæˆï¼å›¾è¡¨å·²ä¿å­˜ä¸º PNG æ–‡ä»¶")
    print("ğŸ“ æ–‡ä»¶: images/basic_ml/overfitting_underfitting_analysis.png")
    print("ğŸ“ æ–‡ä»¶: images/basic_ml/bias_variance_tradeoff.png")
