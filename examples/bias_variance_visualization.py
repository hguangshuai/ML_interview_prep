"""
åå·®-æ–¹å·®æƒè¡¡å¯è§†åŒ–åˆ†æ
ç”Ÿæˆè¯¦ç»†çš„åå·®-æ–¹å·®æƒè¡¡å›¾è¡¨å’Œç¤ºä¾‹
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def generate_sample_data():
    """ç”Ÿæˆç¤ºä¾‹æ•°æ®"""
    np.random.seed(42)
    n_samples = 200
    
    # çœŸå®å‡½æ•°: y = 0.5x + sin(x) + noise
    X = np.linspace(0, 10, n_samples).reshape(-1, 1)
    y = 0.5 * X.flatten() + np.sin(X.flatten()) + np.random.normal(0, 0.3, n_samples)
    
    return X, y

def create_models():
    """åˆ›å»ºä¸åŒå¤æ‚åº¦çš„æ¨¡å‹"""
    models = {}
    
    # 1. çº¿æ€§æ¨¡å‹ï¼ˆé«˜åå·®ï¼Œä½æ–¹å·®ï¼‰
    models['Linear'] = LinearRegression()
    
    # 2. å¤šé¡¹å¼æ¨¡å‹ï¼ˆä¸­åå·®ï¼Œä¸­æ–¹å·®ï¼‰
    models['Polynomial'] = LinearRegression()
    
    # 3. éšæœºæ£®æ—ï¼ˆä½åå·®ï¼Œä¸­æ–¹å·®ï¼‰
    models['Random Forest'] = RandomForestRegressor(n_estimators=50, random_state=42)
    
    return models

def plot_bias_variance_detailed():
    """ç»˜åˆ¶è¯¦ç»†çš„åå·®-æ–¹å·®åˆ†æ"""
    X, y = generate_sample_data()
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    models = create_models()
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('åå·®-æ–¹å·®æƒè¡¡è¯¦ç»†åˆ†æ', fontsize=16, fontweight='bold')
    
    # 1. çº¿æ€§æ¨¡å‹
    ax1 = axes[0, 0]
    model = models['Linear']
    model.fit(X_train, y_train)
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    train_mse = mean_squared_error(y_train, y_pred_train)
    test_mse = mean_squared_error(y_test, y_pred_test)
    
    ax1.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®', color='blue')
    ax1.scatter(X_test, y_test, alpha=0.6, label='æµ‹è¯•æ•°æ®', color='red')
    
    X_plot = np.linspace(0, 10, 100).reshape(-1, 1)
    y_plot = model.predict(X_plot)
    ax1.plot(X_plot, y_plot, 'g-', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
    
    ax1.set_title(f'çº¿æ€§æ¨¡å‹ (é«˜åå·®ï¼Œä½æ–¹å·®)\nè®­ç»ƒMSE: {train_mse:.3f}, æµ‹è¯•MSE: {test_mse:.3f}')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. å¤šé¡¹å¼æ¨¡å‹
    ax2 = axes[0, 1]
    poly_features = PolynomialFeatures(degree=3)
    X_train_poly = poly_features.fit_transform(X_train)
    X_test_poly = poly_features.transform(X_test)
    
    model = models['Polynomial']
    model.fit(X_train_poly, y_train)
    y_pred_train = model.predict(X_train_poly)
    y_pred_test = model.predict(X_test_poly)
    
    train_mse = mean_squared_error(y_train, y_pred_train)
    test_mse = mean_squared_error(y_test, y_pred_test)
    
    ax2.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®', color='blue')
    ax2.scatter(X_test, y_test, alpha=0.6, label='æµ‹è¯•æ•°æ®', color='red')
    
    X_plot_poly = poly_features.transform(X_plot)
    y_plot = model.predict(X_plot_poly)
    ax2.plot(X_plot, y_plot, 'g-', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
    
    ax2.set_title(f'å¤šé¡¹å¼æ¨¡å‹ (ä¸­åå·®ï¼Œä¸­æ–¹å·®)\nè®­ç»ƒMSE: {train_mse:.3f}, æµ‹è¯•MSE: {test_mse:.3f}')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. éšæœºæ£®æ—
    ax3 = axes[0, 2]
    model = models['Random Forest']
    model.fit(X_train, y_train)
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    train_mse = mean_squared_error(y_train, y_pred_train)
    test_mse = mean_squared_error(y_test, y_pred_test)
    
    ax3.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®', color='blue')
    ax3.scatter(X_test, y_test, alpha=0.6, label='æµ‹è¯•æ•°æ®', color='red')
    
    y_plot = model.predict(X_plot)
    ax3.plot(X_plot, y_plot, 'g-', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
    
    ax3.set_title(f'éšæœºæ£®æ— (ä½åå·®ï¼Œä¸­æ–¹å·®)\nè®­ç»ƒMSE: {train_mse:.3f}, æµ‹è¯•MSE: {test_mse:.3f}')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. åå·®-æ–¹å·®æƒè¡¡å›¾
    ax4 = axes[1, 0]
    complexity = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    bias_squared = np.exp(-complexity/3) + 0.1
    variance = complexity * 0.1 + 0.05
    noise = 0.1
    total_error = bias_squared + variance + noise
    
    ax4.plot(complexity, bias_squared, 'b-', linewidth=2, label='åå·®Â² (BiasÂ²)')
    ax4.plot(complexity, variance, 'r-', linewidth=2, label='æ–¹å·® (Variance)')
    ax4.plot(complexity, total_error, 'g-', linewidth=3, label='æ€»è¯¯å·® (Total Error)')
    ax4.axhline(y=noise, color='gray', linestyle='--', alpha=0.7, label='å™ªå£° (Noise)')
    
    optimal_idx = np.argmin(total_error)
    ax4.axvline(x=complexity[optimal_idx], color='orange', linestyle='--', alpha=0.7)
    ax4.plot(complexity[optimal_idx], total_error[optimal_idx], 'o', color='orange', markersize=8)
    
    ax4.set_xlabel('æ¨¡å‹å¤æ‚åº¦ (Model Complexity)')
    ax4.set_ylabel('è¯¯å·® (Error)')
    ax4.set_title('åå·®-æ–¹å·®æƒè¡¡ (Bias-Variance Tradeoff)')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. å­¦ä¹ æ›²çº¿
    ax5 = axes[1, 1]
    train_sizes = np.linspace(0.1, 1.0, 10)
    train_scores = []
    val_scores = []
    
    for size in train_sizes:
        n_samples = int(size * len(X_train))
        X_subset = X_train[:n_samples]
        y_subset = y_train[:n_samples]
        
        model = LinearRegression()
        model.fit(X_subset, y_subset)
        
        train_score = model.score(X_subset, y_subset)
        val_score = model.score(X_test, y_test)
        
        train_scores.append(train_score)
        val_scores.append(val_score)
    
    ax5.plot(train_sizes, train_scores, 'o-', label='è®­ç»ƒåˆ†æ•°', color='blue')
    ax5.plot(train_sizes, val_scores, 'o-', label='éªŒè¯åˆ†æ•°', color='red')
    ax5.set_title('å­¦ä¹ æ›²çº¿ (Learning Curve)')
    ax5.set_xlabel('è®­ç»ƒé›†å¤§å°æ¯”ä¾‹')
    ax5.set_ylabel('RÂ² åˆ†æ•°')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # 6. æ¨¡å‹å¤æ‚åº¦å¯¹æ¯”
    ax6 = axes[1, 2]
    model_names = ['Linear', 'Polynomial', 'Random Forest']
    bias_values = [0.8, 0.3, 0.1]
    variance_values = [0.1, 0.3, 0.4]
    total_errors = [b + v + 0.1 for b, v in zip(bias_values, variance_values)]
    
    x = np.arange(len(model_names))
    width = 0.25
    
    ax6.bar(x - width, bias_values, width, label='åå·®Â²', alpha=0.8)
    ax6.bar(x, variance_values, width, label='æ–¹å·®', alpha=0.8)
    ax6.bar(x + width, total_errors, width, label='æ€»è¯¯å·®', alpha=0.8)
    
    ax6.set_xlabel('æ¨¡å‹ç±»å‹')
    ax6.set_ylabel('è¯¯å·®å€¼')
    ax6.set_title('ä¸åŒæ¨¡å‹çš„åå·®-æ–¹å·®å¯¹æ¯”')
    ax6.set_xticks(x)
    ax6.set_xticklabels(model_names)
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('images/basic_ml/bias_variance_detailed_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_model_complexity_analysis():
    """ç»˜åˆ¶æ¨¡å‹å¤æ‚åº¦åˆ†æ"""
    X, y = generate_sample_data()
    
    # ä¸åŒå¤æ‚åº¦çš„å¤šé¡¹å¼æ¨¡å‹
    degrees = [1, 2, 3, 5, 10, 15]
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('æ¨¡å‹å¤æ‚åº¦å¯¹åå·®-æ–¹å·®çš„å½±å“', fontsize=16, fontweight='bold')
    
    for i, degree in enumerate(degrees):
        row = i // 3
        col = i % 3
        ax = axes[row, col]
        
        # å¤šé¡¹å¼ç‰¹å¾
        poly_features = PolynomialFeatures(degree=degree)
        X_poly = poly_features.fit_transform(X)
        
        # æ‹Ÿåˆæ¨¡å‹
        model = LinearRegression()
        model.fit(X_poly, y)
        y_pred = model.predict(X_poly)
        
        # è®¡ç®—è¯¯å·®
        mse = mean_squared_error(y, y_pred)
        
        # ç»˜åˆ¶ç»“æœ
        ax.scatter(X, y, alpha=0.6, label='çœŸå®æ•°æ®', color='blue')
        ax.plot(X, y_pred, 'r-', linewidth=2, label=f'æ¨¡å‹é¢„æµ‹ (degree={degree})')
        
        ax.set_title(f'å¤šé¡¹å¼å›å½’ (degree={degree})\nMSE: {mse:.4f}')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('images/basic_ml/model_complexity_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

def create_bias_variance_summary():
    """åˆ›å»ºåå·®-æ–¹å·®æ€»ç»“è¡¨"""
    import pandas as pd
    
    summary_data = {
        'æ¨¡å‹ç±»å‹': ['çº¿æ€§å›å½’', 'å¤šé¡¹å¼å›å½’', 'éšæœºæ£®æ—', 'æ·±åº¦ç½‘ç»œ'],
        'åå·®': ['é«˜', 'ä¸­', 'ä½', 'ä½'],
        'æ–¹å·®': ['ä½', 'ä¸­', 'ä¸­', 'é«˜'],
        'é€‚ç”¨åœºæ™¯': [
            'çº¿æ€§å…³ç³»æ˜æ˜¾',
            'éçº¿æ€§ä½†ä¸å¤ªå¤æ‚',
            'å¤æ‚éçº¿æ€§å…³ç³»',
            'éå¸¸å¤æ‚çš„å…³ç³»'
        ],
        'ä¼˜ç¼ºç‚¹': [
            'ç®€å•ç¨³å®šï¼Œä½†å¯èƒ½æ¬ æ‹Ÿåˆ',
            'å¹³è¡¡æ€§å¥½ï¼Œéœ€è¦è°ƒå‚',
            'æ€§èƒ½å¥½ï¼Œå¯è§£é‡Šæ€§ä¸­ç­‰',
            'æ€§èƒ½æœ€å¥½ï¼Œä½†å®¹æ˜“è¿‡æ‹Ÿåˆ'
        ]
    }
    
    df = pd.DataFrame(summary_data)
    print("\nğŸ“Š åå·®-æ–¹å·®æƒè¡¡æ€»ç»“è¡¨:")
    print("=" * 80)
    print(df.to_string(index=False))
    
    return df

def plot_regularization_effect():
    """ç»˜åˆ¶æ­£åˆ™åŒ–å¯¹åå·®-æ–¹å·®çš„å½±å“"""
    from sklearn.linear_model import Ridge, Lasso
    
    X, y = generate_sample_data()
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # é«˜æ¬¡å¤šé¡¹å¼ç‰¹å¾
    poly_features = PolynomialFeatures(degree=10)
    X_train_poly = poly_features.fit_transform(X_train)
    X_test_poly = poly_features.transform(X_test)
    
    # ä¸åŒæ­£åˆ™åŒ–å¼ºåº¦
    alphas = [0, 0.01, 0.1, 1, 10, 100]
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('æ­£åˆ™åŒ–å¯¹åå·®-æ–¹å·®çš„å½±å“', fontsize=16, fontweight='bold')
    
    for i, alpha in enumerate(alphas):
        row = i // 3
        col = i % 3
        ax = axes[row, col]
        
        # Ridgeå›å½’
        model = Ridge(alpha=alpha)
        model.fit(X_train_poly, y_train)
        y_pred_train = model.predict(X_train_poly)
        y_pred_test = model.predict(X_test_poly)
        
        train_mse = mean_squared_error(y_train, y_pred_train)
        test_mse = mean_squared_error(y_test, y_pred_test)
        
        ax.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®', color='blue')
        ax.scatter(X_test, y_test, alpha=0.6, label='æµ‹è¯•æ•°æ®', color='red')
        
        X_plot = np.linspace(0, 10, 100).reshape(-1, 1)
        X_plot_poly = poly_features.transform(X_plot)
        y_plot = model.predict(X_plot_poly)
        ax.plot(X_plot, y_plot, 'g-', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
        
        ax.set_title(f'Ridgeå›å½’ (Î±={alpha})\nè®­ç»ƒMSE: {train_mse:.3f}, æµ‹è¯•MSE: {test_mse:.3f}')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('images/basic_ml/regularization_effect.png', dpi=300, bbox_inches='tight')
    plt.show()

if __name__ == "__main__":
    from sklearn.model_selection import train_test_split
    
    print("ğŸ¯ åå·®-æ–¹å·®æƒè¡¡å¯è§†åŒ–åˆ†æ")
    print("=" * 50)
    
    # ç”Ÿæˆè¯¦ç»†åˆ†æå›¾
    plot_bias_variance_detailed()
    
    # ç”Ÿæˆæ¨¡å‹å¤æ‚åº¦åˆ†æå›¾
    plot_model_complexity_analysis()
    
    # ç”Ÿæˆæ­£åˆ™åŒ–æ•ˆæœå›¾
    plot_regularization_effect()
    
    # åˆ›å»ºæ€»ç»“è¡¨
    summary_df = create_bias_variance_summary()
    
    print("\nâœ… åˆ†æå®Œæˆï¼å›¾è¡¨å·²ä¿å­˜ä¸º PNG æ–‡ä»¶")
    print("ğŸ“ æ–‡ä»¶: images/basic_ml/bias_variance_detailed_analysis.png")
    print("ğŸ“ æ–‡ä»¶: images/basic_ml/model_complexity_analysis.png")
    print("ğŸ“ æ–‡ä»¶: images/basic_ml/regularization_effect.png")
