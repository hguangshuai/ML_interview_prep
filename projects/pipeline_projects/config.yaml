# ML Pipeline Configuration
# Complete configuration for end-to-end machine learning pipeline

pipeline_name: "ml_interview_pipeline"

# Data Preprocessing Configuration
data_preprocessing:
  test_size: 0.2
  random_state: 42
  scaling_method: "standard"  # standard, minmax, robust
  encoding_method: "onehot"    # onehot, label, target
  imputation_method: "mean"   # mean, median, mode, constant
  handle_outliers: true
  outlier_method: "iqr"       # iqr, zscore, isolation
  feature_selection: false
  correlation_threshold: 0.95

# Model Training Configuration
model_training:
  task_type: "classification"  # classification, regression
  cv_folds: 5
  random_state: 42
  scoring: "accuracy"         # accuracy, f1, roc_auc (classification)
                              # neg_mean_squared_error, r2 (regression)
  hyperparameter_tuning: true
  tuning_method: "random"     # grid, random
  n_iter: 50                 # For random search
  save_models: true
  model_dir: "models/"
  verbose: true

# Model Deployment Configuration
model_deployment:
  deployment_type: "api"     # api, docker, batch
  model_format: "joblib"     # joblib, pickle, onnx
  api_port: 5000
  api_host: "0.0.0.0"
  max_request_size: "16MB"
  timeout: 30
  model_dir: "deployed_models/"
  log_dir: "deployment_logs/"
  monitoring_enabled: true
  version_control: true
  health_check_interval: 60
  performance_threshold: 0.8

# Pipeline Output Configuration
output_dir: "pipeline_output/"
log_level: "INFO"

# Available Models Configuration
available_models:
  classification:
    - "logistic_regression"
    - "random_forest"
    - "svm"
    - "decision_tree"
    - "neural_network"
    - "xgboost"
    - "lightgbm"
  
  regression:
    - "linear_regression"
    - "random_forest"
    - "svr"
    - "decision_tree"
    - "neural_network"
    - "xgboost"
    - "lightgbm"

# Hyperparameter Grids
hyperparameter_grids:
  logistic_regression:
    C: [0.1, 1, 10, 100]
    penalty: ["l1", "l2"]
    solver: ["liblinear", "saga"]
  
  random_forest:
    n_estimators: [50, 100, 200]
    max_depth: [null, 10, 20, 30]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
  
  svm:
    C: [0.1, 1, 10, 100]
    kernel: ["linear", "rbf", "poly"]
    gamma: ["scale", "auto", 0.001, 0.01, 0.1]
  
  xgboost:
    n_estimators: [50, 100, 200]
    max_depth: [3, 6, 9]
    learning_rate: [0.01, 0.1, 0.2]
    subsample: [0.8, 0.9, 1.0]
  
  lightgbm:
    n_estimators: [50, 100, 200]
    max_depth: [3, 6, 9]
    learning_rate: [0.01, 0.1, 0.2]
    num_leaves: [31, 50, 100]

# Evaluation Metrics
evaluation_metrics:
  classification:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1_score"
    - "roc_auc"
    - "confusion_matrix"
  
  regression:
    - "mse"
    - "mae"
    - "rmse"
    - "r2_score"
    - "mape"

# Monitoring Configuration
monitoring:
  enabled: true
  metrics:
    - "response_time"
    - "throughput"
    - "error_rate"
    - "cpu_usage"
    - "memory_usage"
  alerting:
    enabled: true
    thresholds:
      response_time: 1000  # ms
      error_rate: 0.05     # 5%
      cpu_usage: 80        # %
      memory_usage: 80      # %

# Data Quality Checks
data_quality:
  enabled: true
  checks:
    - "missing_values"
    - "duplicates"
    - "outliers"
    - "data_types"
    - "value_ranges"
    - "correlation_analysis"
  thresholds:
    max_missing_percentage: 50
    max_duplicate_percentage: 10
    correlation_threshold: 0.95

# Feature Engineering
feature_engineering:
  enabled: true
  techniques:
    - "polynomial_features"
    - "interaction_features"
    - "binning"
    - "scaling"
    - "encoding"
  selection:
    enabled: false
    method: "mutual_info"  # mutual_info, chi2, f_test
    k_best: 10

# Model Interpretability
interpretability:
  enabled: true
  methods:
    - "feature_importance"
    - "permutation_importance"
    - "shap_values"
    - "lime"
  plots:
    - "feature_importance_plot"
    - "shap_summary_plot"
    - "partial_dependence_plot"

# Experiment Tracking
experiment_tracking:
  enabled: true
  backend: "local"  # local, mlflow, wandb
  metrics_to_track:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1_score"
    - "training_time"
    - "model_size"
  parameters_to_track:
    - "model_name"
    - "hyperparameters"
    - "data_preprocessing_steps"
    - "feature_selection"

# Security Configuration
security:
  enabled: false
  authentication:
    required: false
    method: "token"  # token, oauth, basic
  data_encryption:
    enabled: false
    algorithm: "AES256"
  api_rate_limiting:
    enabled: false
    requests_per_minute: 100

# Performance Optimization
performance:
  parallel_processing: true
  n_jobs: -1
  memory_efficient: true
  caching:
    enabled: true
    cache_dir: "cache/"
    max_size: "1GB"

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  handlers:
    - "file"
    - "console"
  file_rotation:
    enabled: true
    max_size: "10MB"
    backup_count: 5

# Notification Configuration
notifications:
  enabled: false
  channels:
    - "email"
    - "slack"
    - "webhook"
  events:
    - "pipeline_start"
    - "pipeline_complete"
    - "pipeline_failure"
    - "model_deployed"
    - "performance_degradation"
