{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4effa339",
   "metadata": {},
   "source": [
    "# 🧠 PyTorch Tensor — Creation & Basic Properties\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Question\n",
    "**Q:** How do you create tensors in PyTorch, and what are the key tensor attributes?\n",
    "\n",
    "**问：** PyTorch 中如何创建张量？有哪些重要属性？\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Explanation\n",
    "Tensors are the basic data structure in PyTorch — similar to NumPy arrays,  \n",
    "but they can also run on GPU and support automatic differentiation.\n",
    "\n",
    "Key attributes:\n",
    "- `.shape` or `.size()` → tensor dimensions  \n",
    "- `.dtype` → data type (e.g., float32, int64)  \n",
    "- `.device` → CPU or GPU  \n",
    "- `.requires_grad` → whether it tracks gradients\n",
    "\n",
    "Common creation functions:\n",
    "- `torch.tensor()` — from Python lists  \n",
    "- `torch.zeros()`, `torch.ones()` — initialize with 0 or 1  \n",
    "- `torch.randn()` — random normal distribution  \n",
    "- `torch.arange()`, `torch.linspace()` — numeric sequences  \n",
    "- `.to(device)` — move to GPU\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6af8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "zeros:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "ones:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "randn:\n",
      " tensor([[ 1.1669,  1.3793, -0.0726],\n",
      "        [ 0.6423,  1.0847, -0.3984]])\n",
      "arange: tensor([0, 2, 4, 6, 8])\n",
      "linspace: tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
      "Shape: torch.Size([2, 3])\n",
      "Dtype: torch.float32\n",
      "Device: cpu\n",
      "Requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "# ✅ PyTorch Tensor Creation Examples\n",
    "\n",
    "import torch\n",
    "\n",
    "# From list\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "print(\"a:\\n\", a)\n",
    "\n",
    "# Zero / Ones tensors\n",
    "zeros = torch.zeros((2, 3))\n",
    "ones = torch.ones((2, 3))\n",
    "print(\"zeros:\\n\", zeros)\n",
    "print(\"ones:\\n\", ones)\n",
    "\n",
    "# Random tensor (Normal distribution)\n",
    "randn = torch.randn((2, 3))\n",
    "print(\"randn:\\n\", randn)\n",
    "\n",
    "# Range tensors\n",
    "arange = torch.arange(0, 10, 2)   # 0, 2, 4, 6, 8\n",
    "linspace = torch.linspace(0, 1, 5) # 5 points between 0 and 1\n",
    "print(\"arange:\", arange)\n",
    "print(\"linspace:\", linspace)\n",
    "\n",
    "# Check attributes\n",
    "print(\"Shape:\", a.shape)\n",
    "print(\"Dtype:\", a.dtype)\n",
    "print(\"Device:\", a.device)\n",
    "print(\"Requires_grad:\", a.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd2fae",
   "metadata": {},
   "source": [
    "# 🧠 PyTorch Tensor Shape Operations — Reshape, View, Squeeze, Unsqueeze, Flatten\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Question  \n",
    "**Q:** How do you change the shape of a tensor in PyTorch?  \n",
    "Explain the difference between `reshape`, `view`, `squeeze`, `unsqueeze`, and `flatten`.\n",
    "\n",
    "**问：** PyTorch 中如何改变张量形状？`reshape`、`view`、`squeeze`、`unsqueeze`、`flatten` 有何区别？\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Explanation  \n",
    "\n",
    "Changing tensor dimensions is common in neural network pipelines — for example, when:\n",
    "- feeding images into fully connected layers\n",
    "- processing batched data\n",
    "- removing unnecessary singleton dimensions\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `reshape()`  \n",
    "Returns a new tensor with the desired shape.  \n",
    "It **creates a new view** if possible; otherwise copies data.  \n",
    "Flexible and safe in general.\n",
    "\n",
    "$$A.reshape(\\text{new\\_shape})$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `view()`  \n",
    "Similar to `reshape`, but **requires the tensor to be contiguous** in memory.  \n",
    "Faster, but may fail if tensor is non-contiguous (e.g., after transpose).  \n",
    "Often used in training loops because of performance.\n",
    "\n",
    "$$A.view(\\text{new\\_shape})$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `squeeze()`  \n",
    "Removes dimensions of size 1.  \n",
    "Example: shape `[1, 3, 1, 4] → [3, 4]`.\n",
    "\n",
    "$$\\text{squeeze}(A): \\text{remove size-1 axes}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `unsqueeze()`  \n",
    "Adds a new dimension of size 1 at a given position.  \n",
    "Used to make tensors broadcastable or add batch/channel axes.\n",
    "\n",
    "$$\\text{unsqueeze}(A, \\text{dim})$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `flatten()`  \n",
    "Flattens all dimensions except optionally one (like batch).  \n",
    "Example: `[batch, channel, height, width] → [batch, -1]`.\n",
    "\n",
    "$$\\text{flatten}(A, \\text{start\\_dim}=1)$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Formula  \n",
    "If a tensor has total elements $N = d_1 \\times d_2 \\times ... \\times d_n$,  \n",
    "then any reshaping must satisfy:\n",
    "\n",
    "$$N_{\\text{before}} = N_{\\text{after}}$$\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Interview tip  \n",
    "In practice:\n",
    "- `view()` is common inside `forward()`  \n",
    "- `squeeze()` and `unsqueeze()` often appear when aligning dimensions  \n",
    "- `flatten()` is used before linear layers (e.g., CNN → FC)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56400202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Shape: torch.Size([12])\n",
      "\n",
      "Reshape (3,4):\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "View (2,6):\n",
      " tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n",
      "\n",
      "Unsqueeze -> shape: torch.Size([1, 3, 4])\n",
      "Squeeze -> shape: torch.Size([3, 4])\n",
      "\n",
      "Flatten (all): torch.Size([12])\n",
      "Flatten from dim=1: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# ✅ Tensor Shape Operations Examples\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.arange(12)\n",
    "print(\"Original x:\", x)\n",
    "print(\"Shape:\", x.shape)\n",
    "\n",
    "# 🔹 reshape\n",
    "x_reshape = x.reshape(3, 4)\n",
    "print(\"\\nReshape (3,4):\\n\", x_reshape)\n",
    "\n",
    "# 🔹 view\n",
    "x_view = x.view(2, 6)\n",
    "print(\"\\nView (2,6):\\n\", x_view)\n",
    "\n",
    "# 🔹 squeeze / unsqueeze\n",
    "x_unsq = x_reshape.unsqueeze(0)      # Add dim at front -> shape (1,3,4)\n",
    "x_sq = x_unsq.squeeze(0)             # Remove dim size=1 -> back to (3,4)\n",
    "print(\"\\nUnsqueeze -> shape:\", x_unsq.shape)\n",
    "print(\"Squeeze -> shape:\", x_sq.shape)\n",
    "\n",
    "# 🔹 flatten\n",
    "x_flat = x_reshape.flatten()         # (12,)\n",
    "x_flat_batch = x_reshape.flatten(start_dim=1)  # keep first dim (3,4)->(3,4)\n",
    "print(\"\\nFlatten (all):\", x_flat.shape)\n",
    "print(\"Flatten from dim=1:\", x_flat_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28673da4",
   "metadata": {},
   "source": [
    "# 🧠 PyTorch Tensor Dimension Swapping — Transpose / Permute / T\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Question  \n",
    "**Q:** How do you swap or reorder tensor dimensions in PyTorch?  \n",
    "Explain the difference between `transpose()`, `permute()`, and `.T`.\n",
    "\n",
    "**问：**  \n",
    "如何在 PyTorch 中交换或重排张量的维度？`transpose()`、`permute()` 和 `.T` 有什么区别？\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Explanation  \n",
    "\n",
    "When working with multi-dimensional tensors (e.g., images, sequences, batches),  \n",
    "you often need to **reorder or swap dimensions**.  \n",
    "Typical use cases include converting image data formats, such as  \n",
    "from `[batch, height, width, channel]` → `[batch, channel, height, width]`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `transpose(dim0, dim1)`\n",
    "\n",
    "Swaps **two** dimensions only.\n",
    "\n",
    "$$x' = \\text{transpose}(x, i, j)$$\n",
    "\n",
    "Example: shape `[2, 3, 4] → [3, 2, 4]`\n",
    "\n",
    "$$x'_{a,\\dots,i,\\dots,j,\\dots,b} = x_{a,\\dots,j,\\dots,i,\\dots,b}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `.T`\n",
    "\n",
    "For **2D tensors**, `.T` is equivalent to `.transpose(0, 1)`:\n",
    "\n",
    "$$x^\\top_{i,j} = x_{j,i}$$\n",
    "\n",
    "For higher-dimensional tensors (≥3D), `.T` **reverses all dimensions**:  \n",
    "Example: `[2, 3, 4] → [4, 3, 2]`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `permute(dims)`\n",
    "\n",
    "Reorders **multiple** dimensions arbitrarily.  \n",
    "It is the most general and powerful method.\n",
    "\n",
    "$$x' = \\text{permute}(x, (\\text{new\\_order}))$$\n",
    "\n",
    "Example:  \n",
    "$$\\text{permute}(x, (0, 3, 1, 2)): [N, H, W, C] \\to [N, C, H, W]$$\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Key Concept\n",
    "\n",
    "All these methods **return a view**, not a data copy — they change the way data is interpreted in memory.  \n",
    "That means the new tensor may become **non-contiguous**,  \n",
    "so you might need to call `.contiguous()` before `.view()`.\n",
    "\n",
    "$$x_{\\text{contiguous}} = x.\\text{permute(...)}.\\text{contiguous()}$$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b693828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([2, 3, 4])\n",
      "\n",
      "After transpose(0,1): torch.Size([3, 2, 4])\n",
      "After permute(1,0,2): torch.Size([3, 2, 4])\n",
      "\n",
      "Matrix:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "m.T:\n",
      " tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[[ 0,  4,  8],\n",
      "         [12, 16, 20]],\n",
      "\n",
      "        [[ 1,  5,  9],\n",
      "         [13, 17, 21]],\n",
      "\n",
      "        [[ 2,  6, 10],\n",
      "         [14, 18, 22]],\n",
      "\n",
      "        [[ 3,  7, 11],\n",
      "         [15, 19, 23]]])\n",
      "\n",
      "Is contiguous? False\n",
      "tensor([[[ 0,  4,  8],\n",
      "         [12, 16, 20]],\n",
      "\n",
      "        [[ 1,  5,  9],\n",
      "         [13, 17, 21]],\n",
      "\n",
      "        [[ 2,  6, 10],\n",
      "         [14, 18, 22]],\n",
      "\n",
      "        [[ 3,  7, 11],\n",
      "         [15, 19, 23]]])\n",
      "After .contiguous(): True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 3D tensor\n",
    "x = torch.arange(24).reshape(2, 3, 4)\n",
    "print(\"Original shape:\", x.shape)  # (2, 3, 4)\n",
    "\n",
    "# 🔹 transpose: swap two dims\n",
    "x_t = x.transpose(0, 1)\n",
    "print(\"\\nAfter transpose(0,1):\", x_t.shape)  # (3, 2, 4)\n",
    "\n",
    "# 🔹 permute: reorder multiple dims\n",
    "x_p = x.permute(1, 0, 2)\n",
    "print(\"After permute(1,0,2):\", x_p.shape)    # (3, 2, 4)\n",
    "\n",
    "# 🔹 .T (for 2D)\n",
    "m = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "print(\"\\nMatrix:\\n\", m)\n",
    "print(\"m.T:\\n\", m.T)  # same as m.transpose(0,1)\n",
    "\n",
    "# 🔹 non-contiguous example\n",
    "x_perm = x.permute(2, 0, 1)\n",
    "print(x_perm)\n",
    "print(\"\\nIs contiguous?\", x_perm.is_contiguous())  # usually False\n",
    "x_contig = x_perm.contiguous()\n",
    "print(x_contig)\n",
    "print(\"After .contiguous():\", x_contig.is_contiguous())  # True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22296eae",
   "metadata": {},
   "source": [
    "# 🧠 Understanding `contiguous()` in PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Question\n",
    "\n",
    "**Q:** What does `contiguous()` mean in PyTorch, and why do we need it?\n",
    "\n",
    "**问：**  \n",
    "PyTorch 中的 `contiguous()` 是什么？为什么需要它？\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Explanation\n",
    "\n",
    "In PyTorch, tensors are stored as **blocks of memory**.  \n",
    "Each tensor has two key properties:\n",
    "\n",
    "1️⃣ **data** – the actual values stored in memory  \n",
    "2️⃣ **stride** – how many memory steps to move to get to the next element in each dimension\n",
    "\n",
    "A tensor is said to be **contiguous** when its elements are stored **sequentially in memory**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Example: Contiguous Tensor\n",
    "\n",
    "A normal tensor created by `torch.arange()` or `torch.randn()` is contiguous by default.\n",
    "\n",
    "$$\\text{Memory layout: } [0, 1, 2, 3, 4, 5]$$\n",
    "\n",
    "This layout matches its shape order, so:\n",
    "\n",
    "$$x.\\text{is\\_contiguous()} = \\text{True}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Example: Non-Contiguous Tensor\n",
    "\n",
    "When you use operations like `transpose()` or `permute()`,  \n",
    "PyTorch **does not copy data** — it only changes how indices map to memory positions using strides.\n",
    "\n",
    "$$\\text{Transpose changes the stride order, not the memory itself.}$$\n",
    "\n",
    "Hence, after transpose:\n",
    "\n",
    "$$x.\\text{is\\_contiguous()} = \\text{False}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Why It Matters\n",
    "\n",
    "The `.view()` function in PyTorch **requires** the tensor to be contiguous,  \n",
    "because it directly reinterprets the existing memory layout.\n",
    "\n",
    "If a tensor is non-contiguous, `.view()` will raise:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7bcc4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "is_contiguous: True\n",
      "\n",
      "y (transposed):\n",
      " tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "is_contiguous: False\n",
      "\n",
      "Error when using view on non-contiguous tensor:\n",
      "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "\n",
      "After .contiguous(): is_contiguous = True\n",
      "Reshaped z: tensor([0, 3, 1, 4, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1️⃣ Create a contiguous tensor\n",
    "x = torch.arange(6).reshape(2, 3)\n",
    "print(\"x:\\n\", x)\n",
    "print(\"is_contiguous:\", x.is_contiguous())  # ✅ True\n",
    "\n",
    "# 2️⃣ Transpose makes it non-contiguous\n",
    "y = x.t()\n",
    "print(\"\\ny (transposed):\\n\", y)\n",
    "print(\"is_contiguous:\", y.is_contiguous())  # ❌ False\n",
    "\n",
    "# 3️⃣ Trying to use .view() now will fail\n",
    "try:\n",
    "    z = y.view(-1)\n",
    "except RuntimeError as e:\n",
    "    print(\"\\nError when using view on non-contiguous tensor:\")\n",
    "    print(e)\n",
    "\n",
    "# 4️⃣ Fix by making it contiguous\n",
    "y_contig = y.contiguous()\n",
    "print(\"\\nAfter .contiguous(): is_contiguous =\", y_contig.is_contiguous())\n",
    "\n",
    "# Now .view() works fine\n",
    "z = y_contig.view(-1)\n",
    "print(\"Reshaped z:\", z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b64108",
   "metadata": {},
   "source": [
    "# 🧠 PyTorch Tensor Concatenation & Splitting — `cat`, `stack`, `split`, `chunk`\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Question\n",
    "\n",
    "**Q:** How do you combine and separate tensors in PyTorch?  \n",
    "Explain the difference between `torch.cat`, `torch.stack`, `torch.split`, and `torch.chunk`.\n",
    "\n",
    "**问：**  \n",
    "在 PyTorch 中如何拼接和分割张量？`torch.cat`、`torch.stack`、`torch.split`、`torch.chunk` 有什么区别？\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Explanation\n",
    "\n",
    "### 🔹 1. `torch.cat(tensors, dim)`\n",
    "\n",
    "Concatenates a **sequence of tensors** along an existing dimension.\n",
    "\n",
    "\\[\n",
    "y = \\text{cat}([x_1, x_2, ...], \\text{dim}=d)\n",
    "\\]\n",
    "\n",
    "All tensors must have the **same shape** except for the concatenation dimension.\n",
    "\n",
    "**Example:**  \n",
    "Concatenating along dim=0 → adds more rows  \n",
    "Concatenating along dim=1 → adds more columns\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. `torch.stack(tensors, dim)`\n",
    "\n",
    "Stacks tensors along a **new dimension**.  \n",
    "Unlike `cat`, it **creates a new axis**.\n",
    "\n",
    "\\[\n",
    "y = \\text{stack}([x_1, x_2, ...], \\text{dim}=d)\n",
    "\\]\n",
    "\n",
    "All tensors must have **exactly the same shape**.\n",
    "\n",
    "Example: stacking 3 vectors of shape `(2,)` → result `(3, 2)`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 3. `torch.split(tensor, split_size_or_sections, dim)`\n",
    "\n",
    "Splits a tensor into **equal parts or specified sizes** along a given dimension.\n",
    "\n",
    "\\[\n",
    "[x_1, x_2, ...] = \\text{split}(x, n, \\text{dim})\n",
    "\\]\n",
    "\n",
    "If `split_size_or_sections` is an integer, it divides into equal chunks.  \n",
    "If it’s a list, it splits with specified lengths.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 4. `torch.chunk(tensor, chunks, dim)`\n",
    "\n",
    "Splits a tensor into a **given number of chunks** along the specified dimension.  \n",
    "The last chunk may be smaller if not evenly divisible.\n",
    "\n",
    "\\[\n",
    "[x_1, x_2, ...] = \\text{chunk}(x, k, \\text{dim})\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Visual Concept\n",
    "\n",
    "If we have\n",
    "\n",
    "\\[\n",
    "A = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}, \\quad\n",
    "B = \\begin{bmatrix}\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "Then:\n",
    "\n",
    "\\[\n",
    "\\text{cat}([A,B], 0) =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}\n",
    ",\\quad\n",
    "\\text{cat}([A,B], 1) =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 5 & 6 \\\\\n",
    "3 & 4 & 7 & 8\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{stack}([A,B], 0) \\Rightarrow \\text{shape }(2,2,2)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Summary\n",
    "\n",
    "| Function | Purpose | Creates New Dim? | Input Shape Requirement |\n",
    "|-----------|----------|------------------|--------------------------|\n",
    "| `torch.cat` | Join along existing dim | ❌ No | Same except concat dim |\n",
    "| `torch.stack` | Join along new dim | ✅ Yes | Exactly same shape |\n",
    "| `torch.split` | Split by size/sections | ❌ No | Any |\n",
    "| `torch.chunk` | Split by number of chunks | ❌ No | Any |\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Interview Tip\n",
    "\n",
    "> Use `cat` when combining tensors along an existing axis,  \n",
    "> use `stack` when you need to add a new dimension (e.g. stacking images into batch),  \n",
    "> use `split` or `chunk` to divide large tensors into smaller batches for processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d768beb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat dim=0:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "cat dim=1:\n",
      " tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n",
      "\n",
      "stack dim=0: torch.Size([2, 2, 2])\n",
      "stack dim=1: torch.Size([2, 2, 2])\n",
      "\n",
      "split by 3: [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
      "split custom [2,4,4]: [[0, 1], [2, 3, 4, 5], [6, 7, 8, 9]]\n",
      "\n",
      "chunk(4): [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1️⃣ Prepare tensors\n",
    "a = torch.tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "b = torch.tensor([[5, 6],\n",
    "                  [7, 8]])\n",
    "\n",
    "# 🔹 cat: concatenate along existing dim\n",
    "cat_dim0 = torch.cat((a, b), dim=0)\n",
    "cat_dim1 = torch.cat((a, b), dim=1)\n",
    "print(\"cat dim=0:\\n\", cat_dim0)\n",
    "print(\"cat dim=1:\\n\", cat_dim1)\n",
    "\n",
    "# 🔹 stack: adds a new dimension\n",
    "stack_0 = torch.stack((a, b), dim=0)\n",
    "stack_1 = torch.stack((a, b), dim=1)\n",
    "print(\"\\nstack dim=0:\", stack_0.shape)  # (2,2,2)\n",
    "print(\"stack dim=1:\", stack_1.shape)    # (2,2,2)\n",
    "\n",
    "# 🔹 split: by size\n",
    "x = torch.arange(10)\n",
    "splits = torch.split(x, 3)  # 3,3,3,1\n",
    "print(\"\\nsplit by 3:\", [s.tolist() for s in splits])\n",
    "\n",
    "# 🔹 split by custom sizes\n",
    "splits_custom = torch.split(x, [2, 4, 4])\n",
    "print(\"split custom [2,4,4]:\", [s.tolist() for s in splits_custom])\n",
    "\n",
    "# 🔹 chunk: by number of parts\n",
    "chunks = torch.chunk(x, 4)  # split into 4 chunks\n",
    "print(\"\\nchunk(4):\", [c.tolist() for c in chunks])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2721bef8",
   "metadata": {},
   "source": [
    "# 🧠 PyTorch Tensor Broadcasting & Expanding — `expand`, `repeat`\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Question\n",
    "\n",
    "**Q:** What is broadcasting in PyTorch, and how do `expand()` and `repeat()` work?  \n",
    "Explain their differences.\n",
    "\n",
    "**问：**  \n",
    "PyTorch 中的广播机制是什么？`expand()` 和 `repeat()` 有什么区别？\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Explanation\n",
    "\n",
    "Broadcasting allows tensors with **different shapes** to be combined in arithmetic operations  \n",
    "(addition, multiplication, etc.) **without explicit copying**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Broadcasting Rule\n",
    "\n",
    "When performing operations like `a + b`, PyTorch automatically compares tensor shapes **from right to left**:\n",
    "\n",
    "1️⃣ If dimensions are equal → ✅ compatible  \n",
    "2️⃣ If one of them is 1 → ✅ expand it to match  \n",
    "3️⃣ Otherwise → ❌ incompatible (raises error)\n",
    "\n",
    "\\[\n",
    "\\text{Example: } (3, 1) + (1, 4) \\Rightarrow (3, 4)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `expand(*sizes)`\n",
    "\n",
    "Creates a **new view** of the tensor by “virtually” expanding dimensions.  \n",
    "**No new memory is allocated** — it only changes the tensor’s stride interpretation.\n",
    "\n",
    "\\[\n",
    "y = x.\\text{expand}(3, 4)\n",
    "\\]\n",
    "\n",
    "⚠️ You can only expand a dimension if its size is **1** or already matches the target.\n",
    "\n",
    "**Efficient but read-only** — modifying the expanded tensor may raise errors.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `repeat(*sizes)`\n",
    "\n",
    "**Physically copies data** along specified dimensions to make a larger tensor.\n",
    "\n",
    "\\[\n",
    "y = x.\\text{repeat}(3, 4)\n",
    "\\]\n",
    "\n",
    "This actually **allocates new memory**, so it’s slower but safe to modify.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Key Difference\n",
    "\n",
    "| Function | Creates Copy? | Modifiable? | Use Case |\n",
    "|-----------|----------------|--------------|-----------|\n",
    "| `expand()` | ❌ No | ⚠️ Read-only | Memory-efficient broadcasting |\n",
    "| `repeat()` | ✅ Yes | ✅ Writable | When actual replication is needed |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Example: shapes\n",
    "\n",
    "\\[\n",
    "x = [1, 2, 3] \\text{ (shape } (3,)) \\\\\n",
    "x.expand(2,3) \\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 2 & 3\n",
    "\\end{bmatrix} \\text{ (no copy)}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "x.repeat(2,1) \\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 2 & 3\n",
    "\\end{bmatrix} \\text{ (copied data)}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Broadcasting in Practice\n",
    "\n",
    "\\[\n",
    "(3,1) + (1,4) \\Rightarrow (3,4)\n",
    "\\]\n",
    "\n",
    "This allows efficient elementwise operations across tensors with different but compatible shapes.\n",
    "\n",
    "**Example:**\n",
    "- `x` shape `(3,1)`\n",
    "- `y` shape `(1,4)`\n",
    "- `x + y` automatically becomes `(3,4)` via broadcasting\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Interview Tip\n",
    "\n",
    "> Broadcasting enables implicit expansion of dimensions for elementwise ops.  \n",
    "> `expand()` provides a lightweight “view” of that expansion,  \n",
    "> while `repeat()` makes an actual data copy.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06ca93",
   "metadata": {},
   "source": [
    "# 🧠 PyTorch Tensor Broadcasting & Expanding — `expand`, `repeat` (Fixed)\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Question\n",
    "\n",
    "**Q:** What is broadcasting in PyTorch, and how do `expand()` and `repeat()` work?  \n",
    "Explain their differences.\n",
    "\n",
    "**问：**  \n",
    "PyTorch 中的广播机制是什么？`expand()` 和 `repeat()` 有什么区别？\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Explanation\n",
    "\n",
    "Broadcasting allows tensors with **different shapes** to be combined in arithmetic operations  \n",
    "(addition, multiplication, etc.) **without explicit copying**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Broadcasting Rule\n",
    "\n",
    "When performing operations like `a + b`, PyTorch automatically compares tensor shapes **from right to left**:\n",
    "\n",
    "1️⃣ If dimensions are equal → ✅ compatible  \n",
    "2️⃣ If one of them is 1 → ✅ expand it to match  \n",
    "3️⃣ Otherwise → ❌ incompatible (raises error)\n",
    "\n",
    "$$\\text{Example: } (3, 1) + (1, 4) \\Rightarrow (3, 4)$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `expand(*sizes)`\n",
    "\n",
    "Creates a **new view** of the tensor by \"virtually\" expanding dimensions.  \n",
    "**No new memory is allocated** — it only changes the tensor's stride interpretation.\n",
    "\n",
    "$$y = x.\\text{expand}(3, 4)$$\n",
    "\n",
    "⚠️ You can only expand a dimension if its size is **1** or already matches the target.\n",
    "\n",
    "**Efficient but read-only** — modifying the expanded tensor may raise errors.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `repeat(*sizes)`\n",
    "\n",
    "**Physically copies data** along specified dimensions to make a larger tensor.\n",
    "\n",
    "$$y = x.\\text{repeat}(3, 4)$$\n",
    "\n",
    "This actually **allocates new memory**, so it's slower but safe to modify.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Key Difference\n",
    "\n",
    "| Function | Creates Copy? | Modifiable? | Use Case |\n",
    "|-----------|----------------|--------------|-----------|\n",
    "| `expand()` | ❌ No | ⚠️ Read-only | Memory-efficient broadcasting |\n",
    "| `repeat()` | ✅ Yes | ✅ Writable | When actual replication is needed |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Example: shapes\n",
    "\n",
    "$$x = [1, 2, 3] \\text{ (shape } (3,)) \\\\\n",
    "x.\\text{expand}(2,3) \\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 2 & 3\n",
    "\\end{bmatrix} \\text{ (no copy)}$$\n",
    "\n",
    "$$x.\\text{repeat}(2,1) \\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 2 & 3\n",
    "\\end{bmatrix} \\text{ (copied data)}$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Broadcasting in Practice\n",
    "\n",
    "$$(3,1) + (1,4) \\Rightarrow (3,4)$$\n",
    "\n",
    "This allows efficient elementwise operations across tensors with different but compatible shapes.\n",
    "\n",
    "**Example:**\n",
    "- `x` shape `(3,1)`\n",
    "- `y` shape `(1,4)`\n",
    "- `x + y` automatically becomes `(3,4)` via broadcasting\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Interview Tip\n",
    "\n",
    "> Broadcasting enables implicit expansion of dimensions for elementwise ops.  \n",
    "> `expand()` provides a lightweight \"view\" of that expansion,  \n",
    "> while `repeat()` makes an actual data copy.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b459511",
   "metadata": {},
   "source": [
    "# 🧠 PyTorch Tensor Concatenation & Splitting — `cat`, `stack`, `split`, `chunk` (Fixed)\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Question\n",
    "\n",
    "**Q:** How do you combine and separate tensors in PyTorch?  \n",
    "Explain the difference between `torch.cat`, `torch.stack`, `torch.split`, and `torch.chunk`.\n",
    "\n",
    "**问：**  \n",
    "在 PyTorch 中如何拼接和分割张量？`torch.cat`、`torch.stack`、`torch.split`、`torch.chunk` 有什么区别？\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Explanation\n",
    "\n",
    "### 🔹 1. `torch.cat(tensors, dim)`\n",
    "\n",
    "Concatenates a **sequence of tensors** along an existing dimension.\n",
    "\n",
    "$$y = \\text{cat}([x_1, x_2, ...], \\text{dim}=d)$$\n",
    "\n",
    "All tensors must have the **same shape** except for the concatenation dimension.\n",
    "\n",
    "**Example:**  \n",
    "Concatenating along dim=0 → adds more rows  \n",
    "Concatenating along dim=1 → adds more columns\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. `torch.stack(tensors, dim)`\n",
    "\n",
    "Stacks tensors along a **new dimension**.  \n",
    "Unlike `cat`, it **creates a new axis**.\n",
    "\n",
    "$$y = \\text{stack}([x_1, x_2, ...], \\text{dim}=d)$$\n",
    "\n",
    "All tensors must have **exactly the same shape**.\n",
    "\n",
    "Example: stacking 3 vectors of shape `(2,)` → result `(3, 2)`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 3. `torch.split(tensor, split_size_or_sections, dim)`\n",
    "\n",
    "Splits a tensor into **equal parts or specified sizes** along a given dimension.\n",
    "\n",
    "$$[x_1, x_2, ...] = \\text{split}(x, n, \\text{dim})$$\n",
    "\n",
    "If `split_size_or_sections` is an integer, it divides into equal chunks.  \n",
    "If it's a list, it splits with specified lengths.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 4. `torch.chunk(tensor, chunks, dim)`\n",
    "\n",
    "Splits a tensor into a **given number of chunks** along the specified dimension.  \n",
    "The last chunk may be smaller if not evenly divisible.\n",
    "\n",
    "$$[x_1, x_2, ...] = \\text{chunk}(x, k, \\text{dim})$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Visual Concept\n",
    "\n",
    "If we have\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}, \\quad\n",
    "B = \\begin{bmatrix}\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\text{cat}([A,B], 0) =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}\n",
    ",\\quad\n",
    "\\text{cat}([A,B], 1) =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 5 & 6 \\\\\n",
    "3 & 4 & 7 & 8\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\text{stack}([A,B], 0) \\Rightarrow \\text{shape }(2,2,2)$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Summary\n",
    "\n",
    "| Function | Purpose | Creates New Dim? | Input Shape Requirement |\n",
    "|-----------|----------|------------------|--------------------------|\n",
    "| `torch.cat` | Join along existing dim | ❌ No | Same except concat dim |\n",
    "| `torch.stack` | Join along new dim | ✅ Yes | Exactly same shape |\n",
    "| `torch.split` | Split by size/sections | ❌ No | Any |\n",
    "| `torch.chunk` | Split by number of chunks | ❌ No | Any |\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Interview Tip\n",
    "\n",
    "> Use `cat` when combining tensors along an existing axis,  \n",
    "> use `stack` when you need to add a new dimension (e.g. stacking images into batch),  \n",
    "> use `split` or `chunk` to divide large tensors into smaller batches for processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbfab09",
   "metadata": {},
   "source": [
    "# 🧠 PyTorch Tensor Concatenation & Splitting — `cat`, `stack`, `split`, `chunk` (Fixed)\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Question\n",
    "\n",
    "**Q:** How do you combine and separate tensors in PyTorch?  \n",
    "Explain the difference between `torch.cat`, `torch.stack`, `torch.split`, and `torch.chunk`.\n",
    "\n",
    "**问：**  \n",
    "在 PyTorch 中如何拼接和分割张量？`torch.cat`、`torch.stack`、`torch.split`、`torch.chunk` 有什么区别？\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Explanation\n",
    "\n",
    "### 🔹 1. `torch.cat(tensors, dim)`\n",
    "\n",
    "Concatenates a **sequence of tensors** along an existing dimension.\n",
    "\n",
    "$$y = \\text{cat}([x_1, x_2, ...], \\text{dim}=d)$$\n",
    "\n",
    "All tensors must have the **same shape** except for the concatenation dimension.\n",
    "\n",
    "**Example:**  \n",
    "Concatenating along dim=0 → adds more rows  \n",
    "Concatenating along dim=1 → adds more columns\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. `torch.stack(tensors, dim)`\n",
    "\n",
    "Stacks tensors along a **new dimension**.  \n",
    "Unlike `cat`, it **creates a new axis**.\n",
    "\n",
    "$$y = \\text{stack}([x_1, x_2, ...], \\text{dim}=d)$$\n",
    "\n",
    "All tensors must have **exactly the same shape**.\n",
    "\n",
    "Example: stacking 3 vectors of shape `(2,)` → result `(3, 2)`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 3. `torch.split(tensor, split_size_or_sections, dim)`\n",
    "\n",
    "Splits a tensor into **equal parts or specified sizes** along a given dimension.\n",
    "\n",
    "$$[x_1, x_2, ...] = \\text{split}(x, n, \\text{dim})$$\n",
    "\n",
    "If `split_size_or_sections` is an integer, it divides into equal chunks.  \n",
    "If it's a list, it splits with specified lengths.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 4. `torch.chunk(tensor, chunks, dim)`\n",
    "\n",
    "Splits a tensor into a **given number of chunks** along the specified dimension.  \n",
    "The last chunk may be smaller if not evenly divisible.\n",
    "\n",
    "$$[x_1, x_2, ...] = \\text{chunk}(x, k, \\text{dim})$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Visual Concept\n",
    "\n",
    "If we have\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}, \\quad\n",
    "B = \\begin{bmatrix}\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\text{cat}([A,B], 0) =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}\n",
    ",\\quad\n",
    "\\text{cat}([A,B], 1) =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 5 & 6 \\\\\n",
    "3 & 4 & 7 & 8\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\text{stack}([A,B], 0) \\Rightarrow \\text{shape }(2,2,2)$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Summary\n",
    "\n",
    "| Function | Purpose | Creates New Dim? | Input Shape Requirement |\n",
    "|-----------|----------|------------------|--------------------------|\n",
    "| `torch.cat` | Join along existing dim | ❌ No | Same except concat dim |\n",
    "| `torch.stack` | Join along new dim | ✅ Yes | Exactly same shape |\n",
    "| `torch.split` | Split by size/sections | ❌ No | Any |\n",
    "| `torch.chunk` | Split by number of chunks | ❌ No | Any |\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Interview Tip\n",
    "\n",
    "> Use `cat` when combining tensors along an existing axis,  \n",
    "> use `stack` when you need to add a new dimension (e.g. stacking images into batch),  \n",
    "> use `split` or `chunk` to divide large tensors into smaller batches for processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ba6e1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape: torch.Size([3, 1])\n",
      "b shape: torch.Size([1, 4])\n",
      "a + b shape: torch.Size([3, 4])\n",
      "\n",
      "expand result:\n",
      " tensor([[1, 2, 3],\n",
      "        [1, 2, 3]])\n",
      "Is same memory: True\n",
      "\n",
      "repeat result:\n",
      " tensor([[999,   2,   3],\n",
      "        [999,   2,   3]])\n",
      "Is same memory: False\n",
      "After modify repeat:\n",
      " tensor([[999,   2,   3],\n",
      "        [999,   2,   3]])\n",
      "Original x unchanged:\n",
      " tensor([999,   2,   3])\n",
      "\n",
      "Broadcasting result:\n",
      " tensor([[0, 1, 2, 3],\n",
      "        [1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1️⃣ Broadcasting automatically\n",
    "a = torch.arange(3).reshape(3, 1)      # shape (3,1)\n",
    "b = torch.arange(4).reshape(1, 4)      # shape (1,4)\n",
    "print(\"a shape:\", a.shape)\n",
    "print(\"b shape:\", b.shape)\n",
    "print(\"a + b shape:\", (a + b).shape)   # (3,4)\n",
    "\n",
    "# 2️⃣ expand(): view, no copy\n",
    "x = torch.tensor([1, 2, 3])\n",
    "x_exp = x.expand(2, 3)  # virtual repeat\n",
    "print(\"\\nexpand result:\\n\", x_exp)\n",
    "print(\"Is same memory:\", x_exp.data_ptr() == x.data_ptr())  # ✅ same memory\n",
    "try:\n",
    "    x_exp[0, 0] = 999   # will affect original or error (depends on stride)\n",
    "except RuntimeError as e:\n",
    "    print(\"Modify expand -> error:\", e)\n",
    "\n",
    "# 3️⃣ repeat(): real copy\n",
    "x_rep = x.repeat(2, 1)\n",
    "print(\"\\nrepeat result:\\n\", x_rep)\n",
    "print(\"Is same memory:\", x_rep.data_ptr() == x.data_ptr())  # ❌ different memory\n",
    "x_rep[0, 0] = 999\n",
    "print(\"After modify repeat:\\n\", x_rep)\n",
    "print(\"Original x unchanged:\\n\", x)\n",
    "\n",
    "# 4️⃣ Broadcasting example\n",
    "c = torch.arange(3).reshape(3, 1)\n",
    "d = torch.arange(4).reshape(1, 4)\n",
    "res = c + d\n",
    "print(\"\\nBroadcasting result:\\n\", res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_coding_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
