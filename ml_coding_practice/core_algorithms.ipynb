{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 核心算法实现练习 (Core Algorithms Implementation Practice)\n",
        "\n",
        "## 📚 包含的算法\n",
        "\n",
        "### 1. Linear Regression (线性回归)\n",
        "- **目标**: 预测连续值\n",
        "- **实现**: 梯度下降 + MSE损失 + L2正则化\n",
        "- **关键**: 理解梯度下降原理\n",
        "\n",
        "### 2. Logistic Regression (逻辑回归)  \n",
        "- **目标**: 二分类问题\n",
        "- **实现**: 交叉熵损失 + Sigmoid激活\n",
        "- **关键**: 数值稳定性处理\n",
        "\n",
        "### 3. K-Nearest Neighbors (K近邻)\n",
        "- **目标**: 基于距离的分类\n",
        "- **实现**: 向量化距离计算\n",
        "- **关键**: 距离度量和K值选择\n",
        "\n",
        "### 4. Naive Bayes (朴素贝叶斯)\n",
        "- **目标**: 基于概率的分类\n",
        "- **实现**: 高斯分布假设\n",
        "- **关键**: 特征独立假设\n",
        "\n",
        "### 5. Decision Tree (决策树)\n",
        "- **目标**: 基于规则的分类\n",
        "- **实现**: 信息增益/Gini系数\n",
        "- **关键**: 过拟合控制和剪枝\n",
        "\n",
        "### 6. K-Means Clustering (K均值聚类)\n",
        "- **目标**: 无监督聚类\n",
        "- **实现**: 迭代优化质心\n",
        "- **关键**: 初始化方法和收敛判断\n",
        "\n",
        "### 7. PCA (主成分分析)\n",
        "- **目标**: 降维和特征提取\n",
        "- **实现**: 特征值分解\n",
        "- **关键**: 解释方差和成分选择\n",
        "\n",
        "### 8. Neural Network (神经网络)\n",
        "- **目标**: 非线性建模\n",
        "- **实现**: 前向/反向传播\n",
        "- **关键**: 梯度消失和初始化\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📝 核心算法编程挑战 (Core Algorithms Coding Challenges)\n",
        "\n",
        "以下每个cell都是一个独立的编程挑战，请根据要求完成代码实现。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 挑战1: 线性回归梯度下降 (Linear Regression with Gradient Descent)\n",
        "\n",
        "**要求 (Requirements):**\n",
        "1. 手动实现批量梯度下降 (Batch Gradient Descent)\n",
        "2. 实现随机梯度下降 (Stochastic Gradient Descent)  \n",
        "3. 添加L2正则化 (Ridge回归)\n",
        "4. 比较不同方法的收敛速度\n",
        "\n",
        "**提示 (Hints):**\n",
        "- 损失函数：MSE = (1/2m) * Σ(y_pred - y_true)²\n",
        "- 梯度：∇J = (1/m) * X^T * (X*θ - y)\n",
        "- L2正则化：J = MSE + λ * ||θ||²\n",
        "- 学习率调优很重要\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
