{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1acb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14d4fa2",
   "metadata": {},
   "source": [
    "## 🔹 1. ReLU（Rectified Linear Unit）\n",
    "\n",
    "**定义**\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(x)=\\max(0,x)\n",
    "$$\n",
    "\n",
    "**导数**\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\mathrm{ReLU}(x)=\n",
    "\\begin{cases}\n",
    "1, & x>0\\\\\\\\\n",
    "0, & x\\le 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59e27dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return torch.maximum(torch.tensor(0.0), x)\n",
    "\n",
    "x = torch.tensor([-1, 0, 1])\n",
    "print(relu(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb5281",
   "metadata": {},
   "source": [
    "## 🔹 2. Sigmoid（Logistic）\n",
    "\n",
    "**定义**\n",
    "\n",
    "$$\n",
    "\\sigma(x)=\\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "**导数**\n",
    "\n",
    "$$\n",
    "\\sigma'(x)=\\sigma(x)\\bigl(1-\\sigma(x)\\bigr)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf23a9",
   "metadata": {},
   "source": [
    "## 🔹 3. Tanh（双曲正切）\n",
    "\n",
    "**定义**\n",
    "\n",
    "$$\n",
    "\\tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\n",
    "$$\n",
    "\n",
    "**导数**\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\tanh(x)=1-\\tanh^2(x)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b3230",
   "metadata": {},
   "source": [
    "## 🔹 4. Softmax（多分类归一化）\n",
    "\n",
    "**定义**\n",
    "\n",
    "$$\n",
    "\\mathrm{Softmax}(z)_i=\\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_j}}\n",
    "$$\n",
    "\n",
    "**雅可比（导数）**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\,\\mathrm{Softmax}(z)_i}{\\partial z_j}\n",
    "=\\mathrm{Softmax}(z)_i\\bigl(\\delta_{ij}-\\mathrm{Softmax}(z)_j\\bigr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f446f7",
   "metadata": {},
   "source": [
    "## 🔹 5. Cross-Entropy（交叉熵损失）\n",
    "\n",
    "**定义（与 Softmax 搭配）**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE}=-\\sum_{i=1}^{K}y_i\\log \\hat y_i, \\quad \\hat y=\\mathrm{Softmax}(z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab1a59",
   "metadata": {},
   "source": [
    "## 🔹 6. MSE（均方误差）\n",
    "\n",
    "**定义**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{MSE}=\\frac{1}{N}\\sum_{i=1}^{N}(\\hat y_i-y_i)^2\n",
    "$$\n",
    "\n",
    "**对 \\(\\hat y_i\\) 的导数**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat y_i}=\\frac{2}{N}(\\hat y_i-y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6bd332",
   "metadata": {},
   "source": [
    "## 🔹 7. L1（平均绝对误差）\n",
    "\n",
    "**定义**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{L1}=\\frac{1}{N}\\sum_{i=1}^{N}|\\hat{y}_i-y_i|\n",
    "$$\n",
    "\n",
    "**对 $\\hat{y}_i$ 的（次）梯度**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i}=\n",
    "\\begin{cases}\n",
    "\\frac{1}{N}, & \\hat{y}_i>y_i\\\\\n",
    "-\\frac{1}{N}, & \\hat{y}_i<y_i\n",
    "\\end{cases}\n",
    "\\quad(\\hat{y}_i=y_i\\ \\text{处取次梯度})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c0959",
   "metadata": {},
   "source": [
    "## 🔹 8. Leaky ReLU\n",
    "\n",
    "**定义（$\\alpha\\in(0,1)$）**\n",
    "\n",
    "$$\n",
    "f(x)=\n",
    "\\begin{cases}\n",
    "x, & x>0\\\\\n",
    "\\alpha x, & x\\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**导数**\n",
    "\n",
    "$$\n",
    "f'(x)=\n",
    "\\begin{cases}\n",
    "1, & x>0\\\\\n",
    "\\alpha, & x\\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹（常用）Softmax + CrossEntropy 的 logits 梯度\n",
    "\n",
    "设 $\\hat{y}=\\mathrm{Softmax}(z)$，$\\mathcal{L}=-\\sum_i y_i\\log \\hat{y}_i$，则\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_i}=\\hat{y}_i-y_i\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_coding_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
