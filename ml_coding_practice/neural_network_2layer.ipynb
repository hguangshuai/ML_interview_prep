{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2层神经网络从零实现\n",
        "\n",
        "这个notebook将实现一个完整的2层神经网络，包括：\n",
        "- 前向传播: `Linear -> ReLU -> Linear -> Softmax`\n",
        "- 反向传播: 手动实现交叉熵损失的反向传播\n",
        "- 梯度检查: 数值验证梯度计算的正确性\n",
        "\n",
        "## 📚 学习目标\n",
        "- 理解神经网络的前向传播和反向传播\n",
        "- 掌握手动实现梯度计算\n",
        "- 学会梯度检查技术\n",
        "- 实现完整的训练循环\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 设置随机种子\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"导入完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 数据准备\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔍 Xavier初始化详解 (Xavier Initialization Explanation)\n",
        "\n",
        "### 什么是Xavier初始化？\n",
        "\n",
        "```python\n",
        "# 这两行代码的作用：\n",
        "torch.nn.init.xavier_uniform_(self.W1)  # 初始化第一层权重\n",
        "torch.nn.init.xavier_uniform_(self.W2)  # 初始化第二层权重\n",
        "```\n",
        "\n",
        "### 为什么需要Xavier初始化？\n",
        "\n",
        "**问题：**\n",
        "- **梯度消失 (Vanishing Gradients)**: 梯度在反向传播过程中逐渐变小，导致深层网络难以训练\n",
        "- **梯度爆炸 (Exploding Gradients)**: 梯度在反向传播过程中逐渐变大，导致训练不稳定\n",
        "\n",
        "**解决方案：**\n",
        "Xavier初始化通过控制权重分布的方差来保持信号在前向和反向传播过程中的强度。\n",
        "\n",
        "### Xavier初始化的数学原理\n",
        "\n",
        "**目标：** 保持每层输出的方差与输入的方差相近\n",
        "\n",
        "**公式：**\n",
        "- 对于均匀分布：`W ~ U(-a, a)`，其中 `a = sqrt(6/(fan_in + fan_out))`\n",
        "- 对于正态分布：`W ~ N(0, 2/(fan_in + fan_out))`\n",
        "\n",
        "**其中：**\n",
        "- `fan_in`: 输入神经元数量\n",
        "- `fan_out`: 输出神经元数量\n",
        "\n",
        "### 实际效果\n",
        "\n",
        "**不使用Xavier初始化：**\n",
        "- 权重可能过大或过小\n",
        "- 激活值可能饱和\n",
        "- 梯度传播不稳定\n",
        "\n",
        "**使用Xavier初始化：**\n",
        "- 权重分布合理\n",
        "- 激活值在有效范围内\n",
        "- 梯度传播稳定\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Xavier初始化效果可视化 (Xavier Initialization Visualization)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 设置参数\n",
        "fan_in = 2      # 输入神经元数量\n",
        "fan_out = 64    # 输出神经元数量\n",
        "num_samples = 10000\n",
        "\n",
        "# 1. 随机初始化（不使用Xavier）\n",
        "random_weights = torch.randn(fan_in, fan_out)\n",
        "\n",
        "# 2. Xavier初始化\n",
        "xavier_weights = torch.randn(fan_in, fan_out)\n",
        "torch.nn.init.xavier_uniform_(xavier_weights)\n",
        "\n",
        "# 3. 可视化权重分布\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 随机初始化的权重分布\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(random_weights.flatten().detach().numpy(), bins=50, alpha=0.7, color='red')\n",
        "plt.title('Random Initialization')\n",
        "plt.xlabel('Weight Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Xavier初始化的权重分布\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(xavier_weights.flatten().detach().numpy(), bins=50, alpha=0.7, color='blue')\n",
        "plt.title('Xavier Initialization')\n",
        "plt.xlabel('Weight Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 对比图\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(random_weights.flatten().detach().numpy(), bins=50, alpha=0.5, color='red', label='Random')\n",
        "plt.hist(xavier_weights.flatten().detach().numpy(), bins=50, alpha=0.5, color='blue', label='Xavier')\n",
        "plt.title('Comparison')\n",
        "plt.xlabel('Weight Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. 统计信息对比\n",
        "print(\"📊 权重统计信息对比:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"随机初始化:\")\n",
        "print(f\"  均值: {random_weights.mean().item():.4f}\")\n",
        "print(f\"  标准差: {random_weights.std().item():.4f}\")\n",
        "print(f\"  最小值: {random_weights.min().item():.4f}\")\n",
        "print(f\"  最大值: {random_weights.max().item():.4f}\")\n",
        "\n",
        "print(f\"\\nXavier初始化:\")\n",
        "print(f\"  均值: {xavier_weights.mean().item():.4f}\")\n",
        "print(f\"  标准差: {xavier_weights.std().item():.4f}\")\n",
        "print(f\"  最小值: {xavier_weights.min().item():.4f}\")\n",
        "print(f\"  最大值: {xavier_weights.max().item():.4f}\")\n",
        "\n",
        "# 5. 理论Xavier范围\n",
        "xavier_range = np.sqrt(6.0 / (fan_in + fan_out))\n",
        "print(f\"\\n🎯 理论Xavier范围: ±{xavier_range:.4f}\")\n",
        "print(f\"实际Xavier范围: ±{xavier_weights.abs().max().item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 生成分类数据\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=4,\n",
        "    n_redundant=0,\n",
        "    n_informative=4,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
        "print(f\"类别分布: {np.bincount(y)}\")\n",
        "\n",
        "# 划分训练集和测试集\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 标准化数据\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 转换为PyTorch张量\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "print(f\"训练集: {X_train_tensor.shape}, 测试集: {X_test_tensor.shape}\")\n",
        "\n",
        "# 可视化数据 (使用前两个特征)\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], \n",
        "                     c=y_train, cmap='viridis', alpha=0.7)\n",
        "plt.colorbar(scatter)\n",
        "plt.xlabel('特征 1')\n",
        "plt.ylabel('特征 2')\n",
        "plt.title('训练数据分布')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 2层神经网络实现\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwoLayerNN:\n",
        "    \"\"\"\n",
        "    2层神经网络实现\n",
        "    结构: Linear -> ReLU -> Linear -> Softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        # 网络参数\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # 初始化权重和偏置\n",
        "        self.W1 = torch.randn(input_size, hidden_size, requires_grad=True)\n",
        "        self.b1 = torch.zeros(hidden_size, requires_grad=True)\n",
        "        self.W2 = torch.randn(hidden_size, output_size, requires_grad=True)\n",
        "        self.b2 = torch.zeros(output_size, requires_grad=True)\n",
        "        \n",
        "        # Xavier初始化\n",
        "        torch.nn.init.xavier_uniform_(self.W1)\n",
        "        torch.nn.init.xavier_uniform_(self.W2)\n",
        "        \n",
        "        print(f\"网络结构: {input_size} -> {hidden_size} -> {output_size}\")\n",
        "        print(f\"学习率: {learning_rate}\")\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"前向传播\"\"\"\n",
        "        # 第一层: Linear -> ReLU\n",
        "        self.z1 = X @ self.W1 + self.b1\n",
        "        self.a1 = torch.relu(self.z1)\n",
        "        \n",
        "        # 第二层: Linear -> Softmax\n",
        "        self.z2 = self.a1 @ self.W2 + self.b2\n",
        "        \n",
        "        # Softmax with numerical stability\n",
        "        exp_scores = torch.exp(self.z2 - torch.max(self.z2, dim=1, keepdim=True)[0])\n",
        "        self.probs = exp_scores / torch.sum(exp_scores, dim=1, keepdim=True)\n",
        "        \n",
        "        return self.probs\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"反向传播\"\"\"\n",
        "        batch_size = X.size(0)\n",
        "        \n",
        "        # 计算交叉熵损失\n",
        "        # 将标签转换为one-hot编码\n",
        "        y_one_hot = torch.zeros_like(self.probs)\n",
        "        y_one_hot.scatter_(1, y.unsqueeze(1), 1)\n",
        "        \n",
        "        # 损失函数: -log(prob_correct_class)\n",
        "        loss = -torch.mean(torch.sum(y_one_hot * torch.log(self.probs + 1e-8), dim=1))\n",
        "        \n",
        "        # 反向传播计算梯度\n",
        "        # 输出层梯度\n",
        "        dz2 = (self.probs - y_one_hot) / batch_size\n",
        "        \n",
        "        # 第二层参数梯度\n",
        "        dW2 = self.a1.T @ dz2\n",
        "        db2 = torch.sum(dz2, dim=0)\n",
        "        \n",
        "        # 第一层激活梯度\n",
        "        da1 = dz2 @ self.W2.T\n",
        "        \n",
        "        # ReLU梯度\n",
        "        dz1 = da1 * (self.z1 > 0).float()\n",
        "        \n",
        "        # 第一层参数梯度\n",
        "        dW1 = X.T @ dz1\n",
        "        db1 = torch.sum(dz1, dim=0)\n",
        "        \n",
        "        # 更新参数\n",
        "        with torch.no_grad():\n",
        "            self.W1 -= self.learning_rate * dW1\n",
        "            self.b1 -= self.learning_rate * db1\n",
        "            self.W2 -= self.learning_rate * dW2\n",
        "            self.b2 -= self.learning_rate * db2\n",
        "        \n",
        "        return loss.item()\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"预测\"\"\"\n",
        "        with torch.no_grad():\n",
        "            probs = self.forward(X)\n",
        "            predictions = torch.argmax(probs, dim=1)\n",
        "        return predictions\n",
        "    \n",
        "    def accuracy(self, X, y):\n",
        "        \"\"\"计算准确率\"\"\"\n",
        "        predictions = self.predict(X)\n",
        "        return torch.mean((predictions == y).float()).item()\n",
        "\n",
        "# 创建网络实例\n",
        "model = TwoLayerNN(input_size=4, hidden_size=10, output_size=3, learning_rate=0.01)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练参数\n",
        "epochs = 1000\n",
        "print_every = 100\n",
        "\n",
        "# 训练循环\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "print(\"开始训练...\")\n",
        "for epoch in range(epochs):\n",
        "    # 前向传播\n",
        "    model.forward(X_train_tensor)\n",
        "    \n",
        "    # 反向传播\n",
        "    loss = model.backward(X_train_tensor, y_train_tensor)\n",
        "    \n",
        "    # 计算准确率\n",
        "    train_acc = model.accuracy(X_train_tensor, y_train_tensor)\n",
        "    test_acc = model.accuracy(X_test_tensor, y_test_tensor)\n",
        "    \n",
        "    # 记录指标\n",
        "    train_losses.append(loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "    \n",
        "    # 打印进度\n",
        "    if epoch % print_every == 0:\n",
        "        print(f\"Epoch {epoch:4d} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "print(f\"\\n训练完成！最终测试准确率: {test_accuracies[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 绘制训练曲线\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 损失曲线\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('训练损失')\n",
        "plt.grid(True)\n",
        "\n",
        "# 准确率曲线\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(train_accuracies, label='训练准确率')\n",
        "plt.plot(test_accuracies, label='测试准确率')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('准确率变化')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 混淆矩阵\n",
        "plt.subplot(1, 3, 3)\n",
        "predictions = model.predict(X_test_tensor)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test_tensor.numpy(), predictions.numpy())\n",
        "\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('混淆矩阵')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(np.unique(y_test)))\n",
        "plt.xticks(tick_marks, np.unique(y_test))\n",
        "plt.yticks(tick_marks, np.unique(y_test))\n",
        "\n",
        "# 添加数值标注\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in np.ndindex(cm.shape):\n",
        "    plt.text(j, i, format(cm[i, j], 'd'),\n",
        "             ha=\"center\", va=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"最终训练准确率: {train_accuracies[-1]:.4f}\")\n",
        "print(f\"最终测试准确率: {test_accuracies[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 梯度检查 (Gradient Checking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_check(model, X, y, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    梯度检查函数\n",
        "    通过数值微分验证解析梯度的正确性\n",
        "    \"\"\"\n",
        "    # 创建模型副本用于梯度检查\n",
        "    model_check = TwoLayerNN(model.input_size, model.hidden_size, model.output_size, model.learning_rate)\n",
        "    \n",
        "    # 复制参数\n",
        "    model_check.W1.data = model.W1.data.clone()\n",
        "    model_check.b1.data = model.b1.data.clone()\n",
        "    model_check.W2.data = model.W2.data.clone()\n",
        "    model_check.b2.data = model.b2.data.clone()\n",
        "    \n",
        "    # 前向传播计算解析梯度\n",
        "    model_check.forward(X)\n",
        "    loss_original = model_check.backward(X, y)\n",
        "    \n",
        "    # 获取解析梯度\n",
        "    grad_W1_analytic = model_check.W1.grad.clone()\n",
        "    grad_b1_analytic = model_check.b1.grad.clone()\n",
        "    grad_W2_analytic = model_check.W2.grad.clone()\n",
        "    grad_b2_analytic = model_check.b2.grad.clone()\n",
        "    \n",
        "    print(\"开始梯度检查...\")\n",
        "    \n",
        "    # 检查W1的梯度\n",
        "    grad_W1_numerical = torch.zeros_like(model_check.W1)\n",
        "    for i in range(model_check.W1.size(0)):\n",
        "        for j in range(model_check.W1.size(1)):\n",
        "            # 创建模型副本\n",
        "            model_plus = TwoLayerNN(model.input_size, model.hidden_size, model.output_size, model.learning_rate)\n",
        "            model_plus.W1.data = model_check.W1.data.clone()\n",
        "            model_plus.b1.data = model_check.b1.data.clone()\n",
        "            model_plus.W2.data = model_check.W2.data.clone()\n",
        "            model_plus.b2.data = model_check.b2.data.clone()\n",
        "            \n",
        "            # 增加epsilon\n",
        "            model_plus.W1.data[i, j] += epsilon\n",
        "            loss_plus = model_plus.backward(X, y)\n",
        "            \n",
        "            # 减少epsilon\n",
        "            model_minus = TwoLayerNN(model.input_size, model.hidden_size, model.output_size, model.learning_rate)\n",
        "            model_minus.W1.data = model_check.W1.data.clone()\n",
        "            model_minus.b1.data = model_check.b1.data.clone()\n",
        "            model_minus.W2.data = model_check.W2.data.clone()\n",
        "            model_minus.b2.data = model_check.b2.data.clone()\n",
        "            \n",
        "            model_minus.W1.data[i, j] -= epsilon\n",
        "            loss_minus = model_minus.backward(X, y)\n",
        "            \n",
        "            # 计算数值梯度\n",
        "            grad_W1_numerical[i, j] = (loss_plus - loss_minus) / (2 * epsilon)\n",
        "    \n",
        "    # 计算相对误差\n",
        "    diff_W1 = torch.abs(grad_W1_analytic - grad_W1_numerical)\n",
        "    rel_error_W1 = diff_W1 / (torch.abs(grad_W1_analytic) + torch.abs(grad_W1_numerical) + epsilon)\n",
        "    max_error_W1 = torch.max(rel_error_W1).item()\n",
        "    \n",
        "    print(f\"W1梯度检查:\")\n",
        "    print(f\"  解析梯度形状: {grad_W1_analytic.shape}\")\n",
        "    print(f\"  数值梯度形状: {grad_W1_numerical.shape}\")\n",
        "    print(f\"  最大相对误差: {max_error_W1:.2e}\")\n",
        "    print(f\"  梯度检查通过: {'✅' if max_error_W1 < 1e-5 else '❌'}\")\n",
        "    \n",
        "    return max_error_W1 < 1e-5\n",
        "\n",
        "# 使用小批量数据进行梯度检查\n",
        "X_small = X_train_tensor[:10]  # 使用前10个样本\n",
        "y_small = y_train_tensor[:10]\n",
        "\n",
        "gradient_check_passed = gradient_check(model, X_small, y_small)\n",
        "print(f\"\\n梯度检查结果: {'通过' if gradient_check_passed else '失败'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 与PyTorch内置模块对比\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 使用PyTorch内置模块构建相同结构的网络\n",
        "class PyTorchNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(PyTorchNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 创建PyTorch模型\n",
        "pytorch_model = PyTorchNN(4, 10, 3)\n",
        "\n",
        "# 复制我们手动实现的权重\n",
        "with torch.no_grad():\n",
        "    pytorch_model.fc1.weight.copy_(model.W1.T)  # 注意转置\n",
        "    pytorch_model.fc1.bias.copy_(model.b1)\n",
        "    pytorch_model.fc2.weight.copy_(model.W2.T)\n",
        "    pytorch_model.fc2.bias.copy_(model.b2)\n",
        "\n",
        "# 测试两个模型是否产生相同结果\n",
        "test_input = X_test_tensor[:5]\n",
        "manual_output = model.forward(test_input)\n",
        "pytorch_output = torch.softmax(pytorch_model(test_input), dim=1)\n",
        "\n",
        "print(\"手动实现 vs PyTorch内置模块对比:\")\n",
        "print(f\"手动实现输出形状: {manual_output.shape}\")\n",
        "print(f\"PyTorch输出形状: {pytorch_output.shape}\")\n",
        "print(f\"输出差异: {torch.max(torch.abs(manual_output - pytorch_output)).item():.2e}\")\n",
        "\n",
        "# 使用PyTorch训练\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(pytorch_model.parameters(), lr=0.01)\n",
        "\n",
        "pytorch_train_losses = []\n",
        "pytorch_train_accuracies = []\n",
        "\n",
        "print(\"\\n使用PyTorch训练相同模型...\")\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = pytorch_model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    pytorch_train_losses.append(loss.item())\n",
        "    \n",
        "    # 计算准确率\n",
        "    with torch.no_grad():\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        accuracy = (predicted == y_train_tensor).float().mean()\n",
        "        pytorch_train_accuracies.append(accuracy.item())\n",
        "    \n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Acc: {accuracy.item():.4f}\")\n",
        "\n",
        "# 比较最终性能\n",
        "manual_final_acc = model.accuracy(X_test_tensor, y_test_tensor)\n",
        "with torch.no_grad():\n",
        "    pytorch_test_output = pytorch_model(X_test_tensor)\n",
        "    _, pytorch_predicted = torch.max(pytorch_test_output, 1)\n",
        "    pytorch_final_acc = (pytorch_predicted == y_test_tensor).float().mean().item()\n",
        "\n",
        "print(f\"\\n最终测试准确率对比:\")\n",
        "print(f\"手动实现: {manual_final_acc:.4f}\")\n",
        "print(f\"PyTorch内置: {pytorch_final_acc:.4f}\")\n",
        "print(f\"差异: {abs(manual_final_acc - pytorch_final_acc):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 总结\n",
        "\n",
        "通过这个notebook，我们成功实现了：\n",
        "\n",
        "### ✅ 完成的功能\n",
        "1. **完整的2层神经网络**: Linear -> ReLU -> Linear -> Softmax\n",
        "2. **手动实现前向传播**: 包括数值稳定的Softmax\n",
        "3. **手动实现反向传播**: 交叉熵损失的反向传播\n",
        "4. **梯度检查**: 数值验证梯度计算的正确性\n",
        "5. **完整训练循环**: 包括损失和准确率监控\n",
        "6. **可视化结果**: 训练曲线、混淆矩阵\n",
        "7. **与PyTorch对比**: 验证实现正确性\n",
        "\n",
        "### 🎯 关键学习点\n",
        "- **数值稳定性**: Softmax计算中减去最大值避免溢出\n",
        "- **梯度计算**: 理解每一层的梯度传播过程\n",
        "- **梯度检查**: 验证手动实现的正确性\n",
        "- **Xavier初始化**: 帮助网络更好地训练\n",
        "\n",
        "### 🚀 扩展练习\n",
        "- 尝试不同的激活函数 (Tanh, LeakyReLU)\n",
        "- 添加Dropout层防止过拟合\n",
        "- 实现更多的层数\n",
        "- 添加批归一化层\n",
        "- 使用不同的优化器 (Adam, RMSprop)\n",
        "\n",
        "这个实现为理解深度学习的基础原理提供了坚实的基础！\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📝 神经网络编程挑战 (Neural Network Coding Challenges)\n",
        "\n",
        "以下每个cell都是一个独立的编程挑战，请根据要求完成代码实现。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 挑战1: 3层神经网络实现 (3-Layer Neural Network)\n",
        "\n",
        "**要求 (Requirements):**\n",
        "1. 修改网络为3层：Linear -> ReLU -> Linear -> ReLU -> Linear -> Softmax\n",
        "2. 实现完整的前向传播\n",
        "3. 实现完整的反向传播\n",
        "4. 比较与2层网络的性能差异\n",
        "\n",
        "**提示 (Hints):**\n",
        "- 需要添加第三层权重W3和偏置b3\n",
        "- 反向传播需要计算额外的梯度\n",
        "- 注意梯度传播的顺序：从输出层到输入层\n",
        "- 使用Xavier初始化所有层\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 在这里实现你的代码\n",
        "# Write your code here\n",
        "\n",
        "class ThreeLayerNN:\n",
        "    \"\"\"\n",
        "    3层神经网络实现\n",
        "    结构: Linear -> ReLU -> Linear -> ReLU -> Linear -> Softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, learning_rate=0.01):\n",
        "        # 初始化参数\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"前向传播\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"反向传播\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"预测\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def accuracy(self, X, y):\n",
        "        \"\"\"计算准确率\"\"\"\n",
        "        pass\n",
        "\n",
        "# 创建3层网络并训练\n",
        "# Create 3-layer network and train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 挑战2: Dropout层实现 (Dropout Layer Implementation)\n",
        "\n",
        "**要求 (Requirements):**\n",
        "1. 手动实现Dropout层的前向传播和反向传播\n",
        "2. 在训练时应用Dropout，测试时关闭\n",
        "3. 分析Dropout对过拟合的影响\n",
        "4. 比较不同Dropout率的效果\n",
        "\n",
        "**提示 (Hints):**\n",
        "- Dropout随机将部分神经元输出设为0\n",
        "- 训练时：输出 = input * mask / (1-p)，其中mask是伯努利分布\n",
        "- 测试时：输出 = input（不应用Dropout）\n",
        "- 反向传播时梯度也要乘以相同的mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 在这里实现你的代码\n",
        "# Write your code here\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    Dropout层实现\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, p=0.5):\n",
        "        # 初始化Dropout参数\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X, training=True):\n",
        "        \"\"\"前向传播\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"反向传播\"\"\"\n",
        "        pass\n",
        "\n",
        "class TwoLayerNNWithDropout:\n",
        "    \"\"\"\n",
        "    带Dropout的2层神经网络\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0.5, learning_rate=0.01):\n",
        "        # 初始化网络参数和Dropout层\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X, training=True):\n",
        "        \"\"\"前向传播\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"反向传播\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"预测\"\"\"\n",
        "        pass\n",
        "\n",
        "# 比较有无Dropout的性能\n",
        "# Compare performance with and without Dropout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 挑战3: 批归一化实现 (Batch Normalization Implementation)\n",
        "\n",
        "**要求 (Requirements):**\n",
        "1. 手动实现Batch Normalization层\n",
        "2. 在神经网络中集成批归一化\n",
        "3. 实现训练和推理模式的区别\n",
        "4. 分析批归一化对训练稳定性的影响\n",
        "\n",
        "**提示 (Hints):**\n",
        "- 批归一化公式：BN(x) = γ * (x - μ) / σ + β\n",
        "- 训练时使用当前批次的均值和方差\n",
        "- 推理时使用移动平均的均值和方差\n",
        "- γ和β是可学习参数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 在这里实现你的代码\n",
        "# Write your code here\n",
        "\n",
        "class BatchNorm:\n",
        "    \"\"\"\n",
        "    批归一化层实现\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        # 初始化参数\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X, training=True):\n",
        "        \"\"\"前向传播\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"反向传播\"\"\"\n",
        "        pass\n",
        "\n",
        "class TwoLayerNNWithBN:\n",
        "    \"\"\"\n",
        "    带批归一化的2层神经网络\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        # 初始化网络参数和批归一化层\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X, training=True):\n",
        "        \"\"\"前向传播\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"反向传播\"\"\"\n",
        "        pass\n",
        "\n",
        "# 比较有无批归一化的训练效果\n",
        "# Compare training effects with and without Batch Normalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 挑战4: Adam优化器实现 (Adam Optimizer Implementation)\n",
        "\n",
        "**要求 (Requirements):**\n",
        "1. 手动实现Adam优化器\n",
        "2. 替换SGD优化器\n",
        "3. 比较Adam和SGD的收敛速度\n",
        "4. 分析Adam的超参数影响\n",
        "\n",
        "**提示 (Hints):**\n",
        "- Adam结合了动量和自适应学习率\n",
        "- 需要维护一阶和二阶矩估计\n",
        "- 更新公式：θ = θ - α * m̂ / (√v̂ + ε)\n",
        "- 典型超参数：β1=0.9, β2=0.999, ε=1e-8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 在这里实现你的代码\n",
        "# Write your code here\n",
        "\n",
        "class AdamOptimizer:\n",
        "    \"\"\"\n",
        "    Adam优化器实现\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        # 初始化优化器参数\n",
        "        pass\n",
        "    \n",
        "    def step(self):\n",
        "        \"\"\"执行一步优化\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        \"\"\"清零梯度\"\"\"\n",
        "        pass\n",
        "\n",
        "class TwoLayerNNWithAdam:\n",
        "    \"\"\"\n",
        "    使用Adam优化器的2层神经网络\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.001):\n",
        "        # 初始化网络参数和Adam优化器\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"前向传播\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"反向传播\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def train_step(self, X, y):\n",
        "        \"\"\"训练一步\"\"\"\n",
        "        pass\n",
        "\n",
        "# 比较SGD和Adam的收敛速度\n",
        "# Compare convergence speed between SGD and Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 挑战5: 完整梯度检查 (Complete Gradient Checking)\n",
        "\n",
        "**要求 (Requirements):**\n",
        "1. 实现完整的梯度检查函数，检查所有参数\n",
        "2. 检查W1, b1, W2, b2的梯度\n",
        "3. 分析数值梯度和解析梯度的差异\n",
        "4. 可视化梯度检查结果\n",
        "\n",
        "**提示 (Hints):**\n",
        "- 对每个参数分别进行梯度检查\n",
        "- 使用中心差分公式提高精度\n",
        "- 计算相对误差：|grad_analytic - grad_numerical| / (|grad_analytic| + |grad_numerical| + eps)\n",
        "- 误差阈值通常设为1e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 在这里实现你的代码\n",
        "# Write your code here\n",
        "\n",
        "def complete_gradient_check(model, X, y, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    完整的梯度检查函数\n",
        "    检查所有参数：W1, b1, W2, b2\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. 计算解析梯度\n",
        "    # Compute analytical gradients\n",
        "    pass\n",
        "    \n",
        "    # 2. 计算数值梯度\n",
        "    # Compute numerical gradients\n",
        "    pass\n",
        "    \n",
        "    # 3. 比较梯度\n",
        "    # Compare gradients\n",
        "    pass\n",
        "    \n",
        "    # 4. 返回检查结果\n",
        "    # Return checking results\n",
        "    pass\n",
        "\n",
        "def visualize_gradient_check(results):\n",
        "    \"\"\"\n",
        "    可视化梯度检查结果\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# 执行完整的梯度检查\n",
        "# Perform complete gradient checking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 挑战6: PyTorch原生实现对比 (PyTorch Native Implementation Comparison)\n",
        "\n",
        "**要求 (Requirements):**\n",
        "1. 使用PyTorch的nn.Module实现相同的2层神经网络\n",
        "2. 使用nn.Linear, nn.ReLU, nn.CrossEntropyLoss等原生组件\n",
        "3. 使用torch.optim优化器进行训练\n",
        "4. 比较自定义实现和PyTorch原生实现的性能\n",
        "\n",
        "**提示 (Hints):**\n",
        "- 继承nn.Module类\n",
        "- 使用nn.Sequential或手动定义层\n",
        "- 使用torch.optim.SGD或torch.optim.Adam\n",
        "- 训练循环更简洁，不需要手动计算梯度\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 在这里实现你的代码\n",
        "# Write your code here\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PyTorchTwoLayerNN(nn.Module):\n",
        "    \"\"\"\n",
        "    使用PyTorch原生组件实现的2层神经网络\n",
        "    结构: Linear -> ReLU -> Linear -> Softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(PyTorchTwoLayerNN, self).__init__()\n",
        "        # 定义网络层\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"前向传播\"\"\"\n",
        "        pass\n",
        "\n",
        "def train_pytorch_model(model, X_train, y_train, X_val, y_val, epochs=1000, lr=0.01):\n",
        "    \"\"\"\n",
        "    使用PyTorch原生组件训练模型\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. 定义损失函数和优化器\n",
        "    # Define loss function and optimizer\n",
        "    pass\n",
        "    \n",
        "    # 2. 训练循环\n",
        "    # Training loop\n",
        "    pass\n",
        "    \n",
        "    # 3. 返回训练历史\n",
        "    # Return training history\n",
        "    pass\n",
        "\n",
        "def compare_implementations():\n",
        "    \"\"\"\n",
        "    比较自定义实现和PyTorch原生实现的性能\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. 创建两个模型\n",
        "    # Create two models\n",
        "    pass\n",
        "    \n",
        "    # 2. 训练两个模型\n",
        "    # Train both models\n",
        "    pass\n",
        "    \n",
        "    # 3. 比较性能\n",
        "    # Compare performance\n",
        "    pass\n",
        "    \n",
        "    # 4. 可视化对比结果\n",
        "    # Visualize comparison results\n",
        "    pass\n",
        "\n",
        "# 执行完整的PyTorch原生实现和对比\n",
        "# Execute complete PyTorch native implementation and comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 PyTorch原生实现参考 (PyTorch Native Implementation Reference)\n",
        "\n",
        "以下是一个完整的PyTorch原生实现示例，你可以参考这个来实现上面的挑战：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch原生实现参考代码 (Reference Implementation)\n",
        "# 这个代码展示了如何使用PyTorch原生组件实现相同的神经网络\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class PyTorchTwoLayerNN(nn.Module):\n",
        "    \"\"\"\n",
        "    使用PyTorch原生组件实现的2层神经网络\n",
        "    结构: Linear -> ReLU -> Linear -> Softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(PyTorchTwoLayerNN, self).__init__()\n",
        "        # 定义网络层\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),  # 第一层：线性变换\n",
        "            nn.ReLU(),                           # 激活函数\n",
        "            nn.Linear(hidden_size, output_size)  # 第二层：线性变换\n",
        "        )\n",
        "        \n",
        "        # Xavier初始化\n",
        "        for layer in self.network:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "                nn.init.zeros_(layer.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"前向传播\"\"\"\n",
        "        return self.network(x)\n",
        "\n",
        "def train_pytorch_model(model, X_train, y_train, X_val, y_val, epochs=1000, lr=0.01):\n",
        "    \"\"\"\n",
        "    使用PyTorch原生组件训练模型\n",
        "    \"\"\"\n",
        "    \n",
        "    # 定义损失函数和优化器\n",
        "    criterion = nn.CrossEntropyLoss()  # 交叉熵损失（包含Softmax）\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    \n",
        "    # 训练历史记录\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    \n",
        "    model.train()  # 设置为训练模式\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # 前向传播\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        \n",
        "        # 反向传播和优化\n",
        "        optimizer.zero_grad()  # 清零梯度\n",
        "        loss.backward()        # 计算梯度\n",
        "        optimizer.step()       # 更新参数\n",
        "        \n",
        "        # 记录训练损失\n",
        "        train_losses.append(loss.item())\n",
        "        \n",
        "        # 计算训练准确率\n",
        "        with torch.no_grad():\n",
        "            train_pred = torch.argmax(outputs, dim=1)\n",
        "            train_acc = torch.mean((train_pred == y_train).float()).item()\n",
        "            train_accuracies.append(train_acc)\n",
        "            \n",
        "            # 计算验证准确率\n",
        "            model.eval()  # 设置为评估模式\n",
        "            val_outputs = model(X_val)\n",
        "            val_pred = torch.argmax(val_outputs, dim=1)\n",
        "            val_acc = torch.mean((val_pred == y_val).float()).item()\n",
        "            val_accuracies.append(val_acc)\n",
        "            model.train()  # 重新设置为训练模式\n",
        "        \n",
        "        # 每100个epoch打印一次\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, '\n",
        "                  f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    \n",
        "    return train_losses, train_accuracies, val_accuracies\n",
        "\n",
        "# 使用示例\n",
        "print(\"🚀 创建PyTorch原生模型...\")\n",
        "pytorch_model = PyTorchTwoLayerNN(input_size=2, hidden_size=64, output_size=3)\n",
        "\n",
        "print(\"📊 开始训练PyTorch原生模型...\")\n",
        "pytorch_losses, pytorch_train_acc, pytorch_val_acc = train_pytorch_model(\n",
        "    pytorch_model, X_train, y_train, X_val, y_val, epochs=1000, lr=0.01\n",
        ")\n",
        "\n",
        "print(\"✅ PyTorch原生模型训练完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化PyTorch原生模型的训练结果\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 训练损失\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(pytorch_losses, 'b-', label='PyTorch Native')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 训练准确率\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(pytorch_train_acc, 'b-', label='PyTorch Native (Train)')\n",
        "plt.plot(pytorch_val_acc, 'b--', label='PyTorch Native (Val)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 决策边界\n",
        "plt.subplot(1, 3, 3)\n",
        "plot_decision_boundary(pytorch_model, X_test, y_test, title='PyTorch Native Decision Boundary')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 计算最终性能\n",
        "pytorch_model.eval()\n",
        "with torch.no_grad():\n",
        "    final_outputs = pytorch_model(X_test)\n",
        "    final_pred = torch.argmax(final_outputs, dim=1)\n",
        "    final_acc = torch.mean((final_pred == y_test).float()).item()\n",
        "\n",
        "print(f\"🎯 PyTorch原生模型最终测试准确率: {final_acc:.4f}\")\n",
        "\n",
        "# 显示模型结构\n",
        "print(\"\\n📋 PyTorch原生模型结构:\")\n",
        "print(pytorch_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔍 两种实现方式对比分析 (Implementation Comparison Analysis)\n",
        "\n",
        "### 自定义实现 vs PyTorch原生实现\n",
        "\n",
        "**自定义实现的优势：**\n",
        "- ✅ 深入理解每个计算步骤\n",
        "- ✅ 完全控制前向和反向传播过程\n",
        "- ✅ 学习底层数学原理\n",
        "- ✅ 面试中展示对算法的理解\n",
        "\n",
        "**PyTorch原生实现的优势：**\n",
        "- ✅ 代码简洁，开发效率高\n",
        "- ✅ 自动处理梯度计算和参数更新\n",
        "- ✅ 内置优化和数值稳定性\n",
        "- ✅ 易于扩展和维护\n",
        "- ✅ 生产环境推荐使用\n",
        "\n",
        "**学习建议：**\n",
        "1. **先掌握自定义实现** - 理解底层原理\n",
        "2. **再学习PyTorch原生** - 提高开发效率\n",
        "3. **对比两种方式** - 加深理解\n",
        "4. **实际项目用原生** - 保证代码质量\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 性能对比分析 (Performance Comparison Analysis)\n",
        "\n",
        "# 1. 训练时间对比\n",
        "import time\n",
        "\n",
        "print(\"⏱️  训练时间对比:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 自定义实现训练时间（假设已经训练过）\n",
        "custom_time = \"已训练完成\"\n",
        "print(f\"自定义实现: {custom_time}\")\n",
        "\n",
        "# PyTorch原生实现训练时间\n",
        "pytorch_time = \"已训练完成\"  \n",
        "print(f\"PyTorch原生: {pytorch_time}\")\n",
        "\n",
        "print(\"\\n📊 最终性能对比:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 假设自定义实现的最终准确率（需要从之前的训练结果获取）\n",
        "# 这里使用一个示例值，实际使用时需要从训练结果中获取\n",
        "custom_final_acc = 0.95  # 请替换为实际的自定义模型准确率\n",
        "\n",
        "print(f\"自定义实现测试准确率: {custom_final_acc:.4f}\")\n",
        "print(f\"PyTorch原生测试准确率: {final_acc:.4f}\")\n",
        "\n",
        "# 2. 代码复杂度对比\n",
        "print(\"\\n📝 代码复杂度对比:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"自定义实现:\")\n",
        "print(\"- 需要手动实现前向传播\")\n",
        "print(\"- 需要手动计算梯度\")\n",
        "print(\"- 需要手动更新参数\")\n",
        "print(\"- 代码量: ~100行\")\n",
        "\n",
        "print(\"\\nPyTorch原生实现:\")\n",
        "print(\"- 自动处理前向传播\")\n",
        "print(\"- 自动计算梯度\")\n",
        "print(\"- 自动更新参数\")\n",
        "print(\"- 代码量: ~30行\")\n",
        "\n",
        "print(\"\\n🎯 学习收获:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"✅ 理解了神经网络的底层原理\")\n",
        "print(\"✅ 掌握了梯度下降和反向传播\")\n",
        "print(\"✅ 学会了PyTorch的高效用法\")\n",
        "print(\"✅ 为面试和实际项目做好了准备\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml_coding_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
