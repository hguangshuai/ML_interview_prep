{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2å±‚ç¥ç»ç½‘ç»œä»é›¶å®ç°\n",
        "\n",
        "è¿™ä¸ªnotebookå°†å®ç°ä¸€ä¸ªå®Œæ•´çš„2å±‚ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬ï¼š\n",
        "- å‰å‘ä¼ æ’­: `Linear -> ReLU -> Linear -> Softmax`\n",
        "- åå‘ä¼ æ’­: æ‰‹åŠ¨å®ç°äº¤å‰ç†µæŸå¤±çš„åå‘ä¼ æ’­\n",
        "- æ¢¯åº¦æ£€æŸ¥: æ•°å€¼éªŒè¯æ¢¯åº¦è®¡ç®—çš„æ­£ç¡®æ€§\n",
        "\n",
        "## ğŸ“š å­¦ä¹ ç›®æ ‡\n",
        "- ç†è§£ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­\n",
        "- æŒæ¡æ‰‹åŠ¨å®ç°æ¢¯åº¦è®¡ç®—\n",
        "- å­¦ä¼šæ¢¯åº¦æ£€æŸ¥æŠ€æœ¯\n",
        "- å®ç°å®Œæ•´çš„è®­ç»ƒå¾ªç¯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# è®¾ç½®éšæœºç§å­\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"å¯¼å…¥å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. æ•°æ®å‡†å¤‡\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” Xavieråˆå§‹åŒ–è¯¦è§£ (Xavier Initialization Explanation)\n",
        "\n",
        "### ä»€ä¹ˆæ˜¯Xavieråˆå§‹åŒ–ï¼Ÿ\n",
        "\n",
        "```python\n",
        "# è¿™ä¸¤è¡Œä»£ç çš„ä½œç”¨ï¼š\n",
        "torch.nn.init.xavier_uniform_(self.W1)  # åˆå§‹åŒ–ç¬¬ä¸€å±‚æƒé‡\n",
        "torch.nn.init.xavier_uniform_(self.W2)  # åˆå§‹åŒ–ç¬¬äºŒå±‚æƒé‡\n",
        "```\n",
        "\n",
        "### ä¸ºä»€ä¹ˆéœ€è¦Xavieråˆå§‹åŒ–ï¼Ÿ\n",
        "\n",
        "**é—®é¢˜ï¼š**\n",
        "- **æ¢¯åº¦æ¶ˆå¤± (Vanishing Gradients)**: æ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€æ¸å˜å°ï¼Œå¯¼è‡´æ·±å±‚ç½‘ç»œéš¾ä»¥è®­ç»ƒ\n",
        "- **æ¢¯åº¦çˆ†ç‚¸ (Exploding Gradients)**: æ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€æ¸å˜å¤§ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®š\n",
        "\n",
        "**è§£å†³æ–¹æ¡ˆï¼š**\n",
        "Xavieråˆå§‹åŒ–é€šè¿‡æ§åˆ¶æƒé‡åˆ†å¸ƒçš„æ–¹å·®æ¥ä¿æŒä¿¡å·åœ¨å‰å‘å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„å¼ºåº¦ã€‚\n",
        "\n",
        "### Xavieråˆå§‹åŒ–çš„æ•°å­¦åŸç†\n",
        "\n",
        "**ç›®æ ‡ï¼š** ä¿æŒæ¯å±‚è¾“å‡ºçš„æ–¹å·®ä¸è¾“å…¥çš„æ–¹å·®ç›¸è¿‘\n",
        "\n",
        "**å…¬å¼ï¼š**\n",
        "- å¯¹äºå‡åŒ€åˆ†å¸ƒï¼š`W ~ U(-a, a)`ï¼Œå…¶ä¸­ `a = sqrt(6/(fan_in + fan_out))`\n",
        "- å¯¹äºæ­£æ€åˆ†å¸ƒï¼š`W ~ N(0, 2/(fan_in + fan_out))`\n",
        "\n",
        "**å…¶ä¸­ï¼š**\n",
        "- `fan_in`: è¾“å…¥ç¥ç»å…ƒæ•°é‡\n",
        "- `fan_out`: è¾“å‡ºç¥ç»å…ƒæ•°é‡\n",
        "\n",
        "### å®é™…æ•ˆæœ\n",
        "\n",
        "**ä¸ä½¿ç”¨Xavieråˆå§‹åŒ–ï¼š**\n",
        "- æƒé‡å¯èƒ½è¿‡å¤§æˆ–è¿‡å°\n",
        "- æ¿€æ´»å€¼å¯èƒ½é¥±å’Œ\n",
        "- æ¢¯åº¦ä¼ æ’­ä¸ç¨³å®š\n",
        "\n",
        "**ä½¿ç”¨Xavieråˆå§‹åŒ–ï¼š**\n",
        "- æƒé‡åˆ†å¸ƒåˆç†\n",
        "- æ¿€æ´»å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…\n",
        "- æ¢¯åº¦ä¼ æ’­ç¨³å®š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Xavieråˆå§‹åŒ–æ•ˆæœå¯è§†åŒ– (Xavier Initialization Visualization)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# è®¾ç½®å‚æ•°\n",
        "fan_in = 2      # è¾“å…¥ç¥ç»å…ƒæ•°é‡\n",
        "fan_out = 64    # è¾“å‡ºç¥ç»å…ƒæ•°é‡\n",
        "num_samples = 10000\n",
        "\n",
        "# 1. éšæœºåˆå§‹åŒ–ï¼ˆä¸ä½¿ç”¨Xavierï¼‰\n",
        "random_weights = torch.randn(fan_in, fan_out)\n",
        "\n",
        "# 2. Xavieråˆå§‹åŒ–\n",
        "xavier_weights = torch.randn(fan_in, fan_out)\n",
        "torch.nn.init.xavier_uniform_(xavier_weights)\n",
        "\n",
        "# 3. å¯è§†åŒ–æƒé‡åˆ†å¸ƒ\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# éšæœºåˆå§‹åŒ–çš„æƒé‡åˆ†å¸ƒ\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(random_weights.flatten().detach().numpy(), bins=50, alpha=0.7, color='red')\n",
        "plt.title('Random Initialization')\n",
        "plt.xlabel('Weight Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Xavieråˆå§‹åŒ–çš„æƒé‡åˆ†å¸ƒ\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(xavier_weights.flatten().detach().numpy(), bins=50, alpha=0.7, color='blue')\n",
        "plt.title('Xavier Initialization')\n",
        "plt.xlabel('Weight Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# å¯¹æ¯”å›¾\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(random_weights.flatten().detach().numpy(), bins=50, alpha=0.5, color='red', label='Random')\n",
        "plt.hist(xavier_weights.flatten().detach().numpy(), bins=50, alpha=0.5, color='blue', label='Xavier')\n",
        "plt.title('Comparison')\n",
        "plt.xlabel('Weight Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. ç»Ÿè®¡ä¿¡æ¯å¯¹æ¯”\n",
        "print(\"ğŸ“Š æƒé‡ç»Ÿè®¡ä¿¡æ¯å¯¹æ¯”:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"éšæœºåˆå§‹åŒ–:\")\n",
        "print(f\"  å‡å€¼: {random_weights.mean().item():.4f}\")\n",
        "print(f\"  æ ‡å‡†å·®: {random_weights.std().item():.4f}\")\n",
        "print(f\"  æœ€å°å€¼: {random_weights.min().item():.4f}\")\n",
        "print(f\"  æœ€å¤§å€¼: {random_weights.max().item():.4f}\")\n",
        "\n",
        "print(f\"\\nXavieråˆå§‹åŒ–:\")\n",
        "print(f\"  å‡å€¼: {xavier_weights.mean().item():.4f}\")\n",
        "print(f\"  æ ‡å‡†å·®: {xavier_weights.std().item():.4f}\")\n",
        "print(f\"  æœ€å°å€¼: {xavier_weights.min().item():.4f}\")\n",
        "print(f\"  æœ€å¤§å€¼: {xavier_weights.max().item():.4f}\")\n",
        "\n",
        "# 5. ç†è®ºXavierèŒƒå›´\n",
        "xavier_range = np.sqrt(6.0 / (fan_in + fan_out))\n",
        "print(f\"\\nğŸ¯ ç†è®ºXavierèŒƒå›´: Â±{xavier_range:.4f}\")\n",
        "print(f\"å®é™…XavierèŒƒå›´: Â±{xavier_weights.abs().max().item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç”Ÿæˆåˆ†ç±»æ•°æ®\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=4,\n",
        "    n_redundant=0,\n",
        "    n_informative=4,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}\")\n",
        "print(f\"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}\")\n",
        "\n",
        "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# æ ‡å‡†åŒ–æ•°æ®\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# è½¬æ¢ä¸ºPyTorchå¼ é‡\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "print(f\"è®­ç»ƒé›†: {X_train_tensor.shape}, æµ‹è¯•é›†: {X_test_tensor.shape}\")\n",
        "\n",
        "# å¯è§†åŒ–æ•°æ® (ä½¿ç”¨å‰ä¸¤ä¸ªç‰¹å¾)\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], \n",
        "                     c=y_train, cmap='viridis', alpha=0.7)\n",
        "plt.colorbar(scatter)\n",
        "plt.xlabel('ç‰¹å¾ 1')\n",
        "plt.ylabel('ç‰¹å¾ 2')\n",
        "plt.title('è®­ç»ƒæ•°æ®åˆ†å¸ƒ')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 2å±‚ç¥ç»ç½‘ç»œå®ç°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwoLayerNN:\n",
        "    \"\"\"\n",
        "    2å±‚ç¥ç»ç½‘ç»œå®ç°\n",
        "    ç»“æ„: Linear -> ReLU -> Linear -> Softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        # ç½‘ç»œå‚æ•°\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # åˆå§‹åŒ–æƒé‡å’Œåç½®\n",
        "        self.W1 = torch.randn(input_size, hidden_size, requires_grad=True)\n",
        "        self.b1 = torch.zeros(hidden_size, requires_grad=True)\n",
        "        self.W2 = torch.randn(hidden_size, output_size, requires_grad=True)\n",
        "        self.b2 = torch.zeros(output_size, requires_grad=True)\n",
        "        \n",
        "        # Xavieråˆå§‹åŒ–\n",
        "        torch.nn.init.xavier_uniform_(self.W1)\n",
        "        torch.nn.init.xavier_uniform_(self.W2)\n",
        "        \n",
        "        print(f\"ç½‘ç»œç»“æ„: {input_size} -> {hidden_size} -> {output_size}\")\n",
        "        print(f\"å­¦ä¹ ç‡: {learning_rate}\")\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n",
        "        # ç¬¬ä¸€å±‚: Linear -> ReLU\n",
        "        self.z1 = X @ self.W1 + self.b1\n",
        "        self.a1 = torch.relu(self.z1)\n",
        "        \n",
        "        # ç¬¬äºŒå±‚: Linear -> Softmax\n",
        "        self.z2 = self.a1 @ self.W2 + self.b2\n",
        "        \n",
        "        # Softmax with numerical stability\n",
        "        exp_scores = torch.exp(self.z2 - torch.max(self.z2, dim=1, keepdim=True)[0])\n",
        "        self.probs = exp_scores / torch.sum(exp_scores, dim=1, keepdim=True)\n",
        "        \n",
        "        return self.probs\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"åå‘ä¼ æ’­\"\"\"\n",
        "        batch_size = X.size(0)\n",
        "        \n",
        "        # è®¡ç®—äº¤å‰ç†µæŸå¤±\n",
        "        # å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç \n",
        "        y_one_hot = torch.zeros_like(self.probs)\n",
        "        y_one_hot.scatter_(1, y.unsqueeze(1), 1)\n",
        "        \n",
        "        # æŸå¤±å‡½æ•°: -log(prob_correct_class)\n",
        "        loss = -torch.mean(torch.sum(y_one_hot * torch.log(self.probs + 1e-8), dim=1))\n",
        "        \n",
        "        # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦\n",
        "        # è¾“å‡ºå±‚æ¢¯åº¦\n",
        "        dz2 = (self.probs - y_one_hot) / batch_size\n",
        "        \n",
        "        # ç¬¬äºŒå±‚å‚æ•°æ¢¯åº¦\n",
        "        dW2 = self.a1.T @ dz2\n",
        "        db2 = torch.sum(dz2, dim=0)\n",
        "        \n",
        "        # ç¬¬ä¸€å±‚æ¿€æ´»æ¢¯åº¦\n",
        "        da1 = dz2 @ self.W2.T\n",
        "        \n",
        "        # ReLUæ¢¯åº¦\n",
        "        dz1 = da1 * (self.z1 > 0).float()\n",
        "        \n",
        "        # ç¬¬ä¸€å±‚å‚æ•°æ¢¯åº¦\n",
        "        dW1 = X.T @ dz1\n",
        "        db1 = torch.sum(dz1, dim=0)\n",
        "        \n",
        "        # æ›´æ–°å‚æ•°\n",
        "        with torch.no_grad():\n",
        "            self.W1 -= self.learning_rate * dW1\n",
        "            self.b1 -= self.learning_rate * db1\n",
        "            self.W2 -= self.learning_rate * dW2\n",
        "            self.b2 -= self.learning_rate * db2\n",
        "        \n",
        "        return loss.item()\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"é¢„æµ‹\"\"\"\n",
        "        with torch.no_grad():\n",
        "            probs = self.forward(X)\n",
        "            predictions = torch.argmax(probs, dim=1)\n",
        "        return predictions\n",
        "    \n",
        "    def accuracy(self, X, y):\n",
        "        \"\"\"è®¡ç®—å‡†ç¡®ç‡\"\"\"\n",
        "        predictions = self.predict(X)\n",
        "        return torch.mean((predictions == y).float()).item()\n",
        "\n",
        "# åˆ›å»ºç½‘ç»œå®ä¾‹\n",
        "model = TwoLayerNN(input_size=4, hidden_size=10, output_size=3, learning_rate=0.01)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è®­ç»ƒå‚æ•°\n",
        "epochs = 1000\n",
        "print_every = 100\n",
        "\n",
        "# è®­ç»ƒå¾ªç¯\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
        "for epoch in range(epochs):\n",
        "    # å‰å‘ä¼ æ’­\n",
        "    model.forward(X_train_tensor)\n",
        "    \n",
        "    # åå‘ä¼ æ’­\n",
        "    loss = model.backward(X_train_tensor, y_train_tensor)\n",
        "    \n",
        "    # è®¡ç®—å‡†ç¡®ç‡\n",
        "    train_acc = model.accuracy(X_train_tensor, y_train_tensor)\n",
        "    test_acc = model.accuracy(X_test_tensor, y_test_tensor)\n",
        "    \n",
        "    # è®°å½•æŒ‡æ ‡\n",
        "    train_losses.append(loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "    \n",
        "    # æ‰“å°è¿›åº¦\n",
        "    if epoch % print_every == 0:\n",
        "        print(f\"Epoch {epoch:4d} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "print(f\"\\nè®­ç»ƒå®Œæˆï¼æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accuracies[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç»˜åˆ¶è®­ç»ƒæ›²çº¿\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# æŸå¤±æ›²çº¿\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('è®­ç»ƒæŸå¤±')\n",
        "plt.grid(True)\n",
        "\n",
        "# å‡†ç¡®ç‡æ›²çº¿\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(train_accuracies, label='è®­ç»ƒå‡†ç¡®ç‡')\n",
        "plt.plot(test_accuracies, label='æµ‹è¯•å‡†ç¡®ç‡')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('å‡†ç¡®ç‡å˜åŒ–')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# æ··æ·†çŸ©é˜µ\n",
        "plt.subplot(1, 3, 3)\n",
        "predictions = model.predict(X_test_tensor)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test_tensor.numpy(), predictions.numpy())\n",
        "\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('æ··æ·†çŸ©é˜µ')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(np.unique(y_test)))\n",
        "plt.xticks(tick_marks, np.unique(y_test))\n",
        "plt.yticks(tick_marks, np.unique(y_test))\n",
        "\n",
        "# æ·»åŠ æ•°å€¼æ ‡æ³¨\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in np.ndindex(cm.shape):\n",
        "    plt.text(j, i, format(cm[i, j], 'd'),\n",
        "             ha=\"center\", va=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {train_accuracies[-1]:.4f}\")\n",
        "print(f\"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accuracies[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. æ¢¯åº¦æ£€æŸ¥ (Gradient Checking)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_check(model, X, y, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    æ¢¯åº¦æ£€æŸ¥å‡½æ•°\n",
        "    é€šè¿‡æ•°å€¼å¾®åˆ†éªŒè¯è§£ææ¢¯åº¦çš„æ­£ç¡®æ€§\n",
        "    \"\"\"\n",
        "    # åˆ›å»ºæ¨¡å‹å‰¯æœ¬ç”¨äºæ¢¯åº¦æ£€æŸ¥\n",
        "    model_check = TwoLayerNN(model.input_size, model.hidden_size, model.output_size, model.learning_rate)\n",
        "    \n",
        "    # å¤åˆ¶å‚æ•°\n",
        "    model_check.W1.data = model.W1.data.clone()\n",
        "    model_check.b1.data = model.b1.data.clone()\n",
        "    model_check.W2.data = model.W2.data.clone()\n",
        "    model_check.b2.data = model.b2.data.clone()\n",
        "    \n",
        "    # å‰å‘ä¼ æ’­è®¡ç®—è§£ææ¢¯åº¦\n",
        "    model_check.forward(X)\n",
        "    loss_original = model_check.backward(X, y)\n",
        "    \n",
        "    # è·å–è§£ææ¢¯åº¦\n",
        "    grad_W1_analytic = model_check.W1.grad.clone()\n",
        "    grad_b1_analytic = model_check.b1.grad.clone()\n",
        "    grad_W2_analytic = model_check.W2.grad.clone()\n",
        "    grad_b2_analytic = model_check.b2.grad.clone()\n",
        "    \n",
        "    print(\"å¼€å§‹æ¢¯åº¦æ£€æŸ¥...\")\n",
        "    \n",
        "    # æ£€æŸ¥W1çš„æ¢¯åº¦\n",
        "    grad_W1_numerical = torch.zeros_like(model_check.W1)\n",
        "    for i in range(model_check.W1.size(0)):\n",
        "        for j in range(model_check.W1.size(1)):\n",
        "            # åˆ›å»ºæ¨¡å‹å‰¯æœ¬\n",
        "            model_plus = TwoLayerNN(model.input_size, model.hidden_size, model.output_size, model.learning_rate)\n",
        "            model_plus.W1.data = model_check.W1.data.clone()\n",
        "            model_plus.b1.data = model_check.b1.data.clone()\n",
        "            model_plus.W2.data = model_check.W2.data.clone()\n",
        "            model_plus.b2.data = model_check.b2.data.clone()\n",
        "            \n",
        "            # å¢åŠ epsilon\n",
        "            model_plus.W1.data[i, j] += epsilon\n",
        "            loss_plus = model_plus.backward(X, y)\n",
        "            \n",
        "            # å‡å°‘epsilon\n",
        "            model_minus = TwoLayerNN(model.input_size, model.hidden_size, model.output_size, model.learning_rate)\n",
        "            model_minus.W1.data = model_check.W1.data.clone()\n",
        "            model_minus.b1.data = model_check.b1.data.clone()\n",
        "            model_minus.W2.data = model_check.W2.data.clone()\n",
        "            model_minus.b2.data = model_check.b2.data.clone()\n",
        "            \n",
        "            model_minus.W1.data[i, j] -= epsilon\n",
        "            loss_minus = model_minus.backward(X, y)\n",
        "            \n",
        "            # è®¡ç®—æ•°å€¼æ¢¯åº¦\n",
        "            grad_W1_numerical[i, j] = (loss_plus - loss_minus) / (2 * epsilon)\n",
        "    \n",
        "    # è®¡ç®—ç›¸å¯¹è¯¯å·®\n",
        "    diff_W1 = torch.abs(grad_W1_analytic - grad_W1_numerical)\n",
        "    rel_error_W1 = diff_W1 / (torch.abs(grad_W1_analytic) + torch.abs(grad_W1_numerical) + epsilon)\n",
        "    max_error_W1 = torch.max(rel_error_W1).item()\n",
        "    \n",
        "    print(f\"W1æ¢¯åº¦æ£€æŸ¥:\")\n",
        "    print(f\"  è§£ææ¢¯åº¦å½¢çŠ¶: {grad_W1_analytic.shape}\")\n",
        "    print(f\"  æ•°å€¼æ¢¯åº¦å½¢çŠ¶: {grad_W1_numerical.shape}\")\n",
        "    print(f\"  æœ€å¤§ç›¸å¯¹è¯¯å·®: {max_error_W1:.2e}\")\n",
        "    print(f\"  æ¢¯åº¦æ£€æŸ¥é€šè¿‡: {'âœ…' if max_error_W1 < 1e-5 else 'âŒ'}\")\n",
        "    \n",
        "    return max_error_W1 < 1e-5\n",
        "\n",
        "# ä½¿ç”¨å°æ‰¹é‡æ•°æ®è¿›è¡Œæ¢¯åº¦æ£€æŸ¥\n",
        "X_small = X_train_tensor[:10]  # ä½¿ç”¨å‰10ä¸ªæ ·æœ¬\n",
        "y_small = y_train_tensor[:10]\n",
        "\n",
        "gradient_check_passed = gradient_check(model, X_small, y_small)\n",
        "print(f\"\\næ¢¯åº¦æ£€æŸ¥ç»“æœ: {'é€šè¿‡' if gradient_check_passed else 'å¤±è´¥'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ä¸PyTorchå†…ç½®æ¨¡å—å¯¹æ¯”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä½¿ç”¨PyTorchå†…ç½®æ¨¡å—æ„å»ºç›¸åŒç»“æ„çš„ç½‘ç»œ\n",
        "class PyTorchNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(PyTorchNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# åˆ›å»ºPyTorchæ¨¡å‹\n",
        "pytorch_model = PyTorchNN(4, 10, 3)\n",
        "\n",
        "# å¤åˆ¶æˆ‘ä»¬æ‰‹åŠ¨å®ç°çš„æƒé‡\n",
        "with torch.no_grad():\n",
        "    pytorch_model.fc1.weight.copy_(model.W1.T)  # æ³¨æ„è½¬ç½®\n",
        "    pytorch_model.fc1.bias.copy_(model.b1)\n",
        "    pytorch_model.fc2.weight.copy_(model.W2.T)\n",
        "    pytorch_model.fc2.bias.copy_(model.b2)\n",
        "\n",
        "# æµ‹è¯•ä¸¤ä¸ªæ¨¡å‹æ˜¯å¦äº§ç”Ÿç›¸åŒç»“æœ\n",
        "test_input = X_test_tensor[:5]\n",
        "manual_output = model.forward(test_input)\n",
        "pytorch_output = torch.softmax(pytorch_model(test_input), dim=1)\n",
        "\n",
        "print(\"æ‰‹åŠ¨å®ç° vs PyTorchå†…ç½®æ¨¡å—å¯¹æ¯”:\")\n",
        "print(f\"æ‰‹åŠ¨å®ç°è¾“å‡ºå½¢çŠ¶: {manual_output.shape}\")\n",
        "print(f\"PyTorchè¾“å‡ºå½¢çŠ¶: {pytorch_output.shape}\")\n",
        "print(f\"è¾“å‡ºå·®å¼‚: {torch.max(torch.abs(manual_output - pytorch_output)).item():.2e}\")\n",
        "\n",
        "# ä½¿ç”¨PyTorchè®­ç»ƒ\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(pytorch_model.parameters(), lr=0.01)\n",
        "\n",
        "pytorch_train_losses = []\n",
        "pytorch_train_accuracies = []\n",
        "\n",
        "print(\"\\nä½¿ç”¨PyTorchè®­ç»ƒç›¸åŒæ¨¡å‹...\")\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = pytorch_model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    pytorch_train_losses.append(loss.item())\n",
        "    \n",
        "    # è®¡ç®—å‡†ç¡®ç‡\n",
        "    with torch.no_grad():\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        accuracy = (predicted == y_train_tensor).float().mean()\n",
        "        pytorch_train_accuracies.append(accuracy.item())\n",
        "    \n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Acc: {accuracy.item():.4f}\")\n",
        "\n",
        "# æ¯”è¾ƒæœ€ç»ˆæ€§èƒ½\n",
        "manual_final_acc = model.accuracy(X_test_tensor, y_test_tensor)\n",
        "with torch.no_grad():\n",
        "    pytorch_test_output = pytorch_model(X_test_tensor)\n",
        "    _, pytorch_predicted = torch.max(pytorch_test_output, 1)\n",
        "    pytorch_final_acc = (pytorch_predicted == y_test_tensor).float().mean().item()\n",
        "\n",
        "print(f\"\\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”:\")\n",
        "print(f\"æ‰‹åŠ¨å®ç°: {manual_final_acc:.4f}\")\n",
        "print(f\"PyTorchå†…ç½®: {pytorch_final_acc:.4f}\")\n",
        "print(f\"å·®å¼‚: {abs(manual_final_acc - pytorch_final_acc):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. æ€»ç»“\n",
        "\n",
        "é€šè¿‡è¿™ä¸ªnotebookï¼Œæˆ‘ä»¬æˆåŠŸå®ç°äº†ï¼š\n",
        "\n",
        "### âœ… å®Œæˆçš„åŠŸèƒ½\n",
        "1. **å®Œæ•´çš„2å±‚ç¥ç»ç½‘ç»œ**: Linear -> ReLU -> Linear -> Softmax\n",
        "2. **æ‰‹åŠ¨å®ç°å‰å‘ä¼ æ’­**: åŒ…æ‹¬æ•°å€¼ç¨³å®šçš„Softmax\n",
        "3. **æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­**: äº¤å‰ç†µæŸå¤±çš„åå‘ä¼ æ’­\n",
        "4. **æ¢¯åº¦æ£€æŸ¥**: æ•°å€¼éªŒè¯æ¢¯åº¦è®¡ç®—çš„æ­£ç¡®æ€§\n",
        "5. **å®Œæ•´è®­ç»ƒå¾ªç¯**: åŒ…æ‹¬æŸå¤±å’Œå‡†ç¡®ç‡ç›‘æ§\n",
        "6. **å¯è§†åŒ–ç»“æœ**: è®­ç»ƒæ›²çº¿ã€æ··æ·†çŸ©é˜µ\n",
        "7. **ä¸PyTorchå¯¹æ¯”**: éªŒè¯å®ç°æ­£ç¡®æ€§\n",
        "\n",
        "### ğŸ¯ å…³é”®å­¦ä¹ ç‚¹\n",
        "- **æ•°å€¼ç¨³å®šæ€§**: Softmaxè®¡ç®—ä¸­å‡å»æœ€å¤§å€¼é¿å…æº¢å‡º\n",
        "- **æ¢¯åº¦è®¡ç®—**: ç†è§£æ¯ä¸€å±‚çš„æ¢¯åº¦ä¼ æ’­è¿‡ç¨‹\n",
        "- **æ¢¯åº¦æ£€æŸ¥**: éªŒè¯æ‰‹åŠ¨å®ç°çš„æ­£ç¡®æ€§\n",
        "- **Xavieråˆå§‹åŒ–**: å¸®åŠ©ç½‘ç»œæ›´å¥½åœ°è®­ç»ƒ\n",
        "\n",
        "### ğŸš€ æ‰©å±•ç»ƒä¹ \n",
        "- å°è¯•ä¸åŒçš„æ¿€æ´»å‡½æ•° (Tanh, LeakyReLU)\n",
        "- æ·»åŠ Dropoutå±‚é˜²æ­¢è¿‡æ‹Ÿåˆ\n",
        "- å®ç°æ›´å¤šçš„å±‚æ•°\n",
        "- æ·»åŠ æ‰¹å½’ä¸€åŒ–å±‚\n",
        "- ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨ (Adam, RMSprop)\n",
        "\n",
        "è¿™ä¸ªå®ç°ä¸ºç†è§£æ·±åº¦å­¦ä¹ çš„åŸºç¡€åŸç†æä¾›äº†åšå®çš„åŸºç¡€ï¼\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“ ç¥ç»ç½‘ç»œç¼–ç¨‹æŒ‘æˆ˜ (Neural Network Coding Challenges)\n",
        "\n",
        "ä»¥ä¸‹æ¯ä¸ªcelléƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ç¼–ç¨‹æŒ‘æˆ˜ï¼Œè¯·æ ¹æ®è¦æ±‚å®Œæˆä»£ç å®ç°ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æŒ‘æˆ˜1: 3å±‚ç¥ç»ç½‘ç»œå®ç° (3-Layer Neural Network)\n",
        "\n",
        "**è¦æ±‚ (Requirements):**\n",
        "1. ä¿®æ”¹ç½‘ç»œä¸º3å±‚ï¼šLinear -> ReLU -> Linear -> ReLU -> Linear -> Softmax\n",
        "2. å®ç°å®Œæ•´çš„å‰å‘ä¼ æ’­\n",
        "3. å®ç°å®Œæ•´çš„åå‘ä¼ æ’­\n",
        "4. æ¯”è¾ƒä¸2å±‚ç½‘ç»œçš„æ€§èƒ½å·®å¼‚\n",
        "\n",
        "**æç¤º (Hints):**\n",
        "- éœ€è¦æ·»åŠ ç¬¬ä¸‰å±‚æƒé‡W3å’Œåç½®b3\n",
        "- åå‘ä¼ æ’­éœ€è¦è®¡ç®—é¢å¤–çš„æ¢¯åº¦\n",
        "- æ³¨æ„æ¢¯åº¦ä¼ æ’­çš„é¡ºåºï¼šä»è¾“å‡ºå±‚åˆ°è¾“å…¥å±‚\n",
        "- ä½¿ç”¨Xavieråˆå§‹åŒ–æ‰€æœ‰å±‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åœ¨è¿™é‡Œå®ç°ä½ çš„ä»£ç \n",
        "# Write your code here\n",
        "\n",
        "class ThreeLayerNN:\n",
        "    \"\"\"\n",
        "    3å±‚ç¥ç»ç½‘ç»œå®ç°\n",
        "    ç»“æ„: Linear -> ReLU -> Linear -> ReLU -> Linear -> Softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, learning_rate=0.01):\n",
        "        # åˆå§‹åŒ–å‚æ•°\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"åå‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"é¢„æµ‹\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def accuracy(self, X, y):\n",
        "        \"\"\"è®¡ç®—å‡†ç¡®ç‡\"\"\"\n",
        "        pass\n",
        "\n",
        "# åˆ›å»º3å±‚ç½‘ç»œå¹¶è®­ç»ƒ\n",
        "# Create 3-layer network and train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æŒ‘æˆ˜2: Dropoutå±‚å®ç° (Dropout Layer Implementation)\n",
        "\n",
        "**è¦æ±‚ (Requirements):**\n",
        "1. æ‰‹åŠ¨å®ç°Dropoutå±‚çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­\n",
        "2. åœ¨è®­ç»ƒæ—¶åº”ç”¨Dropoutï¼Œæµ‹è¯•æ—¶å…³é—­\n",
        "3. åˆ†æDropoutå¯¹è¿‡æ‹Ÿåˆçš„å½±å“\n",
        "4. æ¯”è¾ƒä¸åŒDropoutç‡çš„æ•ˆæœ\n",
        "\n",
        "**æç¤º (Hints):**\n",
        "- Dropoutéšæœºå°†éƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºè®¾ä¸º0\n",
        "- è®­ç»ƒæ—¶ï¼šè¾“å‡º = input * mask / (1-p)ï¼Œå…¶ä¸­maskæ˜¯ä¼¯åŠªåˆ©åˆ†å¸ƒ\n",
        "- æµ‹è¯•æ—¶ï¼šè¾“å‡º = inputï¼ˆä¸åº”ç”¨Dropoutï¼‰\n",
        "- åå‘ä¼ æ’­æ—¶æ¢¯åº¦ä¹Ÿè¦ä¹˜ä»¥ç›¸åŒçš„mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åœ¨è¿™é‡Œå®ç°ä½ çš„ä»£ç \n",
        "# Write your code here\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    Dropoutå±‚å®ç°\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, p=0.5):\n",
        "        # åˆå§‹åŒ–Dropoutå‚æ•°\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X, training=True):\n",
        "        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"åå‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "\n",
        "class TwoLayerNNWithDropout:\n",
        "    \"\"\"\n",
        "    å¸¦Dropoutçš„2å±‚ç¥ç»ç½‘ç»œ\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0.5, learning_rate=0.01):\n",
        "        # åˆå§‹åŒ–ç½‘ç»œå‚æ•°å’ŒDropoutå±‚\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X, training=True):\n",
        "        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"åå‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"é¢„æµ‹\"\"\"\n",
        "        pass\n",
        "\n",
        "# æ¯”è¾ƒæœ‰æ— Dropoutçš„æ€§èƒ½\n",
        "# Compare performance with and without Dropout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æŒ‘æˆ˜3: æ‰¹å½’ä¸€åŒ–å®ç° (Batch Normalization Implementation)\n",
        "\n",
        "**è¦æ±‚ (Requirements):**\n",
        "1. æ‰‹åŠ¨å®ç°Batch Normalizationå±‚\n",
        "2. åœ¨ç¥ç»ç½‘ç»œä¸­é›†æˆæ‰¹å½’ä¸€åŒ–\n",
        "3. å®ç°è®­ç»ƒå’Œæ¨ç†æ¨¡å¼çš„åŒºåˆ«\n",
        "4. åˆ†ææ‰¹å½’ä¸€åŒ–å¯¹è®­ç»ƒç¨³å®šæ€§çš„å½±å“\n",
        "\n",
        "**æç¤º (Hints):**\n",
        "- æ‰¹å½’ä¸€åŒ–å…¬å¼ï¼šBN(x) = Î³ * (x - Î¼) / Ïƒ + Î²\n",
        "- è®­ç»ƒæ—¶ä½¿ç”¨å½“å‰æ‰¹æ¬¡çš„å‡å€¼å’Œæ–¹å·®\n",
        "- æ¨ç†æ—¶ä½¿ç”¨ç§»åŠ¨å¹³å‡çš„å‡å€¼å’Œæ–¹å·®\n",
        "- Î³å’ŒÎ²æ˜¯å¯å­¦ä¹ å‚æ•°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åœ¨è¿™é‡Œå®ç°ä½ çš„ä»£ç \n",
        "# Write your code here\n",
        "\n",
        "class BatchNorm:\n",
        "    \"\"\"\n",
        "    æ‰¹å½’ä¸€åŒ–å±‚å®ç°\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        # åˆå§‹åŒ–å‚æ•°\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X, training=True):\n",
        "        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"åå‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "\n",
        "class TwoLayerNNWithBN:\n",
        "    \"\"\"\n",
        "    å¸¦æ‰¹å½’ä¸€åŒ–çš„2å±‚ç¥ç»ç½‘ç»œ\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        # åˆå§‹åŒ–ç½‘ç»œå‚æ•°å’Œæ‰¹å½’ä¸€åŒ–å±‚\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X, training=True):\n",
        "        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"åå‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "\n",
        "# æ¯”è¾ƒæœ‰æ— æ‰¹å½’ä¸€åŒ–çš„è®­ç»ƒæ•ˆæœ\n",
        "# Compare training effects with and without Batch Normalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æŒ‘æˆ˜4: Adamä¼˜åŒ–å™¨å®ç° (Adam Optimizer Implementation)\n",
        "\n",
        "**è¦æ±‚ (Requirements):**\n",
        "1. æ‰‹åŠ¨å®ç°Adamä¼˜åŒ–å™¨\n",
        "2. æ›¿æ¢SGDä¼˜åŒ–å™¨\n",
        "3. æ¯”è¾ƒAdamå’ŒSGDçš„æ”¶æ•›é€Ÿåº¦\n",
        "4. åˆ†æAdamçš„è¶…å‚æ•°å½±å“\n",
        "\n",
        "**æç¤º (Hints):**\n",
        "- Adamç»“åˆäº†åŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡\n",
        "- éœ€è¦ç»´æŠ¤ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡\n",
        "- æ›´æ–°å…¬å¼ï¼šÎ¸ = Î¸ - Î± * mÌ‚ / (âˆšvÌ‚ + Îµ)\n",
        "- å…¸å‹è¶…å‚æ•°ï¼šÎ²1=0.9, Î²2=0.999, Îµ=1e-8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åœ¨è¿™é‡Œå®ç°ä½ çš„ä»£ç \n",
        "# Write your code here\n",
        "\n",
        "class AdamOptimizer:\n",
        "    \"\"\"\n",
        "    Adamä¼˜åŒ–å™¨å®ç°\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å‚æ•°\n",
        "        pass\n",
        "    \n",
        "    def step(self):\n",
        "        \"\"\"æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        \"\"\"æ¸…é›¶æ¢¯åº¦\"\"\"\n",
        "        pass\n",
        "\n",
        "class TwoLayerNNWithAdam:\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨Adamä¼˜åŒ–å™¨çš„2å±‚ç¥ç»ç½‘ç»œ\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.001):\n",
        "        # åˆå§‹åŒ–ç½‘ç»œå‚æ•°å’ŒAdamä¼˜åŒ–å™¨\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"åå‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def train_step(self, X, y):\n",
        "        \"\"\"è®­ç»ƒä¸€æ­¥\"\"\"\n",
        "        pass\n",
        "\n",
        "# æ¯”è¾ƒSGDå’ŒAdamçš„æ”¶æ•›é€Ÿåº¦\n",
        "# Compare convergence speed between SGD and Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æŒ‘æˆ˜5: å®Œæ•´æ¢¯åº¦æ£€æŸ¥ (Complete Gradient Checking)\n",
        "\n",
        "**è¦æ±‚ (Requirements):**\n",
        "1. å®ç°å®Œæ•´çš„æ¢¯åº¦æ£€æŸ¥å‡½æ•°ï¼Œæ£€æŸ¥æ‰€æœ‰å‚æ•°\n",
        "2. æ£€æŸ¥W1, b1, W2, b2çš„æ¢¯åº¦\n",
        "3. åˆ†ææ•°å€¼æ¢¯åº¦å’Œè§£ææ¢¯åº¦çš„å·®å¼‚\n",
        "4. å¯è§†åŒ–æ¢¯åº¦æ£€æŸ¥ç»“æœ\n",
        "\n",
        "**æç¤º (Hints):**\n",
        "- å¯¹æ¯ä¸ªå‚æ•°åˆ†åˆ«è¿›è¡Œæ¢¯åº¦æ£€æŸ¥\n",
        "- ä½¿ç”¨ä¸­å¿ƒå·®åˆ†å…¬å¼æé«˜ç²¾åº¦\n",
        "- è®¡ç®—ç›¸å¯¹è¯¯å·®ï¼š|grad_analytic - grad_numerical| / (|grad_analytic| + |grad_numerical| + eps)\n",
        "- è¯¯å·®é˜ˆå€¼é€šå¸¸è®¾ä¸º1e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åœ¨è¿™é‡Œå®ç°ä½ çš„ä»£ç \n",
        "# Write your code here\n",
        "\n",
        "def complete_gradient_check(model, X, y, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    å®Œæ•´çš„æ¢¯åº¦æ£€æŸ¥å‡½æ•°\n",
        "    æ£€æŸ¥æ‰€æœ‰å‚æ•°ï¼šW1, b1, W2, b2\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. è®¡ç®—è§£ææ¢¯åº¦\n",
        "    # Compute analytical gradients\n",
        "    pass\n",
        "    \n",
        "    # 2. è®¡ç®—æ•°å€¼æ¢¯åº¦\n",
        "    # Compute numerical gradients\n",
        "    pass\n",
        "    \n",
        "    # 3. æ¯”è¾ƒæ¢¯åº¦\n",
        "    # Compare gradients\n",
        "    pass\n",
        "    \n",
        "    # 4. è¿”å›æ£€æŸ¥ç»“æœ\n",
        "    # Return checking results\n",
        "    pass\n",
        "\n",
        "def visualize_gradient_check(results):\n",
        "    \"\"\"\n",
        "    å¯è§†åŒ–æ¢¯åº¦æ£€æŸ¥ç»“æœ\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# æ‰§è¡Œå®Œæ•´çš„æ¢¯åº¦æ£€æŸ¥\n",
        "# Perform complete gradient checking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æŒ‘æˆ˜6: PyTorchåŸç”Ÿå®ç°å¯¹æ¯” (PyTorch Native Implementation Comparison)\n",
        "\n",
        "**è¦æ±‚ (Requirements):**\n",
        "1. ä½¿ç”¨PyTorchçš„nn.Moduleå®ç°ç›¸åŒçš„2å±‚ç¥ç»ç½‘ç»œ\n",
        "2. ä½¿ç”¨nn.Linear, nn.ReLU, nn.CrossEntropyLossç­‰åŸç”Ÿç»„ä»¶\n",
        "3. ä½¿ç”¨torch.optimä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒ\n",
        "4. æ¯”è¾ƒè‡ªå®šä¹‰å®ç°å’ŒPyTorchåŸç”Ÿå®ç°çš„æ€§èƒ½\n",
        "\n",
        "**æç¤º (Hints):**\n",
        "- ç»§æ‰¿nn.Moduleç±»\n",
        "- ä½¿ç”¨nn.Sequentialæˆ–æ‰‹åŠ¨å®šä¹‰å±‚\n",
        "- ä½¿ç”¨torch.optim.SGDæˆ–torch.optim.Adam\n",
        "- è®­ç»ƒå¾ªç¯æ›´ç®€æ´ï¼Œä¸éœ€è¦æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åœ¨è¿™é‡Œå®ç°ä½ çš„ä»£ç \n",
        "# Write your code here\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PyTorchTwoLayerNN(nn.Module):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨PyTorchåŸç”Ÿç»„ä»¶å®ç°çš„2å±‚ç¥ç»ç½‘ç»œ\n",
        "    ç»“æ„: Linear -> ReLU -> Linear -> Softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(PyTorchTwoLayerNN, self).__init__()\n",
        "        # å®šä¹‰ç½‘ç»œå±‚\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n",
        "        pass\n",
        "\n",
        "def train_pytorch_model(model, X_train, y_train, X_val, y_val, epochs=1000, lr=0.01):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨PyTorchåŸç”Ÿç»„ä»¶è®­ç»ƒæ¨¡å‹\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
        "    # Define loss function and optimizer\n",
        "    pass\n",
        "    \n",
        "    # 2. è®­ç»ƒå¾ªç¯\n",
        "    # Training loop\n",
        "    pass\n",
        "    \n",
        "    # 3. è¿”å›è®­ç»ƒå†å²\n",
        "    # Return training history\n",
        "    pass\n",
        "\n",
        "def compare_implementations():\n",
        "    \"\"\"\n",
        "    æ¯”è¾ƒè‡ªå®šä¹‰å®ç°å’ŒPyTorchåŸç”Ÿå®ç°çš„æ€§èƒ½\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. åˆ›å»ºä¸¤ä¸ªæ¨¡å‹\n",
        "    # Create two models\n",
        "    pass\n",
        "    \n",
        "    # 2. è®­ç»ƒä¸¤ä¸ªæ¨¡å‹\n",
        "    # Train both models\n",
        "    pass\n",
        "    \n",
        "    # 3. æ¯”è¾ƒæ€§èƒ½\n",
        "    # Compare performance\n",
        "    pass\n",
        "    \n",
        "    # 4. å¯è§†åŒ–å¯¹æ¯”ç»“æœ\n",
        "    # Visualize comparison results\n",
        "    pass\n",
        "\n",
        "# æ‰§è¡Œå®Œæ•´çš„PyTorchåŸç”Ÿå®ç°å’Œå¯¹æ¯”\n",
        "# Execute complete PyTorch native implementation and comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š PyTorchåŸç”Ÿå®ç°å‚è€ƒ (PyTorch Native Implementation Reference)\n",
        "\n",
        "ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´çš„PyTorchåŸç”Ÿå®ç°ç¤ºä¾‹ï¼Œä½ å¯ä»¥å‚è€ƒè¿™ä¸ªæ¥å®ç°ä¸Šé¢çš„æŒ‘æˆ˜ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorchåŸç”Ÿå®ç°å‚è€ƒä»£ç  (Reference Implementation)\n",
        "# è¿™ä¸ªä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨PyTorchåŸç”Ÿç»„ä»¶å®ç°ç›¸åŒçš„ç¥ç»ç½‘ç»œ\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class PyTorchTwoLayerNN(nn.Module):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨PyTorchåŸç”Ÿç»„ä»¶å®ç°çš„2å±‚ç¥ç»ç½‘ç»œ\n",
        "    ç»“æ„: Linear -> ReLU -> Linear -> Softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(PyTorchTwoLayerNN, self).__init__()\n",
        "        # å®šä¹‰ç½‘ç»œå±‚\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),  # ç¬¬ä¸€å±‚ï¼šçº¿æ€§å˜æ¢\n",
        "            nn.ReLU(),                           # æ¿€æ´»å‡½æ•°\n",
        "            nn.Linear(hidden_size, output_size)  # ç¬¬äºŒå±‚ï¼šçº¿æ€§å˜æ¢\n",
        "        )\n",
        "        \n",
        "        # Xavieråˆå§‹åŒ–\n",
        "        for layer in self.network:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "                nn.init.zeros_(layer.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n",
        "        return self.network(x)\n",
        "\n",
        "def train_pytorch_model(model, X_train, y_train, X_val, y_val, epochs=1000, lr=0.01):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨PyTorchåŸç”Ÿç»„ä»¶è®­ç»ƒæ¨¡å‹\n",
        "    \"\"\"\n",
        "    \n",
        "    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
        "    criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±ï¼ˆåŒ…å«Softmaxï¼‰\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    \n",
        "    # è®­ç»ƒå†å²è®°å½•\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    \n",
        "    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # å‰å‘ä¼ æ’­\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        \n",
        "        # åå‘ä¼ æ’­å’Œä¼˜åŒ–\n",
        "        optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦\n",
        "        loss.backward()        # è®¡ç®—æ¢¯åº¦\n",
        "        optimizer.step()       # æ›´æ–°å‚æ•°\n",
        "        \n",
        "        # è®°å½•è®­ç»ƒæŸå¤±\n",
        "        train_losses.append(loss.item())\n",
        "        \n",
        "        # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡\n",
        "        with torch.no_grad():\n",
        "            train_pred = torch.argmax(outputs, dim=1)\n",
        "            train_acc = torch.mean((train_pred == y_train).float()).item()\n",
        "            train_accuracies.append(train_acc)\n",
        "            \n",
        "            # è®¡ç®—éªŒè¯å‡†ç¡®ç‡\n",
        "            model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
        "            val_outputs = model(X_val)\n",
        "            val_pred = torch.argmax(val_outputs, dim=1)\n",
        "            val_acc = torch.mean((val_pred == y_val).float()).item()\n",
        "            val_accuracies.append(val_acc)\n",
        "            model.train()  # é‡æ–°è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼\n",
        "        \n",
        "        # æ¯100ä¸ªepochæ‰“å°ä¸€æ¬¡\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, '\n",
        "                  f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    \n",
        "    return train_losses, train_accuracies, val_accuracies\n",
        "\n",
        "# ä½¿ç”¨ç¤ºä¾‹\n",
        "print(\"ğŸš€ åˆ›å»ºPyTorchåŸç”Ÿæ¨¡å‹...\")\n",
        "pytorch_model = PyTorchTwoLayerNN(input_size=2, hidden_size=64, output_size=3)\n",
        "\n",
        "print(\"ğŸ“Š å¼€å§‹è®­ç»ƒPyTorchåŸç”Ÿæ¨¡å‹...\")\n",
        "pytorch_losses, pytorch_train_acc, pytorch_val_acc = train_pytorch_model(\n",
        "    pytorch_model, X_train, y_train, X_val, y_val, epochs=1000, lr=0.01\n",
        ")\n",
        "\n",
        "print(\"âœ… PyTorchåŸç”Ÿæ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯è§†åŒ–PyTorchåŸç”Ÿæ¨¡å‹çš„è®­ç»ƒç»“æœ\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# è®­ç»ƒæŸå¤±\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(pytorch_losses, 'b-', label='PyTorch Native')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# è®­ç»ƒå‡†ç¡®ç‡\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(pytorch_train_acc, 'b-', label='PyTorch Native (Train)')\n",
        "plt.plot(pytorch_val_acc, 'b--', label='PyTorch Native (Val)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# å†³ç­–è¾¹ç•Œ\n",
        "plt.subplot(1, 3, 3)\n",
        "plot_decision_boundary(pytorch_model, X_test, y_test, title='PyTorch Native Decision Boundary')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# è®¡ç®—æœ€ç»ˆæ€§èƒ½\n",
        "pytorch_model.eval()\n",
        "with torch.no_grad():\n",
        "    final_outputs = pytorch_model(X_test)\n",
        "    final_pred = torch.argmax(final_outputs, dim=1)\n",
        "    final_acc = torch.mean((final_pred == y_test).float()).item()\n",
        "\n",
        "print(f\"ğŸ¯ PyTorchåŸç”Ÿæ¨¡å‹æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_acc:.4f}\")\n",
        "\n",
        "# æ˜¾ç¤ºæ¨¡å‹ç»“æ„\n",
        "print(\"\\nğŸ“‹ PyTorchåŸç”Ÿæ¨¡å‹ç»“æ„:\")\n",
        "print(pytorch_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” ä¸¤ç§å®ç°æ–¹å¼å¯¹æ¯”åˆ†æ (Implementation Comparison Analysis)\n",
        "\n",
        "### è‡ªå®šä¹‰å®ç° vs PyTorchåŸç”Ÿå®ç°\n",
        "\n",
        "**è‡ªå®šä¹‰å®ç°çš„ä¼˜åŠ¿ï¼š**\n",
        "- âœ… æ·±å…¥ç†è§£æ¯ä¸ªè®¡ç®—æ­¥éª¤\n",
        "- âœ… å®Œå…¨æ§åˆ¶å‰å‘å’Œåå‘ä¼ æ’­è¿‡ç¨‹\n",
        "- âœ… å­¦ä¹ åº•å±‚æ•°å­¦åŸç†\n",
        "- âœ… é¢è¯•ä¸­å±•ç¤ºå¯¹ç®—æ³•çš„ç†è§£\n",
        "\n",
        "**PyTorchåŸç”Ÿå®ç°çš„ä¼˜åŠ¿ï¼š**\n",
        "- âœ… ä»£ç ç®€æ´ï¼Œå¼€å‘æ•ˆç‡é«˜\n",
        "- âœ… è‡ªåŠ¨å¤„ç†æ¢¯åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°\n",
        "- âœ… å†…ç½®ä¼˜åŒ–å’Œæ•°å€¼ç¨³å®šæ€§\n",
        "- âœ… æ˜“äºæ‰©å±•å’Œç»´æŠ¤\n",
        "- âœ… ç”Ÿäº§ç¯å¢ƒæ¨èä½¿ç”¨\n",
        "\n",
        "**å­¦ä¹ å»ºè®®ï¼š**\n",
        "1. **å…ˆæŒæ¡è‡ªå®šä¹‰å®ç°** - ç†è§£åº•å±‚åŸç†\n",
        "2. **å†å­¦ä¹ PyTorchåŸç”Ÿ** - æé«˜å¼€å‘æ•ˆç‡\n",
        "3. **å¯¹æ¯”ä¸¤ç§æ–¹å¼** - åŠ æ·±ç†è§£\n",
        "4. **å®é™…é¡¹ç›®ç”¨åŸç”Ÿ** - ä¿è¯ä»£ç è´¨é‡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ€§èƒ½å¯¹æ¯”åˆ†æ (Performance Comparison Analysis)\n",
        "\n",
        "# 1. è®­ç»ƒæ—¶é—´å¯¹æ¯”\n",
        "import time\n",
        "\n",
        "print(\"â±ï¸  è®­ç»ƒæ—¶é—´å¯¹æ¯”:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# è‡ªå®šä¹‰å®ç°è®­ç»ƒæ—¶é—´ï¼ˆå‡è®¾å·²ç»è®­ç»ƒè¿‡ï¼‰\n",
        "custom_time = \"å·²è®­ç»ƒå®Œæˆ\"\n",
        "print(f\"è‡ªå®šä¹‰å®ç°: {custom_time}\")\n",
        "\n",
        "# PyTorchåŸç”Ÿå®ç°è®­ç»ƒæ—¶é—´\n",
        "pytorch_time = \"å·²è®­ç»ƒå®Œæˆ\"  \n",
        "print(f\"PyTorchåŸç”Ÿ: {pytorch_time}\")\n",
        "\n",
        "print(\"\\nğŸ“Š æœ€ç»ˆæ€§èƒ½å¯¹æ¯”:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# å‡è®¾è‡ªå®šä¹‰å®ç°çš„æœ€ç»ˆå‡†ç¡®ç‡ï¼ˆéœ€è¦ä»ä¹‹å‰çš„è®­ç»ƒç»“æœè·å–ï¼‰\n",
        "# è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç¤ºä¾‹å€¼ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦ä»è®­ç»ƒç»“æœä¸­è·å–\n",
        "custom_final_acc = 0.95  # è¯·æ›¿æ¢ä¸ºå®é™…çš„è‡ªå®šä¹‰æ¨¡å‹å‡†ç¡®ç‡\n",
        "\n",
        "print(f\"è‡ªå®šä¹‰å®ç°æµ‹è¯•å‡†ç¡®ç‡: {custom_final_acc:.4f}\")\n",
        "print(f\"PyTorchåŸç”Ÿæµ‹è¯•å‡†ç¡®ç‡: {final_acc:.4f}\")\n",
        "\n",
        "# 2. ä»£ç å¤æ‚åº¦å¯¹æ¯”\n",
        "print(\"\\nğŸ“ ä»£ç å¤æ‚åº¦å¯¹æ¯”:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"è‡ªå®šä¹‰å®ç°:\")\n",
        "print(\"- éœ€è¦æ‰‹åŠ¨å®ç°å‰å‘ä¼ æ’­\")\n",
        "print(\"- éœ€è¦æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦\")\n",
        "print(\"- éœ€è¦æ‰‹åŠ¨æ›´æ–°å‚æ•°\")\n",
        "print(\"- ä»£ç é‡: ~100è¡Œ\")\n",
        "\n",
        "print(\"\\nPyTorchåŸç”Ÿå®ç°:\")\n",
        "print(\"- è‡ªåŠ¨å¤„ç†å‰å‘ä¼ æ’­\")\n",
        "print(\"- è‡ªåŠ¨è®¡ç®—æ¢¯åº¦\")\n",
        "print(\"- è‡ªåŠ¨æ›´æ–°å‚æ•°\")\n",
        "print(\"- ä»£ç é‡: ~30è¡Œ\")\n",
        "\n",
        "print(\"\\nğŸ¯ å­¦ä¹ æ”¶è·:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"âœ… ç†è§£äº†ç¥ç»ç½‘ç»œçš„åº•å±‚åŸç†\")\n",
        "print(\"âœ… æŒæ¡äº†æ¢¯åº¦ä¸‹é™å’Œåå‘ä¼ æ’­\")\n",
        "print(\"âœ… å­¦ä¼šäº†PyTorchçš„é«˜æ•ˆç”¨æ³•\")\n",
        "print(\"âœ… ä¸ºé¢è¯•å’Œå®é™…é¡¹ç›®åšå¥½äº†å‡†å¤‡\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml_coding_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
