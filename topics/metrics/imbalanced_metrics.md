# è¯„ä¼°æŒ‡æ ‡ä¸“é¢˜ - è¯¦ç»†ç­”æ¡ˆ

## é—®é¢˜: æ ‡ç­¾ä¸å¹³è¡¡æ—¶ç”¨ä»€ä¹ˆè¯„ä¼°æŒ‡æ ‡

### ğŸ¯ ä¸­æ–‡ç†è§£ (ä¾¿äºè®°å¿†)

#### æ ‡ç­¾ä¸å¹³è¡¡ = "å°‘æ•°æ´¾ vs å¤šæ•°æ´¾"
æƒ³è±¡ä¸€ä¸ªç­çº§è€ƒè¯•ï¼š
- **ä¸å¹³è¡¡æ•°æ®**ï¼š90%å­¦ç”ŸåŠæ ¼ï¼Œ10%ä¸åŠæ ¼
- **é—®é¢˜**ï¼šå¦‚æœç”¨å‡†ç¡®ç‡ï¼Œæ¨¡å‹åªè¦é¢„æµ‹æ‰€æœ‰äººéƒ½åŠæ ¼å°±èƒ½è¾¾åˆ°90%å‡†ç¡®ç‡
- **ç»“æœ**ï¼šæ¨¡å‹çœ‹èµ·æ¥å¾ˆ"èªæ˜"ï¼Œå®é™…ä¸Šä»€ä¹ˆéƒ½æ²¡å­¦åˆ°

#### ä¸ºä»€ä¹ˆå‡†ç¡®ç‡ä¸é€‚ç”¨ï¼Ÿ
- **å‡†ç¡®ç‡é™·é˜±**ï¼šåœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šï¼Œå‡†ç¡®ç‡ä¼šè¯¯å¯¼
- **å°‘æ•°ç±»è¢«å¿½ç•¥**ï¼šæ¨¡å‹å€¾å‘äºé¢„æµ‹å¤šæ•°ç±»
- **éœ€è¦ä¸“é—¨æŒ‡æ ‡**ï¼šå…³æ³¨å°‘æ•°ç±»çš„è¡¨ç°

### ğŸ¤ ç›´æ¥é¢è¯•å›ç­” (Direct Interview Answer)

**For imbalanced datasets, I avoid accuracy and instead use metrics that focus on the minority class performance. The key metrics are Precision, Recall, F1-score, and especially the F1-score for the positive class, along with AUC-ROC and AUC-PR.**

**Precision and Recall are crucial because they directly measure how well the model identifies the minority class. F1-score balances both, while AUC-ROC shows overall ranking ability and AUC-PR is particularly useful for highly imbalanced data.**

**I also use stratified sampling and class weights during training to ensure the model doesn't ignore the minority class. The Matthews Correlation Coefficient (MCC) is another robust metric that works well across all class distributions.**

**In practice, I focus on the minority class metrics and use techniques like SMOTE or cost-sensitive learning to address the imbalance during model training, not just evaluation.**

---

### ğŸ“ è‹±æ–‡æ ‡å‡†é¢è¯•ç­”æ¡ˆ (English Interview Answer)

#### 1. Why Accuracy Fails in Imbalanced Data

**Accuracy Problem:**
```python
# Example: 95% negative, 5% positive
# Naive model: predict all as negative
# Accuracy = 95% (misleading!)

def accuracy_problem_example():
    """Demonstrate accuracy problem in imbalanced data"""
    # Simulate imbalanced dataset
    n_negative = 950
    n_positive = 50
    total = n_negative + n_positive
    
    # Naive model: predict all as negative
    naive_accuracy = n_negative / total  # 95%
    
    # Good model: correctly identifies 40/50 positives
    good_model_accuracy = (n_negative + 40) / total  # 99%
    
    print(f"Naive model accuracy: {naive_accuracy:.1%}")
    print(f"Good model accuracy: {good_model_accuracy:.1%}")
    print("Problem: Small difference despite huge performance gap!")
```

#### 2. Recommended Metrics for Imbalanced Data

**A. Precision and Recall**
```python
# Focus on minority class (positive class)
Precision = TP / (TP + FP)  # How accurate are positive predictions?
Recall = TP / (TP + FN)     # How many positives were found?

# These metrics directly measure minority class performance
```

**B. F1-Score**
```python
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

# Harmonic mean balances precision and recall
# Particularly useful when both are important
```

**C. AUC-ROC (Area Under ROC Curve)**
```python
# Measures ranking ability across all thresholds
# Robust to class imbalance
# Range: 0-1, where 0.5 = random, 1.0 = perfect

def auc_roc_interpretation():
    """Interpret AUC-ROC for imbalanced data"""
    interpretations = {
        0.9: "Excellent ranking ability",
        0.8: "Good ranking ability", 
        0.7: "Fair ranking ability",
        0.6: "Poor ranking ability",
        0.5: "No better than random"
    }
    return interpretations
```

**D. AUC-PR (Area Under Precision-Recall Curve)**
```python
# More sensitive to class imbalance than AUC-ROC
# Better metric when positive class is rare
# Focuses on precision-recall trade-off

def auc_pr_vs_auc_roc():
    """Compare AUC-PR vs AUC-ROC for imbalanced data"""
    return {
        "AUC-ROC": "Good for balanced data, can be optimistic for imbalanced",
        "AUC-PR": "Better for imbalanced data, more realistic performance"
    }
```

**E. Matthews Correlation Coefficient (MCC)**
```python
MCC = (TPÃ—TN - FPÃ—FN) / âˆš((TP+FP)(TP+FN)(TN+FP)(TN+FN))

# Range: -1 to +1
# +1: Perfect prediction, 0: Random, -1: Perfect inverse prediction
# Balanced metric that considers all classes
```

#### 3. Detailed Mathematical Analysis

**Class Imbalance Impact on Metrics:**
```python
def analyze_imbalance_impact():
    """Analyze how class imbalance affects different metrics"""
    
    # Scenario: 90% negative, 10% positive
    # Model 1: 95% accuracy (predicts all negative)
    # Model 2: 85% accuracy but good minority class detection
    
    metrics_analysis = {
        "Accuracy": {
            "Model1": 0.95,  # High but misleading
            "Model2": 0.85,  # Lower but more meaningful
            "Problem": "Favors majority class"
        },
        "Precision": {
            "Model1": 0.0,   # Cannot predict positives
            "Model2": 0.7,   # Good positive prediction accuracy
            "Advantage": "Focuses on minority class"
        },
        "Recall": {
            "Model1": 0.0,   # Cannot find any positives
            "Model2": 0.6,   # Finds 60% of positives
            "Advantage": "Measures minority class detection"
        },
        "F1-Score": {
            "Model1": 0.0,   # No positive predictions
            "Model2": 0.65,  # Balanced performance
            "Advantage": "Balances precision and recall"
        },
        "AUC-ROC": {
            "Model1": 0.5,   # Random performance
            "Model2": 0.8,   # Good ranking ability
            "Advantage": "Threshold-independent"
        }
    }
    
    return metrics_analysis
```

#### 4. Practical Implementation

**Comprehensive Imbalanced Metrics Evaluation:**
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (precision_score, recall_score, f1_score,
                           roc_auc_score, average_precision_score,
                           matthews_corrcoef, classification_report,
                           precision_recall_curve, roc_curve)
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

class ImbalancedMetricsAnalyzer:
    def __init__(self):
        self.metrics = {}
    
    def calculate_all_metrics(self, y_true, y_pred, y_scores):
        """Calculate comprehensive metrics for imbalanced data"""
        
        # Basic metrics
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        # Advanced metrics
        auc_roc = roc_auc_score(y_true, y_scores)
        auc_pr = average_precision_score(y_true, y_scores)
        mcc = matthews_corrcoef(y_true, y_pred)
        
        self.metrics = {
            'Precision': precision,
            'Recall': recall,
            'F1-Score': f1,
            'AUC-ROC': auc_roc,
            'AUC-PR': auc_pr,
            'MCC': mcc
        }
        
        return self.metrics
    
    def plot_imbalanced_analysis(self, y_true, y_scores):
        """Plot comprehensive analysis for imbalanced data"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # ROC Curve
        fpr, tpr, _ = roc_curve(y_true, y_scores)
        auc_roc = roc_auc_score(y_true, y_scores)
        
        ax1.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {auc_roc:.3f})')
        ax1.plot([0, 1], [0, 1], 'r--', alpha=0.5, label='Random')
        ax1.set_xlabel('False Positive Rate')
        ax1.set_ylabel('True Positive Rate')
        ax1.set_title('ROC Curve')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Precision-Recall Curve
        precision, recall, _ = precision_recall_curve(y_true, y_scores)
        auc_pr = average_precision_score(y_true, y_scores)
        
        ax2.plot(recall, precision, 'g-', linewidth=2, label=f'PR (AUC = {auc_pr:.3f})')
        baseline = np.sum(y_true) / len(y_true)  # Random baseline
        ax2.axhline(y=baseline, color='r', linestyle='--', alpha=0.5, label='Random')
        ax2.set_xlabel('Recall')
        ax2.set_ylabel('Precision')
        ax2.set_title('Precision-Recall Curve')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Class distribution
        class_counts = np.bincount(y_true)
        ax3.bar(['Class 0', 'Class 1'], class_counts, color=['blue', 'red'], alpha=0.7)
        ax3.set_ylabel('Count')
        ax3.set_title('Class Distribution')
        ax3.grid(True, alpha=0.3)
        
        # Add percentage labels
        total = sum(class_counts)
        for i, count in enumerate(class_counts):
            ax3.text(i, count + total*0.01, f'{count/total:.1%}', 
                    ha='center', va='bottom', weight='bold')
        
        # Metrics comparison
        metrics_names = list(self.metrics.keys())
        metrics_values = list(self.metrics.values())
        
        bars = ax4.bar(metrics_names, metrics_values, color='skyblue', alpha=0.7)
        ax4.set_ylabel('Score')
        ax4.set_title('Metrics Comparison')
        ax4.set_ylim(0, 1)
        
        # Add value labels
        for bar, value in zip(bars, metrics_values):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', weight='bold')
        
        # Rotate x-axis labels
        plt.setp(ax4.get_xticklabels(), rotation=45, ha='right')
        
        plt.tight_layout()
        plt.show()
    
    def generate_report(self, y_true, y_pred):
        """Generate detailed classification report"""
        print("=== IMBALANCED DATA EVALUATION REPORT ===")
        print(f"Class distribution: {np.bincount(y_true)}")
        print(f"Imbalance ratio: {max(np.bincount(y_true))/min(np.bincount(y_true)):.1f}:1")
        print()
        
        print("=== METRICS SUMMARY ===")
        for metric, value in self.metrics.items():
            print(f"{metric}: {value:.4f}")
        print()
        
        print("=== DETAILED CLASSIFICATION REPORT ===")
        print(classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))

def imbalanced_metrics_demo():
    """Demonstrate metrics for imbalanced data"""
    # Create imbalanced dataset
    X, y = make_classification(n_samples=10000, n_features=20, n_informative=15,
                             n_redundant=5, n_classes=2, weights=[0.9, 0.1],
                             random_state=42)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Train model
    model = LogisticRegression(class_weight='balanced', random_state=42)
    model.fit(X_train, y_train)
    
    # Predictions
    y_pred = model.predict(X_test)
    y_scores = model.predict_proba(X_test)[:, 1]
    
    # Analyze metrics
    analyzer = ImbalancedMetricsAnalyzer()
    analyzer.calculate_all_metrics(y_test, y_pred, y_scores)
    analyzer.plot_imbalanced_analysis(y_test, y_scores)
    analyzer.generate_report(y_test, y_pred)
    
    return analyzer

if __name__ == "__main__":
    analyzer = imbalanced_metrics_demo()
```

#### 5. Best Practices for Imbalanced Data

**Metric Selection Guidelines:**
```python
def metric_selection_guide():
    """Guide for selecting metrics in imbalanced scenarios"""
    
    guidelines = {
        "High imbalance (1:100+)": {
            "Primary": "AUC-PR, F1-Score",
            "Secondary": "Precision, Recall",
            "Avoid": "Accuracy"
        },
        "Medium imbalance (1:10 to 1:100)": {
            "Primary": "F1-Score, AUC-ROC, AUC-PR",
            "Secondary": "Precision, Recall, MCC",
            "Avoid": "Accuracy"
        },
        "Low imbalance (1:3 to 1:10)": {
            "Primary": "F1-Score, AUC-ROC",
            "Secondary": "Accuracy, MCC",
            "Consider": "Precision, Recall"
        }
    }
    
    return guidelines

def class_weight_impact():
    """Demonstrate impact of class weights"""
    return {
        "No weights": "Model biased toward majority class",
        "Balanced weights": "Equal importance to both classes",
        "Custom weights": "Tune based on business cost",
        "SMOTE": "Synthetic minority oversampling",
        "Threshold tuning": "Adjust decision threshold"
    }
```

### ğŸ” é¢è¯•å¸¸è§é—®é¢˜åŠå›ç­”

#### Q1: "Why is accuracy misleading for imbalanced datasets?"

**English Answer:**
Accuracy is misleading because it's dominated by the majority class. In a 95%-5% split, a naive model that predicts all samples as the majority class achieves 95% accuracy, making it appear excellent when it's actually useless. Accuracy doesn't distinguish between different types of errors and gives equal weight to both classes, which is problematic when classes have different importance.

#### Q2: "What metrics should I use for highly imbalanced data?"

**English Answer:**
For highly imbalanced data, I use AUC-PR and F1-score as primary metrics because they focus on the minority class performance. AUC-PR is more sensitive to class imbalance than AUC-ROC and gives a realistic picture of performance. I also use Precision and Recall to understand the specific trade-offs, and Matthews Correlation Coefficient (MCC) as a balanced metric that considers all classes.

#### Q3: "How do you handle class imbalance during evaluation vs training?"

**English Answer:**
During evaluation, I use metrics that focus on minority class performance like F1-score, AUC-PR, and MCC. During training, I use techniques like class weights, SMOTE, or cost-sensitive learning to ensure the model doesn't ignore the minority class. The key is to address imbalance both in training (so the model learns to identify minorities) and evaluation (so we measure the right performance).

### ğŸ’¡ å®æˆ˜æŠ€å·§

#### 1. æŒ‡æ ‡é€‰æ‹©ç­–ç•¥ (Metric Selection Strategy)
- **é«˜ä¸å¹³è¡¡ (1:100+)**ï¼šAUC-PR, F1-Score
- **ä¸­ç­‰ä¸å¹³è¡¡ (1:10-1:100)**ï¼šF1-Score, AUC-ROC, AUC-PR
- **ä½ä¸å¹³è¡¡ (1:3-1:10)**ï¼šF1-Score, AUC-ROC

#### 2. å…³é”®è¯ (Key Terms)
- **Class Imbalance**: ç±»åˆ«ä¸å¹³è¡¡
- **Minority Class**: å°‘æ•°ç±»
- **Majority Class**: å¤šæ•°ç±»
- **AUC-PR**: PRæ›²çº¿ä¸‹é¢ç§¯
- **MCC**: Matthewsç›¸å…³ç³»æ•°
- **SMOTE**: åˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·

#### 3. å¸¸è§é™·é˜± (Common Pitfalls)
- âŒ åœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šä½¿ç”¨å‡†ç¡®ç‡
- âŒ åªå…³æ³¨å•ä¸€æŒ‡æ ‡
- âŒ å¿½ç•¥ä¸šåŠ¡æˆæœ¬å·®å¼‚
- âŒ ä¸è°ƒæ•´å†³ç­–é˜ˆå€¼

### ğŸ“Š å¯è§†åŒ–ç†è§£

#### ä¸å¹³è¡¡æ•°æ®æŒ‡æ ‡å¯¹æ¯”
![ä¸å¹³è¡¡æ•°æ®æŒ‡æ ‡å¯¹æ¯”](../../images/metrics/imbalanced_metrics_comparison.png)

#### ROC vs PRæ›²çº¿å¯¹æ¯”
![ROC vs PRæ›²çº¿å¯¹æ¯”](../../images/metrics/roc_vs_pr_curves.png)

### ğŸ“Š é¢è¯•å‡†å¤‡æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ä¸ºä»€ä¹ˆå‡†ç¡®ç‡åœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šå¤±æ•ˆ
- [ ] æŒæ¡Precisionã€Recallã€F1-scoreçš„å«ä¹‰
- [ ] ç†è§£AUC-ROCå’ŒAUC-PRçš„åŒºåˆ«
- [ ] çŸ¥é“MCCçš„ä¼˜åŠ¿
- [ ] ç†è§£ç±»åˆ«æƒé‡çš„ä½œç”¨
- [ ] æŒæ¡ä¸åŒä¸å¹³è¡¡ç¨‹åº¦çš„æŒ‡æ ‡é€‰æ‹©
- [ ] èƒ½å¤Ÿè§£é‡ŠPRæ›²çº¿å’ŒROCæ›²çº¿
- [ ] çŸ¥é“å¦‚ä½•å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„è®­ç»ƒå’Œè¯„ä¼°

### ğŸ¯ ç»ƒä¹ å»ºè®®

1. **ç†è®ºç»ƒä¹ **: ç†è§£ä¸åŒæŒ‡æ ‡åœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šçš„è¡¨ç°
2. **å®éªŒç»ƒä¹ **: åœ¨ä¸åŒä¸å¹³è¡¡æ¯”ä¾‹çš„æ•°æ®ä¸Šæµ‹è¯•æŒ‡æ ‡
3. **å¯è§†åŒ–ç»ƒä¹ **: ç»˜åˆ¶ROCå’ŒPRæ›²çº¿å¯¹æ¯”
4. **åº”ç”¨ç»ƒä¹ **: åœ¨çœŸå®ä¸å¹³è¡¡æ•°æ®ä¸Šé€‰æ‹©åˆé€‚æŒ‡æ ‡
5. **è°ƒä¼˜ç»ƒä¹ **: ä½¿ç”¨ç±»åˆ«æƒé‡å’Œé˜ˆå€¼è°ƒä¼˜

**è®°ä½**: åœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šï¼Œå‡†ç¡®ç‡ä¼šè¯¯å¯¼ï¼Œè¦ä½¿ç”¨å…³æ³¨å°‘æ•°ç±»æ€§èƒ½çš„æŒ‡æ ‡ï¼

