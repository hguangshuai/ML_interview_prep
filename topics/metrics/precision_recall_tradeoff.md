# è¯„ä¼°æŒ‡æ ‡ä¸“é¢˜ - è¯¦ç»†ç­”æ¡ˆ

## é—®é¢˜: Precisionå’ŒRecallæƒè¡¡ (Precision-Recall Trade-off)

### ğŸ¯ ä¸­æ–‡ç†è§£ (ä¾¿äºè®°å¿†)

#### Precision vs Recall = "å‡†ç¡®æ€§ vs å®Œæ•´æ€§"
æƒ³è±¡åŒ»ç”Ÿè¯Šæ–­ç–¾ç—…ï¼š
- **Precision (ç²¾ç¡®ç‡)**ï¼šåƒ"ç¡®è¯Šå‡†ç¡®ç‡"
  - é—®é¢˜ï¼šåœ¨æ‰€æœ‰è¯Šæ–­ä¸ºé˜³æ€§çš„æ‚£è€…ä¸­ï¼Œæœ‰å¤šå°‘çœŸçš„æ‚£ç—…ï¼Ÿ
  - æ¯”å–»ï¼šåŒ»ç”Ÿè¯Šæ–­çš„10ä¸ªæ‚£è€…ä¸­ï¼Œ8ä¸ªçœŸçš„æ‚£ç—…ï¼Œç²¾ç¡®ç‡=80%
  
- **Recall (å¬å›ç‡)**ï¼šåƒ"ä¸æ¼è¯Šç‡"
  - é—®é¢˜ï¼šåœ¨æ‰€æœ‰çœŸæ­£æ‚£ç—…çš„æ‚£è€…ä¸­ï¼Œæœ‰å¤šå°‘è¢«è¯Šæ–­å‡ºæ¥äº†ï¼Ÿ
  - æ¯”å–»ï¼šå®é™…æœ‰10ä¸ªæ‚£è€…æ‚£ç—…ï¼ŒåŒ»ç”Ÿè¯Šæ–­å‡º7ä¸ªï¼Œå¬å›ç‡=70%

#### æƒè¡¡å…³ç³»
- **æé«˜Precision**ï¼šé™ä½é˜ˆå€¼ï¼Œåªè¯Šæ–­æœ€æœ‰æŠŠæ¡çš„ç—…ä¾‹ â†’ å¯èƒ½æ¼è¯Š
- **æé«˜Recall**ï¼šæé«˜é˜ˆå€¼ï¼Œè¯Šæ–­æ›´å¤šå¯ç–‘ç—…ä¾‹ â†’ å¯èƒ½è¯¯è¯Š
- **çŸ›ç›¾**ï¼šå¾ˆéš¾åŒæ—¶åšåˆ°æ—¢å‡†ç¡®åˆå®Œæ•´

### ğŸ¤ ç›´æ¥é¢è¯•å›ç­” (Direct Interview Answer)

**Precision measures how many of the predicted positive cases are actually positive (accuracy of positive predictions), while Recall measures how many of the actual positive cases are correctly identified (completeness of positive detection).**

**The trade-off occurs because lowering the classification threshold increases Recall (catches more positives) but decreases Precision (more false positives), while raising the threshold increases Precision (fewer false positives) but decreases Recall (misses more positives).**

**Mathematically: Precision = TP/(TP+FP) and Recall = TP/(TP+FN). These metrics are inversely related through the classification threshold - you cannot optimize both simultaneously.**

**I choose based on business context: high Precision when false positives are costly (e.g., spam detection), high Recall when missing positives is costly (e.g., medical diagnosis). The F1-score balances both when both are equally important.**

---

### ğŸ“ è‹±æ–‡æ ‡å‡†é¢è¯•ç­”æ¡ˆ (English Interview Answer)

#### 1. Definitions and Formulas

**Precision (Positive Predictive Value):**
```python
Precision = TP / (TP + FP)
# TP = True Positives, FP = False Positives
```

**Recall (Sensitivity, True Positive Rate):**
```python
Recall = TP / (TP + FN)
# TP = True Positives, FN = False Negatives
```

**F1-Score (Harmonic Mean):**
```python
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

#### 2. Mathematical Relationship

**Threshold Effect:**
```python
# Lower threshold â†’ More predictions as positive
# Higher threshold â†’ Fewer predictions as positive

# As threshold decreases:
# - TP increases, FP increases â†’ Recall â†‘, Precision â†“
# - FN decreases, TN increases

# As threshold increases:
# - TP decreases, FP decreases â†’ Recall â†“, Precision â†‘
# - FN increases, TN decreases
```

#### 3. Trade-off Visualization

**Precision-Recall Curve:**
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, auc

def plot_precision_recall_tradeoff():
    """Visualize precision-recall trade-off"""
    # Generate synthetic data
    np.random.seed(42)
    n_samples = 1000
    
    # True labels
    y_true = np.random.binomial(1, 0.3, n_samples)
    
    # Predicted probabilities (simulate model output)
    y_scores = np.random.beta(2, 5, n_samples)
    y_scores[y_true == 1] += 0.3  # Make positives more likely
    
    # Calculate precision-recall curve
    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
    
    # Plot
    plt.figure(figsize=(12, 8))
    
    # Precision-Recall curve
    plt.subplot(2, 2, 1)
    plt.plot(recall, precision, 'b-', linewidth=2, label='PR Curve')
    plt.fill_between(recall, precision, alpha=0.3)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Precision vs Threshold
    plt.subplot(2, 2, 2)
    plt.plot(thresholds, precision[:-1], 'r-', linewidth=2, label='Precision')
    plt.xlabel('Threshold')
    plt.ylabel('Precision')
    plt.title('Precision vs Threshold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Recall vs Threshold
    plt.subplot(2, 2, 3)
    plt.plot(thresholds, recall[:-1], 'g-', linewidth=2, label='Recall')
    plt.xlabel('Threshold')
    plt.ylabel('Recall')
    plt.title('Recall vs Threshold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # F1-Score vs Threshold
    plt.subplot(2, 2, 4)
    f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])
    plt.plot(thresholds, f1_scores, 'purple', linewidth=2, label='F1-Score')
    plt.xlabel('Threshold')
    plt.ylabel('F1-Score')
    plt.title('F1-Score vs Threshold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return precision, recall, thresholds

if __name__ == "__main__":
    plot_precision_recall_tradeoff()
```

#### 4. Practical Examples

**Example 1: Email Spam Detection**
```python
# High Precision needed (avoid false positives)
# Better to miss some spam than mark important emails as spam
# Threshold should be high â†’ High Precision, Lower Recall

def spam_detection_example():
    """Example: Spam detection prioritizes precision"""
    scenario = "Email Spam Detection"
    
    # Business impact
    false_positive_cost = "High - Important email marked as spam"
    false_negative_cost = "Low - Some spam gets through"
    
    # Optimal strategy
    strategy = "High Precision, Accept Lower Recall"
    threshold = "High threshold (0.8-0.9)"
    
    print(f"Scenario: {scenario}")
    print(f"False Positive Cost: {false_positive_cost}")
    print(f"False Negative Cost: {false_negative_cost}")
    print(f"Strategy: {strategy}")
    print(f"Threshold: {threshold}")
```

**Example 2: Medical Diagnosis**
```python
# High Recall needed (avoid false negatives)
# Better to have false alarms than miss real diseases
# Threshold should be low â†’ High Recall, Lower Precision

def medical_diagnosis_example():
    """Example: Medical diagnosis prioritizes recall"""
    scenario = "Medical Disease Diagnosis"
    
    # Business impact
    false_positive_cost = "Medium - Unnecessary tests, anxiety"
    false_negative_cost = "Very High - Missed disease, death risk"
    
    # Optimal strategy
    strategy = "High Recall, Accept Lower Precision"
    threshold = "Low threshold (0.3-0.5)"
    
    print(f"Scenario: {scenario}")
    print(f"False Positive Cost: {false_positive_cost}")
    print(f"False Negative Cost: {false_negative_cost}")
    print(f"Strategy: {strategy}")
    print(f"Threshold: {threshold}")
```

#### 5. Advanced Metrics

**Area Under PR Curve (AUPRC):**
```python
def calculate_auprc(y_true, y_scores):
    """Calculate Area Under Precision-Recall Curve"""
    from sklearn.metrics import precision_recall_curve, auc
    
    precision, recall, _ = precision_recall_curve(y_true, y_scores)
    auprc = auc(recall, precision)
    
    return auprc, precision, recall

# AUPRC is particularly useful for imbalanced datasets
# Better than AUC-ROC when positive class is rare
```

**F-beta Score:**
```python
def f_beta_score(precision, recall, beta):
    """Calculate F-beta score"""
    if precision + recall == 0:
        return 0
    
    f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)
    return f_beta

# Beta > 1: Emphasizes Recall more (e.g., F2-score)
# Beta < 1: Emphasizes Precision more (e.g., F0.5-score)
# Beta = 1: Equal emphasis (F1-score)
```

### ğŸ’» å®é™…ä»£ç ç¤ºä¾‹

#### Complete Precision-Recall Analysis
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (precision_score, recall_score, f1_score,
                           precision_recall_curve, classification_report,
                           confusion_matrix)
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

class PrecisionRecallAnalyzer:
    def __init__(self):
        self.model = LogisticRegression()
        self.y_true = None
        self.y_scores = None
        self.precision_scores = []
        self.recall_scores = []
        self.f1_scores = []
        self.thresholds = []
    
    def fit_and_predict(self, X_train, X_test, y_train, y_test):
        """Train model and get probability scores"""
        self.model.fit(X_train, y_train)
        self.y_true = y_test
        self.y_scores = self.model.predict_proba(X_test)[:, 1]
        return self.y_scores
    
    def calculate_metrics_at_thresholds(self, threshold_range=None):
        """Calculate metrics at different thresholds"""
        if threshold_range is None:
            threshold_range = np.linspace(0.1, 0.9, 20)
        
        for threshold in threshold_range:
            y_pred = (self.y_scores >= threshold).astype(int)
            
            precision = precision_score(self.y_true, y_pred, zero_division=0)
            recall = recall_score(self.y_true, y_pred, zero_division=0)
            f1 = f1_score(self.y_true, y_pred, zero_division=0)
            
            self.precision_scores.append(precision)
            self.recall_scores.append(recall)
            self.f1_scores.append(f1)
            self.thresholds.append(threshold)
        
        return self.thresholds, self.precision_scores, self.recall_scores, self.f1_scores
    
    def find_optimal_threshold(self, metric='f1'):
        """Find optimal threshold based on specified metric"""
        if metric == 'f1':
            optimal_idx = np.argmax(self.f1_scores)
        elif metric == 'precision':
            optimal_idx = np.argmax(self.precision_scores)
        elif metric == 'recall':
            optimal_idx = np.argmax(self.recall_scores)
        else:
            raise ValueError("Metric must be 'f1', 'precision', or 'recall'")
        
        return self.thresholds[optimal_idx]
    
    def plot_comprehensive_analysis(self):
        """Plot comprehensive precision-recall analysis"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # Precision-Recall curve
        precision_curve, recall_curve, _ = precision_recall_curve(self.y_true, self.y_scores)
        ax1.plot(recall_curve, precision_curve, 'b-', linewidth=2, label='PR Curve')
        ax1.fill_between(recall_curve, precision_curve, alpha=0.3)
        ax1.set_xlabel('Recall')
        ax1.set_ylabel('Precision')
        ax1.set_title('Precision-Recall Curve')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Metrics vs Threshold
        ax2.plot(self.thresholds, self.precision_scores, 'r-', linewidth=2, label='Precision')
        ax2.plot(self.thresholds, self.recall_scores, 'g-', linewidth=2, label='Recall')
        ax2.plot(self.thresholds, self.f1_scores, 'purple', linewidth=2, label='F1-Score')
        ax2.set_xlabel('Threshold')
        ax2.set_ylabel('Score')
        ax2.set_title('Metrics vs Threshold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Trade-off visualization
        ax3.plot(self.precision_scores, self.recall_scores, 'bo-', linewidth=2, markersize=4)
        ax3.set_xlabel('Precision')
        ax3.set_ylabel('Recall')
        ax3.set_title('Precision-Recall Trade-off')
        ax3.grid(True, alpha=0.3)
        
        # Add threshold annotations
        for i in range(0, len(self.thresholds), 5):
            ax3.annotate(f'{self.thresholds[i]:.2f}', 
                        (self.precision_scores[i], self.recall_scores[i]),
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        # Optimal threshold analysis
        optimal_f1_threshold = self.find_optimal_threshold('f1')
        optimal_precision_threshold = self.find_optimal_threshold('precision')
        optimal_recall_threshold = self.find_optimal_threshold('recall')
        
        ax4.bar(['F1-Optimal', 'Precision-Optimal', 'Recall-Optimal'], 
                [optimal_f1_threshold, optimal_precision_threshold, optimal_recall_threshold],
                color=['purple', 'red', 'green'], alpha=0.7)
        ax4.set_ylabel('Optimal Threshold')
        ax4.set_title('Optimal Thresholds for Different Metrics')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return {
            'optimal_f1_threshold': optimal_f1_threshold,
            'optimal_precision_threshold': optimal_precision_threshold,
            'optimal_recall_threshold': optimal_recall_threshold
        }

def precision_recall_demo():
    """Demonstrate precision-recall trade-off"""
    # Generate synthetic data
    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                             n_redundant=5, n_classes=2, weights=[0.7, 0.3],
                             random_state=42)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Analyze precision-recall trade-off
    analyzer = PrecisionRecallAnalyzer()
    analyzer.fit_and_predict(X_train, X_test, y_train, y_test)
    analyzer.calculate_metrics_at_thresholds()
    
    # Plot analysis
    results = analyzer.plot_comprehensive_analysis()
    
    # Print summary
    print("=== PRECISION-RECALL TRADE-OFF ANALYSIS ===")
    print(f"Optimal F1 Threshold: {results['optimal_f1_threshold']:.3f}")
    print(f"Optimal Precision Threshold: {results['optimal_precision_threshold']:.3f}")
    print(f"Optimal Recall Threshold: {results['optimal_recall_threshold']:.3f}")
    
    # Show metrics at different thresholds
    print("\n=== METRICS AT DIFFERENT THRESHOLDS ===")
    for i in range(0, len(analyzer.thresholds), 5):
        print(f"Threshold: {analyzer.thresholds[i]:.2f} | "
              f"Precision: {analyzer.precision_scores[i]:.3f} | "
              f"Recall: {analyzer.recall_scores[i]:.3f} | "
              f"F1: {analyzer.f1_scores[i]:.3f}")

if __name__ == "__main__":
    precision_recall_demo()
```

### ğŸ” é¢è¯•å¸¸è§é—®é¢˜åŠå›ç­”

#### Q1: "What is the difference between Precision and Recall?"

**English Answer:**
Precision measures the accuracy of positive predictions - of all cases predicted as positive, how many are actually positive? Recall measures the completeness of positive detection - of all actual positive cases, how many were correctly identified? Precision focuses on false positives, while Recall focuses on false negatives.

#### Q2: "Why can't we maximize both Precision and Recall simultaneously?"

**English Answer:**
Precision and Recall have an inherent trade-off because they're both affected by the classification threshold in opposite ways. Lowering the threshold increases Recall (catches more positives) but decreases Precision (more false positives). Raising the threshold increases Precision (fewer false positives) but decreases Recall (misses more positives). This trade-off is fundamental to binary classification.

#### Q3: "When would you prioritize Precision over Recall?"

**English Answer:**
I prioritize Precision when false positives are costly or harmful. Examples include spam detection (don't want to mark important emails as spam), fraud detection (don't want to block legitimate transactions), or medical screening where false alarms cause unnecessary anxiety. In these cases, it's better to miss some positive cases than to incorrectly classify negative cases as positive.

#### Q4: "How do you choose the optimal threshold for your model?"

**English Answer:**
I choose the threshold based on the business context and cost of errors. For balanced importance, I use the F1-score to find the threshold that maximizes the harmonic mean of Precision and Recall. For imbalanced datasets, I might use F-beta scores or directly optimize for the business metric. I also consider the precision-recall curve to understand the trade-off at different threshold values.

### ğŸ’¡ å®æˆ˜æŠ€å·§

#### 1. é€‰æ‹©æ ‡å‡† (Selection Criteria)
- **é«˜Precision**ï¼šå‡é˜³æ€§æˆæœ¬é«˜ï¼ˆåƒåœ¾é‚®ä»¶æ£€æµ‹ï¼‰
- **é«˜Recall**ï¼šå‡é˜´æ€§æˆæœ¬é«˜ï¼ˆåŒ»ç–—è¯Šæ–­ï¼‰
- **å¹³è¡¡**ï¼šä½¿ç”¨F1-scoreæˆ–F-beta score

#### 2. å…³é”®è¯ (Key Terms)
- **Precision**: ç²¾ç¡®ç‡
- **Recall**: å¬å›ç‡
- **F1-Score**: F1åˆ†æ•°
- **Threshold**: é˜ˆå€¼
- **Trade-off**: æƒè¡¡
- **AUPRC**: PRæ›²çº¿ä¸‹é¢ç§¯

#### 3. å¸¸è§é™·é˜± (Common Pitfalls)
- âŒ åªå…³æ³¨å•ä¸€æŒ‡æ ‡
- âŒ å¿½ç•¥ä¸šåŠ¡æˆæœ¬
- âŒ ä¸ç†è§£é˜ˆå€¼çš„ä½œç”¨
- âŒ åœ¨ç±»åˆ«ä¸å¹³è¡¡æ•°æ®ä¸Šä½¿ç”¨å‡†ç¡®ç‡

### ğŸ“Š å¯è§†åŒ–ç†è§£

#### Precision-Recallæƒè¡¡å›¾
![Precision-Recallæƒè¡¡å›¾](../../images/metrics/precision_recall_tradeoff.png)

#### é˜ˆå€¼å¯¹æŒ‡æ ‡çš„å½±å“
![é˜ˆå€¼å½±å“å›¾](../../images/metrics/threshold_impact.png)

### ğŸ“Š é¢è¯•å‡†å¤‡æ£€æŸ¥æ¸…å•

- [ ] ç†è§£Precisionå’ŒRecallçš„å®šä¹‰
- [ ] æŒæ¡ä¸¤è€…çš„æ•°å­¦å…¬å¼
- [ ] ç†è§£æƒè¡¡å…³ç³»çš„åŸå› 
- [ ] çŸ¥é“ä½•æ—¶ä¼˜å…ˆå“ªä¸ªæŒ‡æ ‡
- [ ] æŒæ¡F1-scoreçš„è®¡ç®—
- [ ] ç†è§£é˜ˆå€¼çš„ä½œç”¨
- [ ] èƒ½å¤Ÿè§£é‡ŠPRæ›²çº¿
- [ ] çŸ¥é“å¦‚ä½•å¤„ç†ç±»åˆ«ä¸å¹³è¡¡

### ğŸ¯ ç»ƒä¹ å»ºè®®

1. **ç†è®ºç»ƒä¹ **: ç†è§£Precisionå’ŒRecallçš„æ•°å­¦å«ä¹‰
2. **å¯è§†åŒ–ç»ƒä¹ **: ç»˜åˆ¶PRæ›²çº¿å’Œæƒè¡¡å›¾
3. **é˜ˆå€¼ç»ƒä¹ **: åˆ†æä¸åŒé˜ˆå€¼å¯¹æŒ‡æ ‡çš„å½±å“
4. **åº”ç”¨ç»ƒä¹ **: åœ¨ä¸åŒåœºæ™¯ä¸‹é€‰æ‹©åˆé€‚æŒ‡æ ‡
5. **ä»£ç ç»ƒä¹ **: å®ç°å®Œæ•´çš„PRåˆ†æ

**è®°ä½**: Precisionå…³æ³¨å‡†ç¡®æ€§ï¼ŒRecallå…³æ³¨å®Œæ•´æ€§ï¼Œä¸¤è€…å­˜åœ¨å›ºæœ‰æƒè¡¡ï¼Œé€‰æ‹©å–å†³äºä¸šåŠ¡éœ€æ±‚ï¼

