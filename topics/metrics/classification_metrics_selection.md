# ËØÑ‰º∞ÊåáÊ†á‰∏ìÈ¢ò - ËØ¶ÁªÜÁ≠îÊ°à

## ÈóÆÈ¢ò: ÂàÜÁ±ªÈóÆÈ¢òËØ•ÈÄâÁî®‰ªÄ‰πàËØÑ‰º∞ÊåáÊ†á and why

### üéØ ‰∏≠ÊñáÁêÜËß£ (‰æø‰∫éËÆ∞ÂøÜ)

#### ÂàÜÁ±ªÊåáÊ†áÈÄâÊã© = "ÁúãÁóÖË¶ÅÈÄâÂØπÊ£ÄÊü•È°πÁõÆ"
ÊÉ≥Ë±°ÂåªÁîüËØäÊñ≠ÁñæÁóÖÔºö
- **‰∏çÂêåÁñæÁóÖÈúÄË¶Å‰∏çÂêåÊ£ÄÊü•**ÔºöÊÑüÂÜíÁî®‰ΩìÊ∏©ËÆ°ÔºåÂøÉËÑèÁóÖÁî®ÂøÉÁîµÂõæ
- **‰∏çÂêåÂàÜÁ±ªÈóÆÈ¢òÈúÄË¶Å‰∏çÂêåÊåáÊ†á**ÔºöÂûÉÂúæÈÇÆ‰ª∂Áî®Á≤æÁ°ÆÁéáÔºåÂåªÁñóËØäÊñ≠Áî®Âè¨ÂõûÁéá
- **ÂÖ≥ÈîÆÂéüÂàô**ÔºöÊ†πÊçÆ‰∏öÂä°ÈúÄÊ±ÇÂíåÈîôËØØÊàêÊú¨ÈÄâÊã©ÊåáÊ†á

#### ÈÄâÊã©Ê†áÂáÜ
1. **Êï∞ÊçÆÂπ≥Ë°°ÊÄß**ÔºöÂπ≥Ë°° vs ‰∏çÂπ≥Ë°°
2. **‰∏öÂä°ÁõÆÊ†á**ÔºöÂáÜÁ°ÆÊÄß vs ÂÆåÊï¥ÊÄß vs Âπ≥Ë°°
3. **ÈîôËØØÊàêÊú¨**ÔºöÂÅáÈò≥ÊÄß vs ÂÅáÈò¥ÊÄßÁöÑ‰ª£‰ª∑
4. **Ê®°ÂûãÁ±ªÂûã**ÔºöÊ¶ÇÁéáÊ®°Âûã vs ÂÜ≥Á≠ñÊ®°Âûã

### üé§ Áõ¥Êé•Èù¢ËØïÂõûÁ≠î (Direct Interview Answer)

**For classification problems, I choose metrics based on three key factors: data balance, business objectives, and error costs. For balanced data, I use accuracy and F1-score. For imbalanced data, I prioritize F1-score, AUC-ROC, and AUC-PR.**

**When false positives are costly (like spam detection), I focus on Precision. When false negatives are dangerous (like medical diagnosis), I prioritize Recall. When both are equally important, I use F1-score or AUC-ROC.**

**For probability-based models, I use AUC-ROC to measure ranking ability and log-loss for probability calibration. For decision-based models, I use confusion matrix metrics like Precision, Recall, and F1-score.**

**The key is understanding the business context - what type of error is more costly and what performance aspect matters most for the specific application.**

---

### üìù Ëã±ÊñáÊ†áÂáÜÈù¢ËØïÁ≠îÊ°à (English Interview Answer)

#### 1. Metric Selection Framework

**Decision Tree for Classification Metrics:**

```python
def select_classification_metrics(data_type, business_goal, model_type):
    """
    Framework for selecting classification metrics
    
    Parameters:
    - data_type: 'balanced', 'imbalanced', 'highly_imbalanced'
    - business_goal: 'accuracy', 'precision', 'recall', 'balanced'
    - model_type: 'probabilistic', 'decision'
    """
    
    if data_type == 'balanced':
        if business_goal == 'accuracy':
            return ['Accuracy', 'F1-Score']
        elif business_goal == 'precision':
            return ['Precision', 'F1-Score']
        elif business_goal == 'recall':
            return ['Recall', 'F1-Score']
        else:  # balanced
            return ['F1-Score', 'AUC-ROC']
    
    elif data_type == 'imbalanced':
        if business_goal == 'precision':
            return ['Precision', 'F1-Score', 'AUC-PR']
        elif business_goal == 'recall':
            return ['Recall', 'F1-Score', 'AUC-PR']
        else:
            return ['F1-Score', 'AUC-ROC', 'AUC-PR']
    
    elif data_type == 'highly_imbalanced':
        return ['AUC-PR', 'F1-Score', 'Precision', 'Recall']
    
    if model_type == 'probabilistic':
        return ['AUC-ROC', 'Log-Loss'] + previous_metrics
    
    return previous_metrics
```

#### 2. Business Context Analysis

**A. Spam Detection (High Precision Needed)**
```python
def spam_detection_metrics():
    """Metrics for spam detection"""
    context = {
        "Business Goal": "Minimize false positives (important emails marked as spam)",
        "Error Cost": "False positive > False negative",
        "Primary Metrics": ["Precision", "F1-Score"],
        "Secondary Metrics": ["AUC-ROC"],
        "Threshold Strategy": "High threshold (0.8-0.9)"
    }
    return context

# Example: Email spam detection
# Cost of false positive: Important email lost
# Cost of false negative: Some spam gets through
# Priority: High precision
```

**B. Medical Diagnosis (High Recall Needed)**
```python
def medical_diagnosis_metrics():
    """Metrics for medical diagnosis"""
    context = {
        "Business Goal": "Minimize false negatives (missed diseases)",
        "Error Cost": "False negative >> False positive",
        "Primary Metrics": ["Recall", "Sensitivity", "F1-Score"],
        "Secondary Metrics": ["AUC-ROC", "Specificity"],
        "Threshold Strategy": "Low threshold (0.3-0.5)"
    }
    return context

# Example: Cancer screening
# Cost of false negative: Death risk
# Cost of false positive: Unnecessary tests, anxiety
# Priority: High recall
```

**C. Fraud Detection (Balanced Approach)**
```python
def fraud_detection_metrics():
    """Metrics for fraud detection"""
    context = {
        "Business Goal": "Balance between catching fraud and avoiding false alarms",
        "Error Cost": "Both errors are costly",
        "Primary Metrics": ["F1-Score", "AUC-ROC"],
        "Secondary Metrics": ["Precision", "Recall", "MCC"],
        "Threshold Strategy": "Optimize F1-score"
    }
    return context
```

#### 3. Data Characteristics Analysis

**A. Balanced Data (50-50 split)**
```python
def balanced_data_metrics():
    """Metrics for balanced data"""
    return {
        "Primary": ["Accuracy", "F1-Score", "AUC-ROC"],
        "Secondary": ["Precision", "Recall"],
        "Why": "All classes equally represented, accuracy is meaningful",
        "Caution": "Still consider business costs"
    }
```

**B. Moderately Imbalanced Data (80-20 split)**
```python
def moderate_imbalance_metrics():
    """Metrics for moderately imbalanced data"""
    return {
        "Primary": ["F1-Score", "AUC-ROC", "Precision", "Recall"],
        "Secondary": ["Accuracy", "MCC"],
        "Why": "Accuracy still somewhat meaningful, but focus on minority class",
        "Strategy": "Use class weights or threshold tuning"
    }
```

**C. Highly Imbalanced Data (95-5 split)**
```python
def high_imbalance_metrics():
    """Metrics for highly imbalanced data"""
    return {
        "Primary": ["AUC-PR", "F1-Score", "Precision", "Recall"],
        "Secondary": ["AUC-ROC", "MCC"],
        "Avoid": ["Accuracy"],
        "Why": "Accuracy dominated by majority class",
        "Strategy": "Focus entirely on minority class performance"
    }
```

#### 4. Model Type Considerations

**A. Probabilistic Models (Logistic Regression, Neural Networks)**
```python
def probabilistic_model_metrics():
    """Metrics for probabilistic models"""
    return {
        "Ranking Metrics": ["AUC-ROC", "AUC-PR"],
        "Calibration Metrics": ["Log-Loss", "Brier Score"],
        "Decision Metrics": ["Precision", "Recall", "F1-Score"],
        "Why": "Can evaluate probability quality and ranking ability"
    }
```

**B. Decision Models (Decision Trees, SVMs)**
```python
def decision_model_metrics():
    """Metrics for decision models"""
    return {
        "Primary": ["Precision", "Recall", "F1-Score"],
        "Secondary": ["Accuracy", "MCC"],
        "Why": "Only discrete predictions available",
        "Note": "Cannot evaluate probability calibration"
    }
```

#### 5. Comprehensive Metric Selection Guide

**Decision Matrix:**
```python
def classification_metric_guide():
    """Comprehensive guide for classification metrics"""
    
    guide = {
        "Data Balance": {
            "Balanced (40-60% split)": {
                "Primary": ["Accuracy", "F1-Score", "AUC-ROC"],
                "Use Case": "General classification problems"
            },
            "Moderate Imbalance (80-20% split)": {
                "Primary": ["F1-Score", "AUC-ROC", "Precision", "Recall"],
                "Use Case": "Customer churn, fraud detection"
            },
            "High Imbalance (95-5% split)": {
                "Primary": ["AUC-PR", "F1-Score", "Precision", "Recall"],
                "Use Case": "Rare disease detection, anomaly detection"
            }
        },
        
        "Business Priority": {
            "High Precision": {
                "Metrics": ["Precision", "F1-Score"],
                "Use Case": "Spam detection, content filtering"
            },
            "High Recall": {
                "Metrics": ["Recall", "F1-Score", "Sensitivity"],
                "Use Case": "Medical diagnosis, security screening"
            },
            "Balanced": {
                "Metrics": ["F1-Score", "AUC-ROC", "MCC"],
                "Use Case": "General business applications"
            }
        },
        
        "Model Type": {
            "Probabilistic": {
                "Additional": ["AUC-ROC", "Log-Loss", "Brier Score"],
                "Why": "Can evaluate probability quality"
            },
            "Decision": {
                "Focus": ["Precision", "Recall", "F1-Score"],
                "Why": "Only discrete predictions available"
            }
        }
    }
    
    return guide
```

### üíª ÂÆûÈôÖ‰ª£Á†ÅÁ§∫‰æã

#### Comprehensive Metric Selection Implementation
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, average_precision_score,
                           matthews_corrcoef, log_loss, classification_report)
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

class ClassificationMetricSelector:
    def __init__(self):
        self.metrics_history = {}
        self.recommendations = {}
    
    def analyze_data_characteristics(self, y):
        """Analyze data characteristics for metric selection"""
        class_counts = np.bincount(y)
        total_samples = len(y)
        
        # Calculate imbalance ratio
        majority_count = max(class_counts)
        minority_count = min(class_counts)
        imbalance_ratio = majority_count / minority_count
        
        # Determine data type
        if imbalance_ratio <= 2:
            data_type = "balanced"
        elif imbalance_ratio <= 10:
            data_type = "moderate_imbalance"
        else:
            data_type = "high_imbalance"
        
        characteristics = {
            "class_distribution": class_counts,
            "imbalance_ratio": imbalance_ratio,
            "data_type": data_type,
            "majority_class_ratio": majority_count / total_samples
        }
        
        return characteristics
    
    def select_metrics(self, data_characteristics, business_context="balanced", model_type="probabilistic"):
        """Select appropriate metrics based on context"""
        
        data_type = data_characteristics["data_type"]
        
        # Base metrics based on data type
        if data_type == "balanced":
            base_metrics = ["accuracy", "f1_score", "auc_roc"]
        elif data_type == "moderate_imbalance":
            base_metrics = ["f1_score", "auc_roc", "precision", "recall"]
        else:  # high_imbalance
            base_metrics = ["auc_pr", "f1_score", "precision", "recall"]
        
        # Add business context metrics
        if business_context == "high_precision":
            if "precision" not in base_metrics:
                base_metrics.append("precision")
        elif business_context == "high_recall":
            if "recall" not in base_metrics:
                base_metrics.append("recall")
        
        # Add model-specific metrics
        if model_type == "probabilistic":
            if "auc_roc" not in base_metrics:
                base_metrics.append("auc_roc")
            base_metrics.append("log_loss")
        
        return base_metrics
    
    def calculate_metrics(self, y_true, y_pred, y_scores, selected_metrics):
        """Calculate selected metrics"""
        results = {}
        
        for metric in selected_metrics:
            if metric == "accuracy":
                results[metric] = accuracy_score(y_true, y_pred)
            elif metric == "precision":
                results[metric] = precision_score(y_true, y_pred, zero_division=0)
            elif metric == "recall":
                results[metric] = recall_score(y_true, y_pred, zero_division=0)
            elif metric == "f1_score":
                results[metric] = f1_score(y_true, y_pred, zero_division=0)
            elif metric == "auc_roc":
                results[metric] = roc_auc_score(y_true, y_scores)
            elif metric == "auc_pr":
                results[metric] = average_precision_score(y_true, y_scores)
            elif metric == "mcc":
                results[metric] = matthews_corrcoef(y_true, y_pred)
            elif metric == "log_loss":
                results[metric] = log_loss(y_true, y_scores)
        
        return results
    
    def generate_recommendations(self, data_characteristics, business_context, model_type):
        """Generate metric selection recommendations"""
        
        recommendations = {
            "data_analysis": data_characteristics,
            "business_context": business_context,
            "model_type": model_type,
            "selected_metrics": self.select_metrics(data_characteristics, business_context, model_type)
        }
        
        # Add explanations
        explanations = {
            "balanced": "Accuracy and F1-score are reliable for balanced data",
            "moderate_imbalance": "Focus on F1-score and AUC-ROC, consider precision/recall",
            "high_imbalance": "Avoid accuracy, use AUC-PR and minority class metrics",
            "high_precision": "Prioritize precision to minimize false positives",
            "high_recall": "Prioritize recall to minimize false negatives",
            "probabilistic": "Include AUC-ROC and log-loss for probability evaluation"
        }
        
        recommendations["explanations"] = explanations
        
        return recommendations
    
    def plot_metric_comparison(self, results_dict):
        """Plot comparison of different metric selections"""
        
        scenarios = list(results_dict.keys())
        metrics = set()
        for scenario_results in results_dict.values():
            metrics.update(scenario_results.keys())
        metrics = sorted(list(metrics))
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        x = np.arange(len(scenarios))
        width = 0.8 / len(metrics)
        
        for i, metric in enumerate(metrics):
            values = [results_dict[scenario].get(metric, 0) for scenario in scenarios]
            ax.bar(x + i * width, values, width, label=metric.replace('_', ' ').title())
        
        ax.set_xlabel('Scenarios')
        ax.set_ylabel('Metric Value')
        ax.set_title('Metric Comparison Across Different Scenarios')
        ax.set_xticks(x + width * (len(metrics) - 1) / 2)
        ax.set_xticklabels(scenarios, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

def classification_metrics_demo():
    """Demonstrate metric selection for different scenarios"""
    
    # Create different datasets
    scenarios = {
        "Balanced Data": make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.5, 0.5], random_state=42),
        "Moderate Imbalance": make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.8, 0.2], random_state=42),
        "High Imbalance": make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.95, 0.05], random_state=42)
    }
    
    selector = ClassificationMetricSelector()
    results = {}
    
    for scenario_name, (X, y) in scenarios.items():
        print(f"\n=== {scenario_name.upper()} ===")
        
        # Analyze data characteristics
        characteristics = selector.analyze_data_characteristics(y)
        print(f"Data type: {characteristics['data_type']}")
        print(f"Imbalance ratio: {characteristics['imbalance_ratio']:.1f}:1")
        print(f"Class distribution: {characteristics['class_distribution']}")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        
        # Train models
        lr_model = LogisticRegression(random_state=42)
        lr_model.fit(X_train, y_train)
        
        # Predictions
        y_pred = lr_model.predict(X_test)
        y_scores = lr_model.predict_proba(X_test)[:, 1]
        
        # Select metrics based on characteristics
        selected_metrics = selector.select_metrics(characteristics, "balanced", "probabilistic")
        print(f"Selected metrics: {selected_metrics}")
        
        # Calculate metrics
        scenario_results = selector.calculate_metrics(y_test, y_pred, y_scores, selected_metrics)
        results[scenario_name] = scenario_results
        
        # Print results
        print("Metric values:")
        for metric, value in scenario_results.items():
            print(f"  {metric}: {value:.4f}")
    
    # Plot comparison
    selector.plot_metric_comparison(results)
    
    return selector, results

if __name__ == "__main__":
    selector, results = classification_metrics_demo()
```

### üîç Èù¢ËØïÂ∏∏ËßÅÈóÆÈ¢òÂèäÂõûÁ≠î

#### Q1: "How do you choose metrics for a classification problem?"

**English Answer:**
I choose metrics based on three key factors: data balance, business objectives, and error costs. For balanced data, I use accuracy and F1-score. For imbalanced data, I prioritize F1-score, AUC-ROC, and AUC-PR. When false positives are costly, I focus on precision. When false negatives are dangerous, I prioritize recall. I always consider the business context and what type of error is more harmful.

#### Q2: "What's the difference between choosing metrics for balanced vs imbalanced data?"

**English Answer:**
For balanced data, accuracy is meaningful and I can use it along with F1-score and AUC-ROC. For imbalanced data, accuracy becomes misleading because it's dominated by the majority class, so I avoid it and instead use F1-score, AUC-PR, and focus on precision and recall for the minority class. The key difference is that imbalanced data requires metrics that specifically measure minority class performance.

#### Q3: "When would you use AUC-ROC vs precision-recall metrics?"

**English Answer:**
I use AUC-ROC when I want to measure overall ranking ability across all thresholds, especially for balanced data. I use precision-recall metrics (AUC-PR, precision, recall, F1-score) when I'm specifically concerned about the positive class performance, which is crucial for imbalanced data. AUC-PR is particularly useful for highly imbalanced datasets where AUC-ROC can be overly optimistic.

### üí° ÂÆûÊàòÊäÄÂ∑ß

#### 1. ÈÄâÊã©ÊµÅÁ®ã (Selection Process)
1. **ÂàÜÊûêÊï∞ÊçÆÁâπÂæÅ** (Analyze data characteristics)
2. **Á°ÆÂÆö‰∏öÂä°ÁõÆÊ†á** (Identify business objectives)
3. **ËØÑ‰º∞ÈîôËØØÊàêÊú¨** (Assess error costs)
4. **ÈÄâÊã©‰∏ªË¶ÅÊåáÊ†á** (Select primary metrics)
5. **ÈÄâÊã©ËæÖÂä©ÊåáÊ†á** (Select secondary metrics)

#### 2. ÂÖ≥ÈîÆËØç (Key Terms)
- **Data Balance**: Êï∞ÊçÆÂπ≥Ë°°ÊÄß
- **Business Context**: ‰∏öÂä°ËÉåÊôØ
- **Error Cost**: ÈîôËØØÊàêÊú¨
- **Primary Metrics**: ‰∏ªË¶ÅÊåáÊ†á
- **Threshold Strategy**: ÈòàÂÄºÁ≠ñÁï•

#### 3. Â∏∏ËßÅÈô∑Èò± (Common Pitfalls)
- ‚ùå ‰∏çËÄÉËôëÊï∞ÊçÆÂπ≥Ë°°ÊÄß
- ‚ùå ÂøΩÁï•‰∏öÂä°ÈúÄÊ±Ç
- ‚ùå ‰ΩøÁî®‰∏çÂêàÈÄÇÁöÑÊåáÊ†á
- ‚ùå ‰∏çËÄÉËôëÈîôËØØÊàêÊú¨Â∑ÆÂºÇ

### üìä ÂèØËßÜÂåñÁêÜËß£

#### ÂàÜÁ±ªÊåáÊ†áÈÄâÊã©ÊµÅÁ®ãÂõæ
![ÂàÜÁ±ªÊåáÊ†áÈÄâÊã©ÊµÅÁ®ãÂõæ](../../images/metrics/classification_metrics_selection.png)

#### ‰∏çÂêåÂú∫ÊôØÁöÑÊåáÊ†áÂØπÊØî
![‰∏çÂêåÂú∫ÊôØÊåáÊ†áÂØπÊØî](../../images/metrics/scenario_metrics_comparison.png)

### üìä Èù¢ËØïÂáÜÂ§áÊ£ÄÊü•Ê∏ÖÂçï

- [ ] ÁêÜËß£‰∏çÂêåÊï∞ÊçÆÂπ≥Ë°°ÊÄßÁöÑÊåáÊ†áÈÄâÊã©
- [ ] ÊéåÊè°‰∏öÂä°Âú∫ÊôØÂØπÊåáÊ†áÈÄâÊã©ÁöÑÂΩ±Âìç
- [ ] Áü•ÈÅìÊ®°ÂûãÁ±ªÂûãÂØπÊåáÊ†áÈÄâÊã©ÁöÑÂΩ±Âìç
- [ ] ÁêÜËß£ÈîôËØØÊàêÊú¨ÁöÑÈáçË¶ÅÊÄß
- [ ] ÊéåÊè°‰∏ªË¶ÅÊåáÊ†áÂíåËæÖÂä©ÊåáÊ†áÁöÑÈÄâÊã©
- [ ] ËÉΩÂ§üËß£ÈáäÊåáÊ†áÈÄâÊã©ÁöÑÁêÜÁî±
- [ ] ÁêÜËß£ÈòàÂÄºÁ≠ñÁï•ÁöÑ‰ΩúÁî®
- [ ] Áü•ÈÅìÂ¶Ç‰ΩïÂ§ÑÁêÜÁâπÊÆäÊÉÖÂÜµ

### üéØ ÁªÉ‰π†Âª∫ËÆÆ

1. **ÁêÜËÆ∫ÁªÉ‰π†**: ÁêÜËß£‰∏çÂêåÊåáÊ†áÁöÑÈÄÇÁî®Âú∫ÊôØ
2. **ÂàÜÊûêÁªÉ‰π†**: ÂàÜÊûê‰∏çÂêå‰∏öÂä°Âú∫ÊôØÁöÑÊåáÊ†áÈúÄÊ±Ç
3. **ÂÆûÈ™åÁªÉ‰π†**: Âú®‰∏çÂêåÊï∞ÊçÆ‰∏äÊµãËØïÊåáÊ†áÈÄâÊã©
4. **Ê°à‰æãÁªÉ‰π†**: ÂàÜÊûêÁúüÂÆûÈ°πÁõÆÁöÑÊåáÊ†áÈÄâÊã©
5. **ÁªºÂêàÁªÉ‰π†**: ËÆæËÆ°ÂÆåÊï¥ÁöÑÊåáÊ†áÈÄâÊã©ÊµÅÁ®ã

**ËÆ∞‰Ωè**: ÂàÜÁ±ªÊåáÊ†áÈÄâÊã©Ë¶ÅÁªºÂêàËÄÉËôëÊï∞ÊçÆÁâπÂæÅ„ÄÅ‰∏öÂä°ÁõÆÊ†áÂíåÈîôËØØÊàêÊú¨ÔºÅ
