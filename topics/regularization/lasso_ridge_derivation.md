# æ­£åˆ™åŒ–ä¸“é¢˜ - è¯¦ç»†ç­”æ¡ˆ

## é—®é¢˜: Lasso/Ridge çš„æ•°å­¦æŽ¨å¯¼

### ðŸŽ¯ ä¸­æ–‡ç†è§£ (ä¾¿äºŽè®°å¿†)

#### æ•°å­¦æŽ¨å¯¼ = "ä»ŽåŽŸç†åˆ°å®žçŽ°"
æƒ³è±¡è§£å†³æ•°å­¦é¢˜çš„è¿‡ç¨‹ï¼š
- **LassoæŽ¨å¯¼**ï¼šåƒè§£"ç»å¯¹å€¼æ–¹ç¨‹"ï¼Œéœ€è¦åˆ†æƒ…å†µè®¨è®º
  - æ–¹æ³•ï¼šåæ ‡ä¸‹é™æ³•ï¼Œé€ä¸ªä¼˜åŒ–æ¯ä¸ªå‚æ•°
  - å…³é”®ï¼šè½¯é˜ˆå€¼å‡½æ•°
  
- **RidgeæŽ¨å¯¼**ï¼šåƒè§£"äºŒæ¬¡æ–¹ç¨‹"ï¼Œæœ‰æ ‡å‡†å…¬å¼
  - æ–¹æ³•ï¼šç›´æŽ¥æ±‚å¯¼ï¼Œé—­å¼è§£
  - å…³é”®ï¼šçŸ©é˜µæ±‚é€†

#### æŽ¨å¯¼è¿‡ç¨‹
1. **å»ºç«‹ç›®æ ‡å‡½æ•°**ï¼šæŸå¤±å‡½æ•° + æ­£åˆ™åŒ–é¡¹
2. **æ±‚æ¢¯åº¦**ï¼šå¯¹æ¯ä¸ªå‚æ•°æ±‚åå¯¼
3. **æ±‚è§£**ï¼šLassoç”¨è½¯é˜ˆå€¼ï¼ŒRidgeç”¨çŸ©é˜µæ±‚é€†

### ðŸŽ¤ ç›´æŽ¥é¢è¯•å›žç­” (Direct Interview Answer)

**Lasso uses coordinate descent optimization with soft thresholding. The gradient is discontinuous at zero, so we solve âˆ‚L/âˆ‚w_j = -(1/n)X_j^T(y - Xw) + Î» sign(w_j) = 0, leading to the soft thresholding rule: w_j = S(rho_j, Î») where S(a,b) = sign(a)max(|a|-b,0).**

**Ridge has a closed-form solution through matrix calculus. Setting the gradient âˆ‚L/âˆ‚w = -(1/n)X^T(y - Xw) + Î»w = 0 and solving gives w = (X^T X + Î»I)^(-1) X^T y, where the regularization term Î»I ensures the matrix is invertible even when X^T X is singular.**

**The key difference is computational complexity:** Lasso requires iterative coordinate descent because the absolute value is non-differentiable at zero, while Ridge has a direct matrix solution because the squared penalty is smooth everywhere.

---

### ðŸ“ è‹±æ–‡æ ‡å‡†é¢è¯•ç­”æ¡ˆ (English Interview Answer)

#### 1. Lasso Mathematical Derivation

**Step 1: Objective Function**
```python
L(w) = (1/2n)||y - Xw||Â² + Î»||w||â‚
L(w) = (1/2n)(y - Xw)^T(y - Xw) + Î»âˆ‘|w_j|
```

**Step 2: Gradient with Respect to w_j**
```python
âˆ‚L/âˆ‚w_j = -(1/n)X_j^T(y - Xw) + Î» âˆ‚|w_j|/âˆ‚w_j
```

**Step 3: Subgradient for Absolute Value**
```python
âˆ‚|w_j|/âˆ‚w_j = sign(w_j) = {
    1,  if w_j > 0
    -1, if w_j < 0
    [-1, 1], if w_j = 0  # Subgradient
}
```

**Step 4: Coordinate Descent Update**
```python
# For each coordinate j, set âˆ‚L/âˆ‚w_j = 0
-(1/n)X_j^T(y - Xw) + Î» sign(w_j) = 0

# Define rho_j = (1/n)X_j^T(y - Xw_{-j})
# where w_{-j} means w without the j-th component
```

**Step 5: Soft Thresholding Solution**
```python
# The solution is the soft thresholding function:
w_j = S(rho_j, Î») = sign(rho_j) * max(|rho_j| - Î», 0)

# Where S(a, b) is the soft thresholding operator:
S(a, b) = {
    a - b,  if a > b
    a + b,  if a < -b
    0,      if |a| â‰¤ b
}
```

#### 2. Ridge Mathematical Derivation

**Step 1: Objective Function**
```python
L(w) = (1/2n)||y - Xw||Â² + Î»||w||â‚‚Â²
L(w) = (1/2n)(y - Xw)^T(y - Xw) + Î» w^T w
```

**Step 2: Gradient**
```python
âˆ‚L/âˆ‚w = -(1/n)X^T(y - Xw) + Î»w
```

**Step 3: Set Gradient to Zero**
```python
-(1/n)X^T(y - Xw) + Î»w = 0
-(1/n)X^T y + (1/n)X^T X w + Î»w = 0
(1/n)X^T X w + Î»w = (1/n)X^T y
```

**Step 4: Factor Out w**
```python
[(1/n)X^T X + Î»I] w = (1/n)X^T y
```

**Step 5: Closed-Form Solution**
```python
w = [(1/n)X^T X + Î»I]^(-1) (1/n)X^T y
w = [X^T X + nÎ»I]^(-1) X^T y
```

#### 3. Detailed Mathematical Steps

**Lasso Coordinate Descent Algorithm:**
```python
def lasso_coordinate_descent(X, y, lambda_reg, max_iter=1000, tol=1e-4):
    """
    Lasso coordinate descent with mathematical derivation
    """
    n, p = X.shape
    w = np.zeros(p)
    
    for iteration in range(max_iter):
        w_old = w.copy()
        
        for j in range(p):
            # Calculate partial residual: r = y - Xw + w_j * X_j
            r = y - X @ w + w[j] * X[:, j]
            
            # Calculate rho_j = (1/n) * X_j^T * r
            rho_j = X[:, j].T @ r / n
            
            # Soft thresholding: w_j = S(rho_j, Î»)
            if rho_j > lambda_reg:
                w[j] = rho_j - lambda_reg
            elif rho_j < -lambda_reg:
                w[j] = rho_j + lambda_reg
            else:
                w[j] = 0
        
        # Check convergence
        if np.max(np.abs(w - w_old)) < tol:
            break
    
    return w
```

**Ridge Closed-Form Solution:**
```python
def ridge_closed_form(X, y, lambda_reg):
    """
    Ridge regression closed-form solution
    """
    n, p = X.shape
    
    # Method 1: Direct matrix inversion
    XTX = X.T @ X
    XTy = X.T @ y
    A = XTX + n * lambda_reg * np.eye(p)
    w = np.linalg.solve(A, XTy)
    
    return w

def ridge_closed_form_stable(X, y, lambda_reg):
    """
    Numerically stable Ridge solution using SVD
    """
    U, s, Vt = np.linalg.svd(X, full_matrices=False)
    
    # Ridge shrinkage: s_j^2 / (s_j^2 + nÎ»)
    shrinkage = s**2 / (s**2 + X.shape[0] * lambda_reg)
    
    # Compute weights
    w = Vt.T @ np.diag(shrinkage) @ U.T @ y
    
    return w
```

#### 4. Mathematical Properties

**Lasso Properties:**
- **Non-differentiable** at w_j = 0
- **Coordinate-wise separable** objective
- **Convergence guaranteed** under certain conditions
- **Multiple solutions** possible (non-unique)

**Ridge Properties:**
- **Everywhere differentiable**
- **Unique solution** (strictly convex)
- **Numerically stable** with proper regularization
- **Matrix inversion** required

#### 5. Convergence Analysis

**Lasso Convergence:**
```python
# Convergence condition: max eigenvalue of X^T X / n < Î»
# Under this condition, coordinate descent converges to global optimum
```

**Ridge Convergence:**
```python
# Always converges due to strict convexity
# Condition number: Îº(X^T X + Î»I) â‰¤ Îº(X^T X)
# Regularization improves numerical stability
```

### ðŸ’» å®žé™…ä»£ç ç¤ºä¾‹

#### Complete Implementation with Mathematical Details
```python
import numpy as np
from scipy.linalg import solve_triangular
import matplotlib.pyplot as plt

class MathematicalLasso:
    def __init__(self, alpha=1.0, max_iter=1000, tol=1e-4):
        self.alpha = alpha
        self.max_iter = max_iter
        self.tol = tol
        self.coef_ = None
        self.n_iter_ = 0
    
    def _soft_threshold(self, rho, alpha):
        """Soft thresholding function: S(rho, alpha)"""
        return np.sign(rho) * np.maximum(np.abs(rho) - alpha, 0)
    
    def fit(self, X, y):
        """Fit Lasso using coordinate descent"""
        n, p = X.shape
        w = np.zeros(p)
        
        # Store convergence history
        self.convergence_history = []
        
        for iteration in range(self.max_iter):
            w_old = w.copy()
            
            for j in range(p):
                # Calculate partial residual: r = y - Xw + w_j * X_j
                r = y - X @ w + w[j] * X[:, j]
                
                # Calculate rho_j = (1/n) * X_j^T * r
                rho_j = X[:, j].T @ r / n
                
                # Soft thresholding update
                w[j] = self._soft_threshold(rho_j, self.alpha)
            
            # Check convergence
            max_change = np.max(np.abs(w - w_old))
            self.convergence_history.append(max_change)
            
            if max_change < self.tol:
                break
        
        self.coef_ = w
        self.n_iter_ = iteration + 1
        
        return self
    
    def predict(self, X):
        """Make predictions"""
        return X @ self.coef_

class MathematicalRidge:
    def __init__(self, alpha=1.0, solver='auto'):
        self.alpha = alpha
        self.solver = solver
        self.coef_ = None
    
    def fit(self, X, y):
        """Fit Ridge using closed-form solution"""
        n, p = X.shape
        
        if self.solver == 'auto':
            # Use direct solution for small problems
            if p < 1000:
                self.coef_ = self._direct_solution(X, y)
            else:
                self.coef_ = self._iterative_solution(X, y)
        elif self.solver == 'direct':
            self.coef_ = self._direct_solution(X, y)
        elif self.solver == 'svd':
            self.coef_ = self._svd_solution(X, y)
        
        return self
    
    def _direct_solution(self, X, y):
        """Direct matrix solution"""
        XTX = X.T @ X
        XTy = X.T @ y
        A = XTX + X.shape[0] * self.alpha * np.eye(X.shape[1])
        return np.linalg.solve(A, XTy)
    
    def _svd_solution(self, X, y):
        """SVD-based solution for numerical stability"""
        U, s, Vt = np.linalg.svd(X, full_matrices=False)
        shrinkage = s**2 / (s**2 + X.shape[0] * self.alpha)
        return Vt.T @ np.diag(shrinkage) @ U.T @ y
    
    def predict(self, X):
        """Make predictions"""
        return X @ self.coef_

# Mathematical derivation demonstration
def mathematical_derivation_demo():
    """Demonstrate the mathematical derivation"""
    # Generate data
    np.random.seed(42)
    n, p = 50, 10
    X = np.random.randn(n, p)
    true_w = np.random.randn(p)
    true_w[p//2:] = 0  # Make sparse
    y = X @ true_w + 0.1 * np.random.randn(n)
    
    # Fit models
    lasso = MathematicalLasso(alpha=0.1)
    ridge = MathematicalRidge(alpha=0.1)
    
    lasso.fit(X, y)
    ridge.fit(X, y)
    
    print("=== MATHEMATICAL DERIVATION RESULTS ===")
    print(f"True coefficients: {true_w}")
    print(f"Lasso coefficients: {lasso.coef_}")
    print(f"Ridge coefficients: {ridge.coef_}")
    print(f"Lasso iterations: {lasso.n_iter_}")
    print(f"Lasso sparsity: {np.sum(np.abs(lasso.coef_) < 1e-6)}/{p}")
    
    # Plot convergence
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    plt.plot(lasso.convergence_history)
    plt.title('Lasso Convergence')
    plt.xlabel('Iteration')
    plt.ylabel('Max Parameter Change')
    plt.yscale('log')
    
    plt.subplot(1, 2, 2)
    plt.bar(range(p), true_w, alpha=0.7, label='True')
    plt.bar(range(p), lasso.coef_, alpha=0.7, label='Lasso')
    plt.bar(range(p), ridge.coef_, alpha=0.7, label='Ridge')
    plt.title('Coefficient Comparison')
    plt.xlabel('Feature')
    plt.ylabel('Coefficient Value')
    plt.legend()
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    mathematical_derivation_demo()
```

### ðŸ” é¢è¯•å¸¸è§é—®é¢˜åŠå›žç­”

#### Q1: "Derive the Lasso solution step by step."

**English Answer:**
Start with the objective L(w) = (1/2n)||y - Xw||Â² + Î»||w||â‚. Take the gradient âˆ‚L/âˆ‚w_j = -(1/n)X_j^T(y - Xw) + Î» sign(w_j). Set to zero and solve: rho_j = (1/n)X_j^T(y - Xw_{-j}) = Î» sign(w_j). This gives the soft thresholding rule: w_j = S(rho_j, Î») = sign(rho_j)max(|rho_j|-Î», 0).

#### Q2: "Why does Lasso need iterative optimization while Ridge has a closed-form solution?"

**English Answer:**
Lasso's absolute value penalty is non-differentiable at zero, making it impossible to find a direct solution. We use coordinate descent with soft thresholding. Ridge's squared penalty is smooth everywhere, allowing us to take derivatives and solve directly: w = (X^T X + Î»I)^(-1) X^T y.

#### Q3: "What is the soft thresholding function and why is it important?"

**English Answer:**
The soft thresholding function S(a,b) = sign(a)max(|a|-b,0) is the solution to the Lasso optimization problem for each coordinate. It shrinks coefficients toward zero and sets small coefficients exactly to zero, enabling automatic feature selection. It's the mathematical foundation of Lasso's sparsity property.

### ðŸ’¡ å®žæˆ˜æŠ€å·§

#### 1. æŽ¨å¯¼æ­¥éª¤ (Derivation Steps)
1. **å»ºç«‹ç›®æ ‡å‡½æ•°** (Set up objective function)
2. **è®¡ç®—æ¢¯åº¦** (Compute gradient)
3. **æ±‚è§£æ–¹ç¨‹** (Solve the equation)
4. **åº”ç”¨è½¯é˜ˆå€¼** (Apply soft thresholding)

#### 2. å…³é”®è¯ (Key Terms)
- **Soft Thresholding**: è½¯é˜ˆå€¼
- **Coordinate Descent**: åæ ‡ä¸‹é™
- **Closed-form Solution**: é—­å¼è§£
- **Subgradient**: æ¬¡æ¢¯åº¦
- **Matrix Inversion**: çŸ©é˜µæ±‚é€†

#### 3. å¸¸è§é™·é˜± (Common Pitfalls)
- âŒ å¿½ç•¥ç»å¯¹å€¼çš„ä¸å¯å¯¼æ€§
- âŒ ä¸ç†è§£è½¯é˜ˆå€¼å‡½æ•°çš„ä½œç”¨
- âŒ æ··æ·†Lassoå’ŒRidgeçš„æ±‚è§£æ–¹æ³•
- âŒ ä¸è€ƒè™‘æ•°å€¼ç¨³å®šæ€§

### ðŸ“Š å¯è§†åŒ–ç†è§£

#### è½¯é˜ˆå€¼å‡½æ•°å¯è§†åŒ–
![è½¯é˜ˆå€¼å‡½æ•°](../../images/basic_ml/l1_l2_regularization_comparison.png)

#### æ”¶æ•›è¿‡ç¨‹å¯è§†åŒ–
![æ”¶æ•›è¿‡ç¨‹](../../images/basic_ml/regularization_paths.png)

### ðŸ“Š é¢è¯•å‡†å¤‡æ£€æŸ¥æ¸…å•

- [ ] ç†è§£Lassoçš„åæ ‡ä¸‹é™æŽ¨å¯¼
- [ ] æŽŒæ¡è½¯é˜ˆå€¼å‡½æ•°çš„æ•°å­¦å½¢å¼
- [ ] ç†è§£Ridgeçš„é—­å¼è§£æŽ¨å¯¼
- [ ] çŸ¥é“ä¸¤ç§æ–¹æ³•çš„è®¡ç®—å¤æ‚åº¦
- [ ] ç†è§£æ”¶æ•›æ¡ä»¶
- [ ] æŽŒæ¡æ•°å€¼ç¨³å®šçš„å®žçŽ°æ–¹æ³•
- [ ] èƒ½å¤Ÿè§£é‡Šä¸ºä»€ä¹ˆLassoéœ€è¦è¿­ä»£
- [ ] ç†è§£çŸ©é˜µæ±‚é€†çš„æ•°å€¼é—®é¢˜

### ðŸŽ¯ ç»ƒä¹ å»ºè®®

1. **æŽ¨å¯¼ç»ƒä¹ **: æ‰‹æŽ¨Lassoå’ŒRidgeçš„æ•°å­¦å…¬å¼
2. **å®žçŽ°ç»ƒä¹ **: ç¼–å†™åæ ‡ä¸‹é™å’Œé—­å¼è§£ç®—æ³•
3. **æ•°å€¼ç»ƒä¹ **: æ¯”è¾ƒä¸åŒæ±‚è§£æ–¹æ³•çš„æ•°å€¼ç¨³å®šæ€§
4. **æ”¶æ•›ç»ƒä¹ **: åˆ†æžæ”¶æ•›æ¡ä»¶å’Œé€Ÿåº¦
5. **åº”ç”¨ç»ƒä¹ **: åœ¨çœŸå®žæ•°æ®ä¸ŠéªŒè¯æŽ¨å¯¼ç»“æžœ

**è®°ä½**: æ•°å­¦æŽ¨å¯¼æ˜¯ç†è§£ç®—æ³•çš„å…³é”®ï¼ŒLassoç”¨è½¯é˜ˆå€¼ï¼ŒRidgeç”¨çŸ©é˜µæ±‚é€†ï¼
