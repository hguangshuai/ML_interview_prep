# æ­£åˆ™åŒ–ä¸“é¢˜ - è¯¦ç»†ç­”æ¡ˆ

## é—®é¢˜: L1 vs L2 æ­£åˆ™åŒ–çš„åŒºåˆ«å’Œå¯¹æ¯”

### ğŸ¯ ä¸­æ–‡ç†è§£ (ä¾¿äºè®°å¿†)

#### L1 vs L2 = "ä¸åŒçš„æƒ©ç½šæ–¹å¼"
æƒ³è±¡ä¸¤ç§ä¸åŒçš„æƒ©ç½šåˆ¶åº¦ï¼š
- **L1æ­£åˆ™åŒ– (Lasso)**ï¼šåƒ"ä¸€åˆ€åˆ‡"ï¼Œç›´æ¥åˆ é™¤ä¸é‡è¦çš„ç‰¹å¾
  - æ¯”å–»ï¼šè€å¸ˆè¦æ±‚åˆ é™¤æ‰€æœ‰ä¸ç›¸å…³çš„çŸ¥è¯†ç‚¹
  - ç»“æœï¼šäº§ç”Ÿç¨€ç–è§£ï¼Œå¾ˆå¤šæƒé‡å˜ä¸º0
  
- **L2æ­£åˆ™åŒ– (Ridge)**ï¼šåƒ"æ¸©å’Œæé†’"ï¼Œè®©æ‰€æœ‰æƒé‡éƒ½å˜å°
  - æ¯”å–»ï¼šè€å¸ˆè¦æ±‚æ‰€æœ‰çŸ¥è¯†ç‚¹éƒ½è¦å­¦ï¼Œä½†ä¸è¦é’»ç‰›è§’å°–
  - ç»“æœï¼šæ‰€æœ‰æƒé‡éƒ½ä¿ç•™ï¼Œä½†æ•°å€¼å˜å°

#### æ ¸å¿ƒåŒºåˆ«
1. **æƒ©ç½šå‡½æ•°**ï¼šL1ç”¨ç»å¯¹å€¼ï¼ŒL2ç”¨å¹³æ–¹
2. **ç¨€ç–æ€§**ï¼šL1äº§ç”Ÿç¨€ç–è§£ï¼ŒL2ä¸äº§ç”Ÿ
3. **ç‰¹å¾é€‰æ‹©**ï¼šL1è‡ªåŠ¨é€‰æ‹©ï¼ŒL2éœ€è¦æ‰‹åŠ¨
4. **è®¡ç®—å¤æ‚åº¦**ï¼šL1åœ¨é›¶ç‚¹ä¸å¯å¯¼ï¼ŒL2å¤„å¤„å¯å¯¼

### ğŸ¤ ç›´æ¥é¢è¯•å›ç­” (Direct Interview Answer)

**L1 and L2 regularization differ fundamentally in their penalty functions and effects on model coefficients. L1 regularization uses absolute values of weights (Î»âˆ‘|w_i|), creating sparse solutions where many coefficients become exactly zero, performing automatic feature selection. L2 regularization uses squared weights (Î»âˆ‘w_iÂ²), shrinking all coefficients toward zero while keeping them non-zero.**

**The mathematical difference is:** L1 penalty = Î»âˆ‘|w_i| vs L2 penalty = Î»âˆ‘w_iÂ². L1's absolute value creates sharp corners at zero, forcing some weights to exactly zero during optimization, while L2's smooth penalty curve keeps all weights small but non-zero.

**Geometrically, L1 forms a diamond-shaped constraint region** (L1 ball) that intersects the loss function at sharp corners, naturally leading to sparse solutions. **L2 forms a circular constraint region** (L2 ball) that intersects smoothly, keeping all coefficients.

**I choose L1 when I need automatic feature selection** and interpretable models with fewer features. **I choose L2 when I want to prevent overfitting** while keeping all features, especially when features are correlated.

---

### ğŸ“ è‹±æ–‡æ ‡å‡†é¢è¯•ç­”æ¡ˆ (English Interview Answer)

#### 1. Mathematical Definitions

**L1 Regularization (Lasso):**
```python
Loss = MSE + Î»âˆ‘|w_i|
```

**L2 Regularization (Ridge):**
```python
Loss = MSE + Î»âˆ‘w_iÂ²
```

#### 2. Detailed Comparison

| Aspect | L1 Regularization | L2 Regularization |
|--------|------------------|------------------|
| **Penalty Function** | |w_i| | w_iÂ² |
| **Gradient** | sign(w_i) | 2w_i |
| **Sparsity** | Creates sparse solutions | No sparsity |
| **Feature Selection** | Automatic | Manual required |
| **Computational** | Non-differentiable at 0 | Everywhere differentiable |
| **Geometric Shape** | Diamond (L1 ball) | Circle (L2 ball) |
| **Optimization** | Coordinate descent | Gradient descent |
| **Solution Type** | Multiple solutions possible | Unique solution |

#### 3. Mathematical Properties

**L1 Regularization Properties:**
- **Convex but not strictly convex**
- **Non-differentiable at zero**
- **Produces sparse solutions**
- **Multiple optimal solutions possible**

**L2 Regularization Properties:**
- **Strictly convex**
- **Everywhere differentiable**
- **Unique optimal solution**
- **No sparsity**

#### 4. Geometric Interpretation

**L1 Ball (Diamond):**
```python
# L1 constraint: |wâ‚| + |wâ‚‚| â‰¤ t
# Forms a diamond shape in 2D
# Intersects loss function at corners (sparse solutions)
```

**L2 Ball (Circle):**
```python
# L2 constraint: wâ‚Â² + wâ‚‚Â² â‰¤ t
# Forms a circle in 2D
# Intersects loss function smoothly (no sparsity)
```

### ğŸ’» å®é™…ä»£ç ç¤ºä¾‹

#### L1 vs L2 Implementation
```python
import numpy as np
from sklearn.linear_model import Lasso, Ridge
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

def l1_l2_comparison_example():
    """Compare L1 and L2 regularization"""
    # Generate synthetic data
    np.random.seed(42)
    n, p = 100, 20
    X = np.random.randn(n, p)
    
    # Create sparse true weights
    true_weights = np.zeros(p)
    true_weights[:5] = [2, -1.5, 1, -0.5, 0.8]
    
    # Generate target
    y = X @ true_weights + 0.1 * np.random.randn(n)
    
    # Standardize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Fit models
    lasso = Lasso(alpha=0.1)
    ridge = Ridge(alpha=0.1)
    
    lasso.fit(X_scaled, y)
    ridge.fit(X_scaled, y)
    
    # Compare results
    print("Feature Selection Results:")
    print(f"L1 non-zero features: {np.sum(np.abs(lasso.coef_) > 1e-6)}")
    print(f"L2 non-zero features: {np.sum(np.abs(ridge.coef_) > 1e-6)}")
    
    print("\nCoefficient Values (first 10):")
    print("True:", true_weights[:10])
    print("L1:  ", lasso.coef_[:10])
    print("L2:  ", ridge.coef_[:10])

# Visualization function
def plot_l1_l2_penalties():
    """Plot L1 and L2 penalty functions"""
    w = np.linspace(-3, 3, 1000)
    
    l1_penalty = np.abs(w)
    l2_penalty = w**2
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(w, l1_penalty, 'b-', linewidth=3, label='L1: |w|')
    plt.plot(w, l2_penalty, 'r-', linewidth=3, label='L2: wÂ²')
    plt.title('Penalty Functions')
    plt.xlabel('Weight Value')
    plt.ylabel('Penalty')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    l1_grad = np.sign(w)
    l2_grad = 2 * w
    plt.plot(w, l1_grad, 'b-', linewidth=3, label='L1: sign(w)')
    plt.plot(w, l2_grad, 'r-', linewidth=3, label='L2: 2w')
    plt.title('Gradient Functions')
    plt.xlabel('Weight Value')
    plt.ylabel('Gradient')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    l1_l2_comparison_example()
    plot_l1_l2_penalties()
```

### ğŸ” é¢è¯•å¸¸è§é—®é¢˜åŠå›ç­”

#### Q1: "What's the main difference between L1 and L2 regularization?"

**English Answer:**
The main difference is in the penalty function: L1 uses absolute values (|w|) creating sparse solutions with automatic feature selection, while L2 uses squared values (wÂ²) shrinking all coefficients without sparsity. L1 is non-differentiable at zero, L2 is smooth everywhere.

#### Q2: "When would you choose L1 over L2 regularization?"

**English Answer:**
I choose L1 when I need automatic feature selection, want interpretable models with fewer features, or have high-dimensional data where many features might be irrelevant. L1 is particularly useful for variable selection in regression problems.

#### Q3: "Why does L1 create sparse solutions while L2 doesn't?"

**English Answer:**
L1's absolute penalty function has sharp corners at zero where the gradient is discontinuous. During optimization, when a coefficient approaches zero, the L1 penalty can "push" it to exactly zero. L2's smooth penalty makes it difficult to reach exactly zero.

### ğŸ’¡ å®æˆ˜æŠ€å·§

#### 1. é€‰æ‹©æ ‡å‡† (Selection Criteria)
- **ä½¿ç”¨L1**ï¼šéœ€è¦ç‰¹å¾é€‰æ‹©ã€é«˜ç»´æ•°æ®ã€å¯è§£é‡Šæ€§é‡è¦
- **ä½¿ç”¨L2**ï¼šç‰¹å¾ç›¸å…³ã€éœ€è¦ä¿ç•™æ‰€æœ‰ç‰¹å¾ã€æ•°å€¼ç¨³å®šæ€§é‡è¦

#### 2. å…³é”®è¯ (Key Terms)
- **Sparsity**: ç¨€ç–æ€§
- **Feature Selection**: ç‰¹å¾é€‰æ‹©
- **L1 Ball**: L1çƒ
- **L2 Ball**: L2çƒ
- **Non-differentiable**: ä¸å¯å¯¼
- **Sharp Corner**: å°–é”æ‹è§’

#### 3. å¸¸è§é™·é˜± (Common Pitfalls)
- âŒ æ··æ·†L1å’ŒL2çš„å‡ ä½•å½¢çŠ¶
- âŒ ä¸ç†è§£ä¸ºä»€ä¹ˆL1äº§ç”Ÿç¨€ç–è§£
- âŒ å¿½ç•¥æ•°æ®æ ‡å‡†åŒ–çš„é‡è¦æ€§
- âŒ ä¸è€ƒè™‘è®¡ç®—å¤æ‚åº¦å·®å¼‚

### ğŸ“Š å¯è§†åŒ–ç†è§£

#### L1 vs L2 æ­£åˆ™åŒ–å¯¹æ¯”
![L1 vs L2 æ­£åˆ™åŒ–å¯¹æ¯”](../../images/basic_ml/l1_l2_regularization_comparison.png)

#### ç¨€ç–æ€§å¯è§†åŒ–
![ç¨€ç–æ€§å¯è§†åŒ–](../../images/basic_ml/sparsity_visualization.png)

### ğŸ“Š é¢è¯•å‡†å¤‡æ£€æŸ¥æ¸…å•

- [ ] ç†è§£L1å’ŒL2æƒ©ç½šå‡½æ•°çš„æ•°å­¦å®šä¹‰
- [ ] æŒæ¡å‡ ä½•å½¢çŠ¶å·®å¼‚ï¼ˆè±å½¢ vs åœ†å½¢ï¼‰
- [ ] ç†è§£ä¸ºä»€ä¹ˆL1äº§ç”Ÿç¨€ç–è§£
- [ ] çŸ¥é“ä½•æ—¶é€‰æ‹©L1æˆ–L2
- [ ] æŒæ¡è®¡ç®—å¤æ‚åº¦å·®å¼‚
- [ ] ç†è§£ä¼˜åŒ–ç®—æ³•çš„å·®å¼‚
- [ ] èƒ½å¤Ÿå®ç°ç®€å•çš„L1/L2ç®—æ³•
- [ ] çŸ¥é“å®é™…åº”ç”¨åœºæ™¯

### ğŸ¯ ç»ƒä¹ å»ºè®®

1. **ç†è®ºç»ƒä¹ **: ç†è§£L1å’ŒL2çš„æ•°å­¦æ€§è´¨
2. **å‡ ä½•ç»ƒä¹ **: ç»˜åˆ¶L1çƒå’ŒL2çƒçš„å½¢çŠ¶
3. **ä»£ç ç»ƒä¹ **: å®ç°ç®€å•çš„L1/L2æ­£åˆ™åŒ–
4. **åº”ç”¨ç»ƒä¹ **: åœ¨çœŸå®æ•°æ®ä¸Šæ¯”è¾ƒæ•ˆæœ
5. **ä¼˜åŒ–ç»ƒä¹ **: ç†è§£ä¸åŒä¼˜åŒ–ç®—æ³•çš„é€‚ç”¨æ€§

**è®°ä½**: L1ç”¨äºç‰¹å¾é€‰æ‹©ï¼ŒL2ç”¨äºæƒé‡æ”¶ç¼©ï¼Œé€‰æ‹©å“ªç§å–å†³äºå…·ä½“é—®é¢˜éœ€æ±‚ï¼
