# æ­£åˆ™åŒ–ä¸“é¢˜ - è¯¦ç»†ç­”æ¡ˆ

## é—®é¢˜: Lasso/Ridge çš„è§£é‡Šå’Œå…ˆéªŒåˆ†å¸ƒ

### ğŸ¯ ä¸­æ–‡ç†è§£ (ä¾¿äºè®°å¿†)

#### Lasso vs Ridge = "ä¸åŒçš„å­¦ä¹ ç­–ç•¥"
æƒ³è±¡ä¸¤ç§ä¸åŒçš„å­¦ä¹ æ–¹æ³•ï¼š
- **Lasso (L1)**ï¼šåƒ"é‡ç‚¹çªç ´"ï¼Œåªå­¦æœ€é‡è¦çš„çŸ¥è¯†ï¼Œå…¶ä»–ç›´æ¥å¿½ç•¥
  - å…ˆéªŒï¼šè®¤ä¸ºå¤§éƒ¨åˆ†ç‰¹å¾éƒ½æ˜¯æ— ç”¨çš„
  - ç»“æœï¼šè‡ªåŠ¨åˆ é™¤ä¸é‡è¦ç‰¹å¾
  
- **Ridge (L2)**ï¼šåƒ"å…¨é¢å­¦ä¹ "ï¼Œæ‰€æœ‰çŸ¥è¯†éƒ½è¦å­¦ï¼Œä½†ä¸è¦é’»ç‰›è§’å°–
  - å…ˆéªŒï¼šè®¤ä¸ºæ‰€æœ‰ç‰¹å¾éƒ½æœ‰ç”¨ï¼Œä½†ä¸è¦è¿‡åº¦
  - ç»“æœï¼šä¿ç•™æ‰€æœ‰ç‰¹å¾ï¼Œä½†æƒé‡å˜å°

#### å…ˆéªŒåˆ†å¸ƒ = "å­¦ä¹ å‰çš„å‡è®¾"
- **Lassoçš„å…ˆéªŒ**ï¼šLaplaceåˆ†å¸ƒï¼Œåœ¨é›¶ç‚¹æœ‰å°–é”å³°å€¼
- **Ridgeçš„å…ˆéªŒ**ï¼šGaussianåˆ†å¸ƒï¼Œå¹³æ»‘çš„é’Ÿå½¢æ›²çº¿

### ğŸ¤ ç›´æ¥é¢è¯•å›ç­” (Direct Interview Answer)

**Lasso (Least Absolute Shrinkage and Selection Operator) uses L1 regularization with a Laplace prior distribution that has sharp peaks at zero, encouraging sparsity and automatic feature selection. Ridge regression uses L2 regularization with a Gaussian prior distribution that is smooth and centered at zero, encouraging small but non-zero coefficients.**

**The prior distributions are:** Lasso corresponds to P(w) âˆ exp(-Î»|w|) (Laplace prior) which assumes most weights should be zero. Ridge corresponds to P(w) âˆ exp(-Î»wÂ²/2) (Gaussian prior) which assumes all weights should be small but non-zero.

**From a Bayesian perspective, regularization is equivalent to maximum a posteriori (MAP) estimation** where the regularization term acts as prior knowledge about the parameter distribution. Lasso's Laplace prior encourages sparsity, while Ridge's Gaussian prior encourages shrinkage.

**I choose Lasso when I believe many features are irrelevant** and want automatic feature selection. **I choose Ridge when I believe all features are potentially useful** but want to prevent overfitting through weight shrinkage.

---

### ğŸ“ è‹±æ–‡æ ‡å‡†é¢è¯•ç­”æ¡ˆ (English Interview Answer)

#### 1. Lasso Explanation

**Lasso (Least Absolute Shrinkage and Selection Operator):**

**Objective Function:**
```python
L(w) = (1/2n)||y - Xw||Â² + Î»||w||â‚
```

**Prior Distribution:**
```python
# Laplace (Double Exponential) Prior
P(w) âˆ exp(-Î»|w|)
```

**Characteristics:**
- **Mean**: 0
- **Variance**: 2/Î»Â²
- **Shape**: Sharp peak at zero, heavy tails
- **Effect**: Encourages sparsity, automatic feature selection

#### 2. Ridge Explanation

**Ridge Regression:**

**Objective Function:**
```python
L(w) = (1/2n)||y - Xw||Â² + Î»||w||â‚‚Â²
```

**Prior Distribution:**
```python
# Gaussian (Normal) Prior
P(w) âˆ exp(-Î»wÂ²/2)
```

**Characteristics:**
- **Mean**: 0
- **Variance**: 1/Î»
- **Shape**: Smooth bell curve
- **Effect**: No sparsity, shrinks all coefficients

#### 3. Bayesian Interpretation

**Maximum Likelihood Estimation (No Regularization):**
```python
w_ML = argmax P(y|X, w)
```

**Maximum A Posteriori Estimation (With Regularization):**
```python
w_MAP = argmax P(y|X, w) Ã— P(w)
```

**The regularization term P(w) represents prior knowledge:**
- **Lasso**: "Most weights should be zero"
- **Ridge**: "All weights should be small"

#### 4. Prior Distribution Visualization

**Laplace Prior (for Lasso):**
```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def plot_prior_distributions():
    """Plot Laplace and Gaussian priors"""
    w = np.linspace(-5, 5, 1000)
    
    # Laplace prior
    laplace_pdf = 0.5 * np.exp(-np.abs(w))
    
    # Gaussian prior
    gaussian_pdf = (1/np.sqrt(2*np.pi)) * np.exp(-0.5 * w**2)
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(w, laplace_pdf, 'b-', linewidth=3, label='Laplace Prior')
    plt.fill_between(w, laplace_pdf, alpha=0.3, color='blue')
    plt.title('Laplace Prior (for Lasso)')
    plt.xlabel('Weight Value')
    plt.ylabel('Probability Density')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(w, gaussian_pdf, 'r-', linewidth=3, label='Gaussian Prior')
    plt.fill_between(w, gaussian_pdf, alpha=0.3, color='red')
    plt.title('Gaussian Prior (for Ridge)')
    plt.xlabel('Weight Value')
    plt.ylabel('Probability Density')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    plot_prior_distributions()
```

### ğŸ’» å®é™…ä»£ç ç¤ºä¾‹

#### Lasso and Ridge Implementation
```python
import numpy as np
from sklearn.linear_model import Lasso, Ridge
from sklearn.preprocessing import StandardScaler
from scipy import stats

class BayesianRegularizedRegression:
    def __init__(self, alpha=1.0, regularization='lasso'):
        self.alpha = alpha
        self.regularization = regularization
        self.coef_ = None
        self.scaler = StandardScaler()
    
    def fit(self, X, y):
        """Fit regularized regression with Bayesian interpretation"""
        # Standardize features
        X_scaled = self.scaler.fit_transform(X)
        
        if self.regularization == 'lasso':
            self._fit_lasso(X_scaled, y)
        elif self.regularization == 'ridge':
            self._fit_ridge(X_scaled, y)
        else:
            raise ValueError("Regularization must be 'lasso' or 'ridge'")
    
    def _fit_lasso(self, X, y):
        """Lasso with Laplace prior interpretation"""
        # Using sklearn's Lasso implementation
        lasso = Lasso(alpha=self.alpha, max_iter=10000)
        lasso.fit(X, y)
        self.coef_ = lasso.coef_
        
        # Print prior information
        print(f"Lasso with Laplace Prior:")
        print(f"Prior variance: {2/(self.alpha**2):.4f}")
        print(f"Sparsity level: {np.sum(np.abs(self.coef_) < 1e-6)}/{len(self.coef_)} features")
    
    def _fit_ridge(self, X, y):
        """Ridge with Gaussian prior interpretation"""
        # Using sklearn's Ridge implementation
        ridge = Ridge(alpha=self.alpha)
        ridge.fit(X, y)
        self.coef_ = ridge.coef_
        
        # Print prior information
        print(f"Ridge with Gaussian Prior:")
        print(f"Prior variance: {1/self.alpha:.4f}")
        print(f"All {len(self.coef_)} features retained")
    
    def predict(self, X):
        """Make predictions"""
        X_scaled = self.scaler.transform(X)
        return X_scaled @ self.coef_
    
    def get_prior_samples(self, n_samples=1000):
        """Generate samples from the prior distribution"""
        if self.regularization == 'lasso':
            # Laplace distribution
            prior_samples = np.random.laplace(0, 1/self.alpha, 
                                            (n_samples, len(self.coef_)))
        elif self.regularization == 'ridge':
            # Gaussian distribution
            prior_samples = np.random.normal(0, np.sqrt(1/self.alpha), 
                                           (n_samples, len(self.coef_)))
        
        return prior_samples

# Example usage
def prior_comparison_example():
    """Compare Lasso and Ridge priors"""
    # Generate synthetic data
    np.random.seed(42)
    n, p = 100, 10
    X = np.random.randn(n, p)
    
    # Create sparse true weights
    true_weights = np.zeros(p)
    true_weights[:3] = [2, -1.5, 1]
    y = X @ true_weights + 0.1 * np.random.randn(n)
    
    # Fit models
    lasso_model = BayesianRegularizedRegression(alpha=0.1, regularization='lasso')
    ridge_model = BayesianRegularizedRegression(alpha=0.1, regularization='ridge')
    
    print("=== LASSO MODEL ===")
    lasso_model.fit(X, y)
    
    print("\n=== RIDGE MODEL ===")
    ridge_model.fit(X, y)
    
    # Compare coefficient distributions
    print("\n=== COEFFICIENT COMPARISON ===")
    print("True weights:", true_weights)
    print("Lasso coef:  ", lasso_model.coef_)
    print("Ridge coef:  ", ridge_model.coef_)

if __name__ == "__main__":
    prior_comparison_example()
```

### ğŸ” é¢è¯•å¸¸è§é—®é¢˜åŠå›ç­”

#### Q1: "What are the prior distributions for Lasso and Ridge?"

**English Answer:**
Lasso corresponds to a Laplace (double exponential) prior: P(w) âˆ exp(-Î»|w|), which has sharp peaks at zero encouraging sparsity. Ridge corresponds to a Gaussian prior: P(w) âˆ exp(-Î»wÂ²/2), which is smooth and centered at zero, encouraging small but non-zero weights.

#### Q2: "How does the Bayesian interpretation help understand regularization?"

**English Answer:**
Regularization is equivalent to maximum a posteriori (MAP) estimation where the regularization term acts as prior knowledge about parameter distribution. The prior encodes our beliefs about the model parameters before seeing the data, helping prevent overfitting by incorporating domain knowledge.

#### Q3: "Why does Lasso's Laplace prior encourage sparsity?"

**English Answer:**
The Laplace prior has a sharp peak at zero and heavy tails. This means it assigns high probability to weights near zero, encouraging the optimization algorithm to set many weights to exactly zero. The sharp peak at zero makes it easier to reach exact zero during optimization.

### ğŸ’¡ å®æˆ˜æŠ€å·§

#### 1. å…ˆéªŒé€‰æ‹©æ ‡å‡† (Prior Selection Criteria)
- **ä½¿ç”¨Laplaceå…ˆéªŒ**ï¼šç›¸ä¿¡å¤§éƒ¨åˆ†ç‰¹å¾æ— å…³
- **ä½¿ç”¨Gaussianå…ˆéªŒ**ï¼šç›¸ä¿¡æ‰€æœ‰ç‰¹å¾éƒ½æœ‰ç”¨

#### 2. å…³é”®è¯ (Key Terms)
- **Laplace Prior**: Laplaceå…ˆéªŒ
- **Gaussian Prior**: Gaussianå…ˆéªŒ
- **MAP Estimation**: æœ€å¤§åéªŒä¼°è®¡
- **Bayesian Interpretation**: è´å¶æ–¯è§£é‡Š
- **Prior Knowledge**: å…ˆéªŒçŸ¥è¯†

#### 3. å¸¸è§é™·é˜± (Common Pitfalls)
- âŒ ä¸ç†è§£å…ˆéªŒåˆ†å¸ƒçš„æ•°å­¦å½¢å¼
- âŒ æ··æ·†å…ˆéªŒå’Œä¼¼ç„¶çš„ä½œç”¨
- âŒ å¿½ç•¥å…ˆéªŒå‚æ•°Î»çš„é€‰æ‹©
- âŒ ä¸ç†è§£è´å¶æ–¯è§£é‡Š

### ğŸ“Š å¯è§†åŒ–ç†è§£

#### å…ˆéªŒåˆ†å¸ƒå¯¹æ¯”
![å…ˆéªŒåˆ†å¸ƒå¯¹æ¯”](../../images/basic_ml/prior_distributions.png)

#### Lasso/Ridge ç³»æ•°å¯¹æ¯”
![ç¨€ç–æ€§å¯è§†åŒ–](../../images/basic_ml/sparsity_visualization.png)

### ğŸ“Š é¢è¯•å‡†å¤‡æ£€æŸ¥æ¸…å•

- [ ] ç†è§£Lassoå’ŒRidgeçš„æ•°å­¦å®šä¹‰
- [ ] æŒæ¡Laplaceå’ŒGaussianå…ˆéªŒåˆ†å¸ƒ
- [ ] ç†è§£è´å¶æ–¯è§£é‡Š
- [ ] çŸ¥é“å…ˆéªŒåˆ†å¸ƒå¦‚ä½•å½±å“ç»“æœ
- [ ] ç†è§£MAPä¼°è®¡çš„æ¦‚å¿µ
- [ ] èƒ½å¤Ÿè§£é‡Šä¸ºä»€ä¹ˆLaplaceå…ˆéªŒé¼“åŠ±ç¨€ç–æ€§
- [ ] çŸ¥é“å¦‚ä½•é€‰æ‹©å…ˆéªŒå‚æ•°
- [ ] ç†è§£å…ˆéªŒçŸ¥è¯†çš„å®é™…æ„ä¹‰

### ğŸ¯ ç»ƒä¹ å»ºè®®

1. **ç†è®ºç»ƒä¹ **: ç†è§£è´å¶æ–¯ç»Ÿè®¡åŸºç¡€
2. **æ•°å­¦ç»ƒä¹ **: æ¨å¯¼MAPä¼°è®¡å…¬å¼
3. **å¯è§†åŒ–ç»ƒä¹ **: ç»˜åˆ¶ä¸åŒå…ˆéªŒåˆ†å¸ƒ
4. **åº”ç”¨ç»ƒä¹ **: åœ¨ä¸åŒæ•°æ®ä¸Šæ¯”è¾ƒæ•ˆæœ
5. **å‚æ•°ç»ƒä¹ **: è°ƒæ•´å…ˆéªŒå‚æ•°è§‚å¯Ÿå½±å“

**è®°ä½**: æ­£åˆ™åŒ–æœ¬è´¨ä¸Šæ˜¯è´å¶æ–¯æ–¹æ³•ï¼Œå…ˆéªŒåˆ†å¸ƒç¼–ç äº†æˆ‘ä»¬å¯¹å‚æ•°çš„å…ˆéªŒä¿¡å¿µï¼
