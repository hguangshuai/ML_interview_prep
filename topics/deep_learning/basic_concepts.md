# æ·±åº¦å­¦ä¹ åŸºç¡€é—®é¢˜ - è¯¦ç»†ç­”æ¡ˆ

## é—®é¢˜1: ä»€ä¹ˆæ˜¯æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ

### ğŸ¯ ä¸­æ–‡ç†è§£ (ä¾¿äºè®°å¿†)

#### æ¢¯åº¦æ¶ˆå¤± = "ä¿¡å·è¡°å‡"
æƒ³è±¡å£°éŸ³åœ¨ç®¡é“ä¸­ä¼ æ’­ï¼š
- **é—®é¢˜**ï¼šå£°éŸ³åœ¨é•¿ç®¡é“ä¸­è¶Šæ¥è¶Šå°ï¼Œæœ€åå¬ä¸è§
- **ç¥ç»ç½‘ç»œ**ï¼šæ¢¯åº¦ä¿¡å·åœ¨æ·±å±‚ç½‘ç»œä¸­è¶Šæ¥è¶Šå¼±ï¼Œæ·±å±‚å‚æ•°æ— æ³•æ›´æ–°
- **ç»“æœ**ï¼šæ·±å±‚ç½‘ç»œå­¦ä¸åˆ°ä¸œè¥¿ï¼Œåªæœ‰æµ…å±‚åœ¨å­¦

#### ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿï¼Ÿ
1. **æ¿€æ´»å‡½æ•°é—®é¢˜**ï¼šsigmoidã€tanhå¯¼æ•°å°äº1ï¼Œä¿¡å·è¶Šæ¥è¶Šå¼±
2. **æƒé‡åˆå§‹åŒ–**ï¼šæƒé‡å¤ªå°ï¼Œä¿¡å·ä¼ æ’­è¿‡ç¨‹ä¸­è¡°å‡
3. **ç½‘ç»œå¤ªæ·±**ï¼šå±‚æ•°è¶Šå¤šï¼Œä¿¡å·è¡°å‡è¶Šä¸¥é‡

### ğŸ¤ ç›´æ¥é¢è¯•å›ç­” (Direct Interview Answer)

**The vanishing gradient problem occurs when gradients become exponentially small during backpropagation in deep networks, preventing effective training of deep layers.**

**The mathematical cause is:** `âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚W_L Ã— âˆ(i=1 to L-1) Ïƒ'(z_i) Ã— W_i`. When activation function derivatives (like sigmoid's 0-0.25 range) and weights are consistently less than 1, their product decreases exponentially with depth.

**I solve this using ReLU activation functions** which have constant gradient of 1 for positive inputs, preventing signal decay. I also use proper weight initialization like Xavier/He initialization to maintain gradient variance.

**For very deep networks, I implement residual connections (ResNet)** that create direct paths for gradients to flow backward, bypassing the vanishing gradient issue entirely.

**Batch normalization helps by normalizing inputs to each layer**, reducing internal covariate shift and allowing higher learning rates while maintaining gradient flow.

**In practice, I combine these techniques** - ReLU + proper initialization + batch normalization + residual connections for training very deep networks effectively.

---

### ğŸ“ è‹±æ–‡æ ‡å‡†é¢è¯•ç­”æ¡ˆ (English Interview Answer)

#### 1. Definition and Causes

**Vanishing Gradient Problem** occurs when gradients become exponentially small during backpropagation in deep networks, making it difficult to train deep layers.

**Main Causes:**
- **Activation Functions**: Sigmoid/tanh derivatives < 1, causing signal decay
- **Weight Initialization**: Improper initialization leads to gradient decay
- **Network Depth**: More layers = more gradient attenuation

#### 2. Mathematical Explanation

**æ¢¯åº¦æ¶ˆå¤±çš„æ•°å­¦åŸç†ï¼š**

```
For a deep network with L layers:
âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚W_L Ã— âˆ(i=1 to L-1) Ïƒ'(z_i) Ã— W_i
```

**æ¯ä¸ªç¬¦å·çš„å«ä¹‰ï¼š**

- **âˆ‚L/âˆ‚Wâ‚**ï¼šç¬¬ä¸€å±‚æƒé‡çš„æ¢¯åº¦
  - ä¾‹å­ï¼šç½‘ç»œç¬¬ä¸€å±‚å‚æ•°çš„æ›´æ–°é‡
  - è¿™æ˜¯æˆ‘ä»¬æƒ³è¦è®¡ç®—çš„æ¢¯åº¦

- **âˆ‚L/âˆ‚W_L**ï¼šæœ€åä¸€å±‚æƒé‡çš„æ¢¯åº¦
  - ä¾‹å­ï¼šè¾“å‡ºå±‚å‚æ•°çš„æ¢¯åº¦
  - è¿™ä¸ªæ¢¯åº¦é€šå¸¸æ¯”è¾ƒå¤§

- **âˆ(i=1 to L-1)**ï¼šä»ç¬¬1å±‚åˆ°ç¬¬L-1å±‚çš„è¿ä¹˜
  - ä¾‹å­ï¼šå¦‚æœç½‘ç»œæœ‰5å±‚ï¼Œå°±æ˜¯ç¬¬1ã€2ã€3ã€4å±‚çš„è¿ä¹˜
  - è¡¨ç¤ºæ¢¯åº¦ä»æœ€åä¸€å±‚ä¼ æ’­åˆ°ç¬¬ä¸€å±‚

- **Ïƒ'(z_i)**ï¼šç¬¬iå±‚æ¿€æ´»å‡½æ•°çš„å¯¼æ•°
  - ä¾‹å­ï¼šsigmoidå‡½æ•°çš„å¯¼æ•°æœ€å¤§å€¼æ˜¯0.25
  - å¦‚æœæ¯å±‚å¯¼æ•°éƒ½å°äº1ï¼Œè¿ä¹˜åä¼šè¶Šæ¥è¶Šå°

- **W_i**ï¼šç¬¬iå±‚çš„æƒé‡
  - ä¾‹å­ï¼šå¦‚æœæƒé‡åˆå§‹åŒ–å¤ªå°ï¼Œä¹Ÿä¼šå¯¼è‡´æ¢¯åº¦è¡°å‡

**ä¸ºä»€ä¹ˆæ¢¯åº¦ä¼šæ¶ˆå¤±ï¼Ÿ**

å‡è®¾æ¯å±‚çš„Ïƒ'(z_i) Ã— W_i = 0.5ï¼ˆå°äº1ï¼‰ï¼š
- ç¬¬1å±‚æ¢¯åº¦ = æœ€åä¸€å±‚æ¢¯åº¦ Ã— 0.5
- ç¬¬2å±‚æ¢¯åº¦ = æœ€åä¸€å±‚æ¢¯åº¦ Ã— 0.5 Ã— 0.5 = 0.25
- ç¬¬3å±‚æ¢¯åº¦ = æœ€åä¸€å±‚æ¢¯åº¦ Ã— 0.5Â³ = 0.125
- ...
- ç¬¬10å±‚æ¢¯åº¦ = æœ€åä¸€å±‚æ¢¯åº¦ Ã— 0.5Â¹â° â‰ˆ 0.001

**å®é™…ä¾‹å­ï¼š**
```
ç½‘ç»œæœ‰10å±‚ï¼Œæ¯å±‚sigmoidæ¿€æ´»å‡½æ•°ï¼š
- sigmoidå¯¼æ•°æœ€å¤§å€¼ = 0.25
- å¦‚æœæƒé‡ä¹Ÿå°äº1ï¼Œæ¯”å¦‚0.8
- æ¯å±‚è¡°å‡å› å­ = 0.25 Ã— 0.8 = 0.2
- 10å±‚åæ¢¯åº¦ = åŸå§‹æ¢¯åº¦ Ã— 0.2Â¹â° â‰ˆ åŸå§‹æ¢¯åº¦ Ã— 0.0000001
- ç»“æœï¼šæ¢¯åº¦å‡ ä¹ä¸º0ï¼Œæ·±å±‚å‚æ•°æ— æ³•æ›´æ–°
```

**If each Ïƒ'(z_i) Ã— W_i < 1, then gradient vanishes exponentially**
- å¦‚æœæ¯å±‚çš„è¡°å‡å› å­éƒ½å°äº1
- æ¢¯åº¦ä¼šæŒ‡æ•°çº§è¡°å‡
- æ·±å±‚ç½‘ç»œæ— æ³•å­¦ä¹ 

#### 3. Solutions

**A. Better Activation Functions**
```python
# ReLU - derivative is 1 for positive inputs
def relu(x):
    return max(0, x)

# Leaky ReLU - small gradient for negative inputs
def leaky_relu(x, alpha=0.01):
    return max(alpha * x, x)

# Swish - smooth, non-monotonic
def swish(x):
    return x * sigmoid(x)
```

**B. Residual Connections (ResNet)**
```python
import tensorflow as tf

class ResidualBlock(tf.keras.layers.Layer):
    def __init__(self, filters):
        super().__init__()
        self.conv1 = tf.keras.layers.Conv2D(filters, 3, padding='same')
        self.bn1 = tf.keras.layers.BatchNormalization()
        self.conv2 = tf.keras.layers.Conv2D(filters, 3, padding='same')
        self.bn2 = tf.keras.layers.BatchNormalization()
        
    def call(self, x):
        residual = x
        x = tf.nn.relu(self.bn1(self.conv1(x)))
        x = self.bn2(self.conv2(x))
        x += residual  # Skip connection
        return tf.nn.relu(x)
```

**C. Batch Normalization**
```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dense(64),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation('relu')
])
```

**D. Proper Weight Initialization**
```python
# Xavier/Glorot initialization
tf.keras.layers.Dense(128, kernel_initializer='glorot_uniform')

# He initialization (for ReLU)
tf.keras.layers.Dense(128, kernel_initializer='he_normal')

# Custom initialization
def custom_init(shape, dtype=None):
    return tf.random.normal(shape, stddev=0.1)

tf.keras.layers.Dense(128, kernel_initializer=custom_init)
```

**E. Gradient Clipping**
```python
# In training loop
optimizer = tf.keras.optimizers.Adam()
with tf.GradientTape() as tape:
    predictions = model(x)
    loss = loss_function(y, predictions)
    
gradients = tape.gradient(loss, model.trainable_variables)
# Clip gradients
gradients = [tf.clip_by_norm(g, 1.0) for g in gradients]
optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```

---

## é—®é¢˜2: è§£é‡Šæ³¨æ„åŠ›æœºåˆ¶çš„åŸç†

### ğŸ¯ ä¸­æ–‡ç†è§£ (ä¾¿äºè®°å¿†)

#### æ³¨æ„åŠ›æœºåˆ¶ = "é€‰æ‹©æ€§å…³æ³¨"
æƒ³è±¡é˜…è¯»æ—¶ï¼š
- **ä¼ ç»Ÿæ–¹æ³•**ï¼šé€å­—é€å¥è¯»ï¼Œæ— æ³•è·³è·ƒ
- **æ³¨æ„åŠ›æœºåˆ¶**ï¼šå¯ä»¥å¿«é€Ÿæ‰«è§†ï¼Œé‡ç‚¹å…³æ³¨é‡è¦éƒ¨åˆ†
- **ç¥ç»ç½‘ç»œ**ï¼šåŠ¨æ€å†³å®šå…³æ³¨è¾“å…¥åºåˆ—çš„å“ªäº›éƒ¨åˆ†

#### ä¸‰ä¸ªå…³é”®ç»„ä»¶
1. **Query (Q)** - "æˆ‘è¦ä»€ä¹ˆ"ï¼šå½“å‰è¦å¤„ç†çš„ä¿¡æ¯
2. **Key (K)** - "æœ‰ä»€ä¹ˆ"ï¼šæ‰€æœ‰å¯ç”¨çš„ä¿¡æ¯
3. **Value (V)** - "å®é™…å†…å®¹"ï¼šæ¯ä¸ªä½ç½®çš„çœŸå®ä¿¡æ¯

### ğŸ“ è‹±æ–‡æ ‡å‡†é¢è¯•ç­”æ¡ˆ (English Interview Answer)

#### 1. Core Concept

**Attention Mechanism** allows models to dynamically focus on different parts of the input sequence, rather than processing all positions equally.

**Key Components:**
- **Query (Q)**: What we're looking for
- **Key (K)**: What's available at each position  
- **Value (V)**: The actual content at each position

#### 2. Mathematical Formulation

**æ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦å…¬å¼è¯¦ç»†è§£é‡Šï¼š**

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
```

**æ¯ä¸ªç¬¦å·çš„å«ä¹‰ï¼š**

- **Q (Query)**ï¼šæŸ¥è¯¢çŸ©é˜µ
  - ä¾‹å­ï¼šå½“å‰è¦å¤„ç†çš„ä½ç½®ï¼ˆæ¯”å¦‚ç¿»è¯‘ä¸­çš„ç›®æ ‡è¯ï¼‰
  - å½¢çŠ¶ï¼š[batch_size, seq_len, d_model]
  - ä½œç”¨ï¼šè¡¨ç¤º"æˆ‘æƒ³è¦ä»€ä¹ˆä¿¡æ¯"

- **K (Key)**ï¼šé”®çŸ©é˜µ
  - ä¾‹å­ï¼šè¾“å…¥åºåˆ—ä¸­æ‰€æœ‰ä½ç½®ï¼ˆæ¯”å¦‚æºè¯­è¨€çš„æ‰€æœ‰è¯ï¼‰
  - å½¢çŠ¶ï¼š[batch_size, seq_len, d_model]
  - ä½œç”¨ï¼šè¡¨ç¤º"æ¯ä¸ªä½ç½®æœ‰ä»€ä¹ˆä¿¡æ¯"

- **V (Value)**ï¼šå€¼çŸ©é˜µ
  - ä¾‹å­ï¼šæ¯ä¸ªä½ç½®çš„å®é™…å†…å®¹ï¼ˆæ¯”å¦‚æºè¯­è¨€è¯çš„å«ä¹‰ï¼‰
  - å½¢çŠ¶ï¼š[batch_size, seq_len, d_model]
  - ä½œç”¨ï¼šè¡¨ç¤º"æ¯ä¸ªä½ç½®çš„å…·ä½“ä¿¡æ¯"

- **QK^T**ï¼šæŸ¥è¯¢å’Œé”®çš„ç›¸ä¼¼åº¦çŸ©é˜µ
  - ä¾‹å­ï¼šè®¡ç®—å½“å‰è¯ä¸æ‰€æœ‰è¯çš„ç›¸ä¼¼åº¦
  - å½¢çŠ¶ï¼š[batch_size, seq_len, seq_len]
  - ä½œç”¨ï¼šå†³å®šæ³¨æ„åŠ›æƒé‡

- **âˆšd_k**ï¼šç¼©æ”¾å› å­
  - ä¾‹å­ï¼šå¦‚æœd_k=64ï¼Œé‚£ä¹ˆâˆšd_k=8
  - ä½œç”¨ï¼šé˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´softmaxé¥±å’Œ

- **softmax**ï¼šå½’ä¸€åŒ–å‡½æ•°
  - ä¾‹å­ï¼šå°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
  - ä½œç”¨ï¼šç¡®ä¿æ‰€æœ‰æ³¨æ„åŠ›æƒé‡å’Œä¸º1

- **V**ï¼šåŠ æƒæ±‚å’Œ
  - ä¾‹å­ï¼šæ ¹æ®æ³¨æ„åŠ›æƒé‡ç»„åˆæ‰€æœ‰ä½ç½®çš„ä¿¡æ¯
  - ä½œç”¨ï¼šå¾—åˆ°æœ€ç»ˆçš„ä¸Šä¸‹æ–‡è¡¨ç¤º

**è®¡ç®—æ­¥éª¤è¯¦è§£ï¼š**

1. **è®¡ç®—ç›¸ä¼¼åº¦**ï¼šQK^T
   ```
   å¯¹äºæ¯ä¸ªæŸ¥è¯¢ä½ç½®iå’Œé”®ä½ç½®jï¼š
   ç›¸ä¼¼åº¦[i,j] = Q[i] Â· K[j]  (ç‚¹ç§¯)
   ```

2. **ç¼©æ”¾**ï¼šé™¤ä»¥âˆšd_k
   ```
   ç¼©æ”¾ç›¸ä¼¼åº¦[i,j] = ç›¸ä¼¼åº¦[i,j] / âˆšd_k
   ```

3. **å½’ä¸€åŒ–**ï¼šsoftmax
   ```
   æ³¨æ„åŠ›æƒé‡[i,j] = exp(ç¼©æ”¾ç›¸ä¼¼åº¦[i,j]) / Î£_k exp(ç¼©æ”¾ç›¸ä¼¼åº¦[i,k])
   ```

4. **åŠ æƒæ±‚å’Œ**ï¼šä¹˜ä»¥V
   ```
   è¾“å‡º[i] = Î£_j æ³¨æ„åŠ›æƒé‡[i,j] Ã— V[j]
   ```

**å®é™…ä¾‹å­ï¼š**
å‡è®¾ç¿»è¯‘"Hello World"ï¼š
- Qï¼šå½“å‰è¦ç¿»è¯‘çš„è¯ï¼ˆæ¯”å¦‚"ä½ å¥½"ï¼‰
- Kï¼šæºè¯­è¨€çš„æ‰€æœ‰è¯ï¼ˆ"Hello", "World"ï¼‰
- Vï¼šæºè¯­è¨€è¯çš„å«ä¹‰
- æ³¨æ„åŠ›æƒé‡ï¼šå†³å®š"ä½ å¥½"åº”è¯¥å…³æ³¨"Hello"è¿˜æ˜¯"World"
- è¾“å‡ºï¼šæ ¹æ®æƒé‡ç»„åˆçš„ä¸Šä¸‹æ–‡ä¿¡æ¯

**Where:**
- QK^T: Similarity between query and keys
- âˆšd_k: Scaling factor (dimension of key vectors)
- softmax: Normalize attention weights
- V: Weighted combination of values

#### 3. Implementation

**A. Scaled Dot-Product Attention**
```python
import tensorflow as tf

def scaled_dot_product_attention(Q, K, V, mask=None):
    # Compute attention scores
    scores = tf.matmul(Q, K, transpose_b=True)
    d_k = tf.cast(tf.shape(K)[-1], tf.float32)
    scaled_scores = scores / tf.math.sqrt(d_k)
    
    # Apply mask if provided
    if mask is not None:
        scaled_scores += (mask * -1e9)
    
    # Apply softmax
    attention_weights = tf.nn.softmax(scaled_scores, axis=-1)
    
    # Weighted sum of values
    output = tf.matmul(attention_weights, V)
    
    return output, attention_weights
```

**B. Multi-Head Attention**
```python
class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.depth = d_model // num_heads
        
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)
        self.dense = tf.keras.layers.Dense(d_model)
        
    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])
    
    def call(self, v, k, q, mask=None):
        batch_size = tf.shape(q)[0]
        
        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)
        
        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)
        
        attention, attention_weights = scaled_dot_product_attention(
            q, k, v, mask)
        
        attention = tf.transpose(attention, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(attention, 
                                    (batch_size, -1, self.d_model))
        
        output = self.dense(concat_attention)
        return output, attention_weights
```

#### 4. Advantages

**English Answer:**
- **Parallelization**: Can process all positions simultaneously
- **Long-range Dependencies**: Direct connections between distant positions
- **Interpretability**: Attention weights show what the model focuses on
- **Flexibility**: Can be applied to various sequence tasks

---

## é—®é¢˜3: Transformerç›¸æ¯”RNNçš„ä¼˜åŠ¿

### ğŸ¯ ä¸­æ–‡ç†è§£ (ä¾¿äºè®°å¿†)

#### Transformer vs RNN = "å¹¶è¡Œ vs ä¸²è¡Œ"
æƒ³è±¡å¤„ç†æ–‡æ¡£ï¼š
- **RNNæ–¹å¼**ï¼šå¿…é¡»ä¸€ä¸ªå­—ä¸€ä¸ªå­—è¯»ï¼Œä¸èƒ½è·³è·ƒï¼Œä¸èƒ½å¹¶è¡Œ
- **Transformeræ–¹å¼**ï¼šå¯ä»¥åŒæ—¶çœ‹æ‰€æœ‰å­—ï¼Œå¹¶è¡Œå¤„ç†ï¼Œå¿«é€Ÿç†è§£

#### æ ¸å¿ƒä¼˜åŠ¿
1. **å¹¶è¡ŒåŒ–**ï¼šæ‰€æœ‰ä½ç½®åŒæ—¶å¤„ç†ï¼Œè®­ç»ƒé€Ÿåº¦å¿«
2. **é•¿è·ç¦»ä¾èµ–**ï¼šæ³¨æ„åŠ›æœºåˆ¶ç›´æ¥è¿æ¥ä»»æ„ä½ç½®
3. **æ¢¯åº¦ç¨³å®š**ï¼šæ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
4. **å¯è§£é‡Šæ€§**ï¼šæ³¨æ„åŠ›æƒé‡å¯è§†åŒ–ï¼ŒçŸ¥é“æ¨¡å‹å…³æ³¨ä»€ä¹ˆ

### ğŸ“ è‹±æ–‡æ ‡å‡†é¢è¯•ç­”æ¡ˆ (English Interview Answer)

#### 1. Key Advantages

**A. Parallelization**
```python
# RNN: Sequential processing (slow)
def rnn_forward(x):
    h = initial_state
    for t in range(seq_len):
        h = rnn_cell(x[t], h)  # Must wait for previous step
    return h

# Transformer: Parallel processing (fast)
def transformer_forward(x):
    # All positions processed simultaneously
    attention_output = multi_head_attention(x, x, x)
    return attention_output
```

**B. Long-range Dependencies**
- **RNN**: O(n) complexity, gradient decay over long sequences
- **Transformer**: O(1) complexity, direct attention to any position

**C. Training Efficiency**
```python
# RNN: Sequential, hard to parallelize
class RNNModel:
    def forward(self, x):
        # Must process step by step
        for t in range(seq_len):
            output[t] = self.cell(x[t], hidden[t-1])

# Transformer: Fully parallelizable
class TransformerModel:
    def forward(self, x):
        # All positions processed at once
        attention = self.attention(x, x, x)
        return self.feed_forward(attention)
```

#### 2. Mathematical Comparison

**RNN Complexity:**
- Time: O(n) - must process sequentially
- Space: O(n) - store hidden states
- Gradient: Exponential decay/explosion

**Transformer Complexity:**
- Time: O(nÂ²) - attention over all pairs
- Space: O(nÂ²) - attention matrix
- Gradient: Stable, no vanishing/exploding

#### 3. When to Use Each

**Use RNN when:**
- Sequential processing is natural
- Memory efficiency is critical
- Short sequences (< 100 tokens)

**Use Transformer when:**
- Long sequences (> 100 tokens)
- Parallel training is important
- Need to capture long-range dependencies
- Have sufficient computational resources

### ğŸ” é¢è¯•å¸¸è§é—®é¢˜åŠå›ç­”

#### Q1: "Why is Transformer better than RNN for long sequences?"

**English Answer:**
- **Gradient Flow**: RNNs suffer from vanishing gradients over long sequences
- **Parallelization**: Transformers can process all positions simultaneously
- **Direct Connections**: Attention mechanism directly connects distant positions
- **Training Speed**: Parallel processing makes training much faster

#### Q2: "What are the computational trade-offs?"

**English Answer:**
- **RNN**: O(n) time, O(n) space, but sequential processing
- **Transformer**: O(nÂ²) time, O(nÂ²) space, but parallel processing
- **Trade-off**: Transformer uses more computation but trains much faster

#### Q3: "How does attention solve the long-range dependency problem?"

**English Answer:**
```python
# RNN: Information must flow through all intermediate steps
# Step 1 -> Step 2 -> ... -> Step N (gradient decay)

# Transformer: Direct attention from any position to any position
# Step 1 <-> Step N (direct connection via attention)
```

### ğŸ’¡ å®æˆ˜æŠ€å·§

#### 1. å›ç­”ç»“æ„ (Answer Structure)
1. **å®šä¹‰** (Definition): è§£é‡Šæ¢¯åº¦æ¶ˆå¤±å’Œæ³¨æ„åŠ›æœºåˆ¶
2. **åŸç†** (Principles): æ•°å­¦å…¬å¼å’Œå®ç°ç»†èŠ‚
3. **å¯¹æ¯”** (Comparison): Transformer vs RNNçš„ä¼˜åŠ¿
4. **ä»£ç ** (Code): å®é™…å®ç°ç¤ºä¾‹
5. **åº”ç”¨** (Applications): ä½•æ—¶ä½¿ç”¨å“ªç§æ–¹æ³•

#### 2. å…³é”®è¯ (Key Terms)
- **Vanishing Gradient**: æ¢¯åº¦æ¶ˆå¤±
- **Attention Mechanism**: æ³¨æ„åŠ›æœºåˆ¶
- **Parallelization**: å¹¶è¡ŒåŒ–
- **Long-range Dependencies**: é•¿è·ç¦»ä¾èµ–
- **Scaled Dot-Product**: ç¼©æ”¾ç‚¹ç§¯

#### 3. å¸¸è§é™·é˜± (Common Pitfalls)
- âŒ åªè°ˆç†è®ºï¼Œæ²¡æœ‰ä»£ç å®ç°
- âŒ æ··æ·†æ³¨æ„åŠ›çš„ä¸åŒå˜ä½“
- âŒ å¿½ç•¥è®¡ç®—å¤æ‚åº¦çš„æƒè¡¡
- âŒ æ²¡æœ‰æåˆ°å®é™…åº”ç”¨åœºæ™¯

### ğŸ“Š é¢è¯•å‡†å¤‡æ£€æŸ¥æ¸…å•

- [ ] èƒ½æ¸…æ™°è§£é‡Šæ¢¯åº¦æ¶ˆå¤±çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆ
- [ ] ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦åŸç†
- [ ] æŒæ¡Transformerçš„æ¶æ„ç»†èŠ‚
- [ ] çŸ¥é“RNNå’ŒTransformerçš„ä¼˜ç¼ºç‚¹å¯¹æ¯”
- [ ] èƒ½æä¾›å®é™…ä»£ç å®ç°
- [ ] ç†è§£å¹¶è¡ŒåŒ–çš„é‡è¦æ€§
- [ ] æŒæ¡é•¿è·ç¦»ä¾èµ–å»ºæ¨¡æ–¹æ³•
- [ ] äº†è§£ä¸åŒåœºæ™¯ä¸‹çš„é€‰æ‹©ç­–ç•¥

### ğŸ¯ ç»ƒä¹ å»ºè®®

1. **ç†è®ºç»ƒä¹ **: ç”¨è‡ªå·±çš„è¯è§£é‡Šæ¢¯åº¦æ¶ˆå¤±å’Œæ³¨æ„åŠ›æœºåˆ¶
2. **ä»£ç ç»ƒä¹ **: å®ç°ç®€å•çš„æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformerç»„ä»¶
3. **å¯¹æ¯”åˆ†æ**: åˆ†æRNNå’ŒTransformeråœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°
4. **å¯è§†åŒ–**: ç»˜åˆ¶æ³¨æ„åŠ›æƒé‡å›¾ç†è§£æ¨¡å‹è¡Œä¸º
5. **æ¨¡æ‹Ÿé¢è¯•**: ç»ƒä¹ å®Œæ•´çš„å›ç­”æµç¨‹

**è®°ä½**: é¢è¯•å®˜æœŸæœ›ä½ ä¸ä»…ç†è§£æ¦‚å¿µï¼Œè¿˜è¦èƒ½å®ç°å’Œå¯¹æ¯”ä¸åŒæ–¹æ³•ï¼

## é—®é¢˜4: ä»€ä¹ˆæ˜¯Dropoutï¼Ÿä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ

Dropoutæ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œåœ¨è®­ç»ƒæ—¶éšæœºå°†éƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºè®¾ä¸º0ã€‚

### å·¥ä½œåŸç†ï¼š
- è®­ç»ƒæ—¶ï¼šéšæœºä¸¢å¼ƒp%çš„ç¥ç»å…ƒ
- æµ‹è¯•æ—¶ï¼šä½¿ç”¨æ‰€æœ‰ç¥ç»å…ƒï¼Œä½†è¾“å‡ºä¹˜ä»¥(1-p)

### æœ‰æ•ˆæ€§åŸå› ï¼š
1. **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šå‡å°‘ç¥ç»å…ƒé—´çš„å…±é€‚åº”
2. **æ¨¡å‹é›†æˆ**ï¼šç›¸å½“äºè®­ç»ƒå¤šä¸ªå­ç½‘ç»œ
3. **æé«˜æ³›åŒ–èƒ½åŠ›**ï¼šå¢å¼ºæ¨¡å‹é²æ£’æ€§

### å®ç°ï¼š
```python
# è®­ç»ƒæ—¶
output = dropout(input, p=0.5)

# æµ‹è¯•æ—¶
output = input * (1 - 0.5)
```

## é—®é¢˜5: æ‰¹å½’ä¸€åŒ–(Batch Normalization)çš„ä½œç”¨

æ‰¹å½’ä¸€åŒ–å¯¹æ¯å±‚çš„è¾“å…¥è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚

### å…¬å¼ï¼š
- æ ‡å‡†åŒ–ï¼š`xÌ‚ = (x - Î¼) / Ïƒ`
- ç¼©æ”¾åç§»ï¼š`y = Î³xÌ‚ + Î²`

### ä½œç”¨ï¼š
1. **åŠ é€Ÿè®­ç»ƒ**ï¼šå‡å°‘å†…éƒ¨åå˜é‡åç§»
2. **ç¨³å®šæ¢¯åº¦**ï¼šé¿å…æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
3. **æ­£åˆ™åŒ–æ•ˆæœ**ï¼šå‡å°‘å¯¹Dropoutçš„ä¾èµ–
4. **å…è®¸æ›´é«˜å­¦ä¹ ç‡**ï¼šè®­ç»ƒæ›´ç¨³å®š

### ä½ç½®ï¼š
- **å·ç§¯å±‚å**ï¼š`Conv â†’ BN â†’ ReLU`
- **å…¨è¿æ¥å±‚å‰**ï¼š`FC â†’ BN â†’ ReLU`
