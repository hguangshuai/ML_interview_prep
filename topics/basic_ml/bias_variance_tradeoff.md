# æœºå™¨å­¦ä¹ åŸºç¡€é—®é¢˜ - è¯¦ç»†ç­”æ¡ˆ

## é—®é¢˜: ä»€ä¹ˆæ˜¯åå·®-æ–¹å·®æƒè¡¡(Bias-Variance Tradeoff)ï¼Ÿ

### ä¸­æ–‡ç†è§£ (ä¾¿äºŽè®°å¿†)

#### åå·®-æ–¹å·®æƒè¡¡ = "å‡†ç¡®åº¦ vs ç¨³å®šæ€§"
æƒ³è±¡å°„ç®­æ¯”èµ›ï¼š
- **åå·®**ï¼šç³»ç»Ÿæ€§è¯¯å·®ï¼Œåƒçž„å‡†å™¨åäº†ï¼Œæ€»æ˜¯å°„ååŒä¸€ä¸ªæ–¹å‘
- **æ–¹å·®**ï¼šéšæœºæ€§è¯¯å·®ï¼Œåƒæ‰‹æŠ–ï¼Œæ¯æ¬¡å°„ç®­éƒ½åç¦»ç›®æ ‡ä¸åŒæ–¹å‘
- **æƒè¡¡**ï¼šéœ€è¦åœ¨å‡†ç¡®æ€§å’Œç¨³å®šæ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡

#### åå·® (Bias) - "ç³»ç»Ÿæ€§è¯¯å·®"
- **çŽ°è±¡**ï¼šæ¨¡åž‹è¿‡äºŽç®€å•ï¼Œæ— æ³•æ•æ‰æ•°æ®ä¸­çš„å¤æ‚æ¨¡å¼
- **ç»“æžœ**ï¼šè®­ç»ƒè¯¯å·®å’Œæµ‹è¯•è¯¯å·®éƒ½è¾ƒé«˜
- **æ¯”å–»**ï¼šåƒç”¨ç›´çº¿æ‹Ÿåˆæ›²çº¿æ•°æ®ï¼Œæ€»æ˜¯æœ‰ç³»ç»Ÿæ€§åå·®

#### æ–¹å·® (Variance) - "éšæœºæ€§è¯¯å·®"
- **çŽ°è±¡**ï¼šæ¨¡åž‹è¿‡äºŽå¤æ‚ï¼Œå¯¹è®­ç»ƒæ•°æ®ä¸­å¾®å°å˜åŒ–è¿‡äºŽæ•æ„Ÿ
- **ç»“æžœ**ï¼šè®­ç»ƒè¯¯å·®ä½Žï¼Œæµ‹è¯•è¯¯å·®é«˜
- **æ¯”å–»**ï¼šåƒè¿‡åº¦æ‹Ÿåˆï¼Œè®°ä½äº†æ‰€æœ‰ç»†èŠ‚åŒ…æ‹¬å™ªå£°

### è‹±æ–‡æ ‡å‡†é¢è¯•ç­”æ¡ˆ

#### 1. åŸºç¡€å®šä¹‰ (Basic Definitions)

**Bias** is the systematic error from oversimplified assumptions in the learning algorithm. High bias can cause the model to miss relevant relations between features and target outputs (underfitting).

**Variance** is the error from sensitivity to small fluctuations in the training set. High variance can cause overfitting, where the model learns the noise in the training data rather than the underlying pattern.

#### 2. æ•°å­¦è¡¨ç¤º (Mathematical Representation)

**è¯¯å·®åˆ†è§£å…¬å¼è¯¦ç»†è§£é‡Šï¼š**

```
Total Error = BiasÂ² + Variance + Irreducible Error
```

**æ¯ä¸ªç¬¦å·çš„å«ä¹‰ï¼š**

- **Total Error**ï¼šæ€»è¯¯å·®
  - ä¾‹å­ï¼šæ¨¡åž‹åœ¨æµ‹è¯•é›†ä¸Šçš„å¹³å‡å¹³æ–¹è¯¯å·®
  - è¿™æ˜¯æˆ‘ä»¬æƒ³è¦æœ€å°åŒ–çš„ç›®æ ‡

- **BiasÂ²**ï¼šåå·®çš„å¹³æ–¹ï¼ˆç³»ç»Ÿæ€§è¯¯å·®ï¼‰
  - ä¾‹å­ï¼šæ¨¡åž‹é¢„æµ‹çš„å¹³å‡å€¼ä¸ŽçœŸå®žå€¼çš„å·®è·
  - å¦‚æžœæ¨¡åž‹æ€»æ˜¯é¢„æµ‹è¿‡é«˜æˆ–è¿‡ä½Žï¼Œå°±æœ‰é«˜åå·®
  - æ•°å­¦å®šä¹‰ï¼šBias = E[fÌ‚(x)] - f(x)
    - E[fÌ‚(x)]ï¼šæ¨¡åž‹é¢„æµ‹çš„æœŸæœ›å€¼
    - f(x)ï¼šçœŸå®žå€¼

- **Variance**ï¼šæ–¹å·®ï¼ˆéšæœºæ€§è¯¯å·®ï¼‰
  - ä¾‹å­ï¼šæ¨¡åž‹é¢„æµ‹çš„æ³¢åŠ¨ç¨‹åº¦
  - å¦‚æžœæ¨¡åž‹å¯¹è®­ç»ƒæ•°æ®çš„å°å˜åŒ–å¾ˆæ•æ„Ÿï¼Œå°±æœ‰é«˜æ–¹å·®
  - æ•°å­¦å®šä¹‰ï¼šVariance = E[(fÌ‚(x) - E[fÌ‚(x)])Â²]
    - fÌ‚(x) - E[fÌ‚(x)]ï¼šå•æ¬¡é¢„æµ‹ä¸Žå¹³å‡é¢„æµ‹çš„åå·®

- **Irreducible Error**ï¼šä¸å¯å‡å°‘çš„è¯¯å·®ï¼ˆå™ªå£°ï¼‰
  - ä¾‹å­ï¼šæ•°æ®ä¸­çš„éšæœºå™ªå£°
  - è¿™æ˜¯æ•°æ®æœ¬èº«çš„é—®é¢˜ï¼Œæ— æ³•é€šè¿‡æ”¹è¿›æ¨¡åž‹æ¥å‡å°‘
  - æ¯”å¦‚ï¼šæµ‹é‡è¯¯å·®ã€æ ‡ç­¾é”™è¯¯ç­‰

**å®žé™…ä¾‹å­ï¼š**
å‡è®¾æˆ‘ä»¬è¦é¢„æµ‹æˆ¿ä»·ï¼š
- **é«˜åå·®**ï¼šæ¨¡åž‹æ€»æ˜¯é¢„æµ‹æˆ¿ä»·æ¯”å®žé™…ä½Ž10ä¸‡ï¼ˆç³»ç»Ÿæ€§ä½Žä¼°ï¼‰
- **é«˜æ–¹å·®**ï¼šæ¨¡åž‹é¢„æµ‹å¾ˆä¸ç¨³å®šï¼Œæœ‰æ—¶é«˜æœ‰æ—¶ä½Žï¼ˆå¯¹è®­ç»ƒæ•°æ®æ•æ„Ÿï¼‰
- **å™ªå£°**ï¼šæˆ¿ä»·æ•°æ®ä¸­æœ‰æµ‹é‡é”™è¯¯æˆ–å¼‚å¸¸å€¼

```
æ€»è¯¯å·® = (ç³»ç»Ÿæ€§ä½Žä¼°)Â² + (é¢„æµ‹ä¸ç¨³å®š) + (æ•°æ®å™ªå£°)
```

**ä¸ºä»€ä¹ˆæ˜¯BiasÂ²è€Œä¸æ˜¯Biasï¼Ÿ**
- å› ä¸ºåå·®å¯èƒ½æ˜¯æ­£æ•°æˆ–è´Ÿæ•°ï¼ˆé«˜ä¼°æˆ–ä½Žä¼°ï¼‰
- å¹³æ–¹åŽå˜æˆæ­£æ•°ï¼Œä¾¿äºŽä¸Žå…¶ä»–è¯¯å·®é¡¹ç›¸åŠ 
- æ•°å­¦ä¸Šæ›´åˆç†ï¼Œç¬¦åˆæœ€å°äºŒä¹˜æ³•çš„åŽŸç†

#### 3. å››ç§çŠ¶æ€ (Four States)

| çŠ¶æ€ | åå·® | æ–¹å·® | çŽ°è±¡ | è§£å†³æ–¹æ¡ˆ |
|------|------|------|------|----------|
| **ç†æƒ³** | ä½Ž | ä½Ž | æ—¢å‡†ç¡®åˆç¨³å®š | ä¿æŒå½“å‰å¤æ‚åº¦ |
| **è¿‡æ‹Ÿåˆ** | ä½Ž | é«˜ | è®­ç»ƒå¥½ï¼Œæµ‹è¯•å·® | å‡å°‘å¤æ‚åº¦ï¼Œæ­£åˆ™åŒ– |
| **æ¬ æ‹Ÿåˆ** | é«˜ | ä½Ž | è®­ç»ƒå’Œæµ‹è¯•éƒ½å·® | å¢žåŠ å¤æ‚åº¦ï¼Œç‰¹å¾å·¥ç¨‹ |
| **æœ€å·®** | é«˜ | é«˜ | æ—¢ä¸å‡†åˆä¸ç¨³å®š | é‡æ–°è®¾è®¡æ¨¡åž‹ |

#### 4. å®žé™…æ£€æµ‹æ–¹æ³• (Detection Methods)

**High Bias Indicators:**
```python
# æ£€æµ‹é«˜åå·®
if train_error > threshold and test_error â‰ˆ train_error:
    print("High bias detected - model too simple")
```

**High Variance Indicators:**
```python
# æ£€æµ‹é«˜æ–¹å·®
if train_error << test_error:
    print("High variance detected - model too complex")
```

#### 5. è§£å†³æ–¹æ¡ˆ (Solutions)

**For High Bias (Underfitting):**
1. **Increase Model Complexity**: Add more layers/features
2. **Feature Engineering**: Create more informative features
3. **Reduce Regularization**: Lower penalty strength
4. **Longer Training**: Train for more epochs
5. **Ensemble Methods**: Combine multiple models

**For High Variance (Overfitting):**
1. **Regularization**: Add L1/L2 penalty terms
2. **Cross-validation**: Better estimate of model performance
3. **Early Stopping**: Stop training when validation loss increases
4. **Dropout**: Randomly disable neurons during training
5. **Data Augmentation**: Increase effective training data size

### é¢è¯•å¸¸è§é—®é¢˜åŠå›žç­”

#### Q1: "Explain the mathematical derivation of bias-variance decomposition"

**Answer:**
```python
# åå·®-æ–¹å·®åˆ†è§£æŽ¨å¯¼
import numpy as np

def bias_variance_decomposition():
    """
    æ€»è¯¯å·® = E[(y - fÌ‚(x))Â²] = BiasÂ² + Variance + Noise
    
    Where:
    - BiasÂ² = [E[fÌ‚(x)] - f(x)]Â²  # ç³»ç»Ÿæ€§è¯¯å·®
    - Variance = E[(fÌ‚(x) - E[fÌ‚(x)])Â²]  # éšæœºæ€§è¯¯å·®
    - Noise = E[(y - f(x))Â²]  # æ•°æ®å™ªå£°
    """
    
    # æ¨¡æ‹Ÿå¤šä¸ªæ¨¡åž‹é¢„æµ‹
    predictions = []  # ä¸åŒè®­ç»ƒé›†çš„æ¨¡åž‹é¢„æµ‹
    
    for i in range(100):  # 100ä¸ªä¸åŒçš„è®­ç»ƒé›†
        model = train_model(train_data_i)
        pred = model.predict(test_x)
        predictions.append(pred)
    
    # è®¡ç®—åå·®å’Œæ–¹å·®
    mean_pred = np.mean(predictions)
    bias_squared = (mean_pred - true_y) ** 2
    variance = np.var(predictions)
    
    return bias_squared, variance
```

#### Q2: "How do you choose the right model complexity?"

**Answer:**
```python
# æ¨¡åž‹å¤æ‚åº¦é€‰æ‹©
from sklearn.model_selection import validation_curve
import matplotlib.pyplot as plt

def find_optimal_complexity():
    # ä½¿ç”¨éªŒè¯æ›²çº¿
    train_scores, val_scores = validation_curve(
        model, X, y, param_name='complexity', 
        param_range=complexity_range, cv=5
    )
    
    # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    plt.plot(complexity_range, train_scores.mean(axis=1), 'o-', label='Training')
    plt.plot(complexity_range, val_scores.mean(axis=1), 'o-', label='Validation')
    
    # æœ€ä¼˜å¤æ‚åº¦ï¼šéªŒè¯åˆ†æ•°æœ€é«˜çš„ç‚¹
    optimal_complexity = complexity_range[np.argmax(val_scores.mean(axis=1))]
    
    return optimal_complexity
```

#### Q3: "What's the difference between L1 and L2 regularization in this context?"

**Answer:**
- **L1 (Lasso)**: 
  - Penalty: `Î» * Î£|w|`
  - Effect on bias: Increases bias (feature selection)
  - Effect on variance: Decreases variance (sparse solutions)
  - Use case: When you want to remove irrelevant features

- **L2 (Ridge)**:
  - Penalty: `Î» * Î£wÂ²`  
  - Effect on bias: Slightly increases bias
  - Effect on variance: Significantly decreases variance
  - Use case: When you want to keep all features but reduce overfitting

#### Q4: "How does cross-validation help with bias-variance tradeoff?"

**Answer:**
```python
# äº¤å‰éªŒè¯å¸®åŠ©ä¼°è®¡çœŸå®žæ€§èƒ½
from sklearn.model_selection import cross_val_score

def evaluate_model_bias_variance(model, X, y):
    # äº¤å‰éªŒè¯åˆ†æ•°
    cv_scores = cross_val_score(model, X, y, cv=5)
    
    # è®­ç»ƒåˆ†æ•°
    model.fit(X, y)
    train_score = model.score(X, y)
    
    # åˆ†æžåå·®å’Œæ–¹å·®
    bias_estimate = train_score - cv_scores.mean()
    variance_estimate = cv_scores.std()
    
    print(f"Bias estimate: {bias_estimate:.3f}")
    print(f"Variance estimate: {variance_estimate:.3f}")
    
    return bias_estimate, variance_estimate
```

### å®žæˆ˜æŠ€å·§

#### 1. å›žç­”ç»“æž„ (Answer Structure)
1. **å®šä¹‰** (Definition): æ¸…æ™°å®šä¹‰åå·®å’Œæ–¹å·®
2. **æ•°å­¦** (Mathematics): ç»™å‡ºè¯¯å·®åˆ†è§£å…¬å¼
3. **å››ç§çŠ¶æ€** (Four States): è§£é‡Šä¸åŒç»„åˆçš„å«ä¹‰
4. **æ£€æµ‹** (Detection): å¦‚ä½•è¯†åˆ«é—®é¢˜
5. **è§£å†³** (Solutions): å…·ä½“çš„è§£å†³æ–¹æ³•
6. **ä¾‹å­** (Examples): æä¾›å®žé™…ä»£ç æ¡ˆä¾‹

#### 2. å…³é”®è¯ (Key Terms)
- **Bias**: ç³»ç»Ÿæ€§è¯¯å·®ï¼Œæ¬ æ‹Ÿåˆ
- **Variance**: éšæœºæ€§è¯¯å·®ï¼Œè¿‡æ‹Ÿåˆ
- **Tradeoff**: æƒè¡¡å…³ç³»
- **Regularization**: æ­£åˆ™åŒ–
- **Cross-validation**: äº¤å‰éªŒè¯
- **Model complexity**: æ¨¡åž‹å¤æ‚åº¦

#### 3. å¸¸è§é™·é˜± (Common Pitfalls)
- âŒ æ··æ·†åå·®å’Œæ–¹å·®çš„å®šä¹‰
- âŒ å¿½ç•¥æ•°å­¦å…¬å¼çš„é‡è¦æ€§
- âŒ æ²¡æœ‰æåˆ°å››ç§çŠ¶æ€çš„å®Œæ•´åˆ†æž
- âŒ è§£å†³æ–¹æ¡ˆè¿‡äºŽç®€å•ï¼Œæ²¡æœ‰å…·ä½“æ–¹æ³•

### ðŸ“Š å¯è§†åŒ–ç†è§£

#### åå·®-æ–¹å·®æƒè¡¡å›¾
![åå·®-æ–¹å·®æƒè¡¡å›¾](../../images/basic_ml/bias_variance_tradeoff.png)

#### è¯¦ç»†åˆ†æžå›¾
![è¯¦ç»†åˆ†æžå›¾](../../images/basic_ml/bias_variance_detailed_analysis.png)

#### æ¨¡åž‹å¤æ‚åº¦åˆ†æž
![æ¨¡åž‹å¤æ‚åº¦åˆ†æž](../../images/basic_ml/model_complexity_analysis.png)

#### æ­£åˆ™åŒ–æ•ˆæžœ
![æ­£åˆ™åŒ–æ•ˆæžœ](../../images/basic_ml/regularization_effect.png)

#### æ¨¡åž‹å¤æ‚åº¦ vs è¯¯å·®
```
ä½Žå¤æ‚åº¦ â†’ é«˜åå·®ï¼Œä½Žæ–¹å·® (æ¬ æ‹Ÿåˆ)
é«˜å¤æ‚åº¦ â†’ ä½Žåå·®ï¼Œé«˜æ–¹å·® (è¿‡æ‹Ÿåˆ)
æœ€ä¼˜å¤æ‚åº¦ â†’ å¹³è¡¡åå·®å’Œæ–¹å·®
```

#### å­¦ä¹ æ›²çº¿æ¨¡å¼
```
é«˜åå·®: è®­ç»ƒè¯¯å·® â‰ˆ éªŒè¯è¯¯å·® (éƒ½é«˜)
é«˜æ–¹å·®: è®­ç»ƒè¯¯å·® << éªŒè¯è¯¯å·®
ç†æƒ³: è®­ç»ƒè¯¯å·® â‰ˆ éªŒè¯è¯¯å·® (éƒ½ä½Ž)
```

### é¢è¯•å‡†å¤‡æ£€æŸ¥æ¸…å•

- [ ] èƒ½æ¸…æ™°å®šä¹‰åå·®å’Œæ–¹å·®
- [ ] ç†è§£æ•°å­¦åˆ†è§£å…¬å¼
- [ ] çŸ¥é“å››ç§çŠ¶æ€çš„å®Œæ•´åˆ†æž
- [ ] æŽŒæ¡æ£€æµ‹æ–¹æ³•
- [ ] äº†è§£å„ç§è§£å†³æ–¹æ¡ˆ
- [ ] èƒ½æä¾›å®žé™…ä»£ç ä¾‹å­
- [ ] ç†è§£æ­£åˆ™åŒ–çš„ä½œç”¨æœºåˆ¶
- [ ] çŸ¥é“äº¤å‰éªŒè¯çš„é‡è¦æ€§
- [ ] èƒ½è§£é‡Šæ¨¡åž‹å¤æ‚åº¦é€‰æ‹©

### ç»ƒä¹ å»ºè®®

1. **ç†è®ºç»ƒä¹ **: ç”¨è‡ªå·±çš„è¯è§£é‡Šæ¦‚å¿µå’Œæ•°å­¦å…¬å¼
2. **ä»£ç ç»ƒä¹ **: å®žçŽ°åå·®-æ–¹å·®åˆ†è§£
3. **å¯è§†åŒ–ç»ƒä¹ **: ç»˜åˆ¶å­¦ä¹ æ›²çº¿å’Œå¤æ‚åº¦åˆ†æžå›¾
4. **æ¡ˆä¾‹åˆ†æž**: åˆ†æžä¸åŒæ¨¡åž‹çš„åå·®-æ–¹å·®ç‰¹æ€§
5. **æ¨¡æ‹Ÿé¢è¯•**: ç»ƒä¹ å®Œæ•´çš„å›žç­”æµç¨‹

è®°ä½ï¼šé¢è¯•å®˜æœŸæœ›ä½ ä¸ä»…ç†è§£æ¦‚å¿µï¼Œè¿˜è¦èƒ½æ•°å­¦æŽ¨å¯¼å’Œå®žé™…åº”ç”¨ï¼
