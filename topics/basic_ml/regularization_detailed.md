# æœºå™¨å­¦ä¹ åŸºç¡€é—®é¢˜ - è¯¦ç»†ç­”æ¡ˆ

## é—®é¢˜: æ­£åˆ™åŒ–(Regularization)è¯¦è§£ - L1 vs L2, Lasso vs Ridge

### ğŸ¯ ä¸­æ–‡ç†è§£ (ä¾¿äºè®°å¿†)

#### æ­£åˆ™åŒ– = "é˜²æ­¢è¿‡åº¦å­¦ä¹ "
æƒ³è±¡å­¦ç”Ÿå‡†å¤‡è€ƒè¯•ï¼š
- **é—®é¢˜**ï¼šå­¦ç”ŸèƒŒäº†å¤ªå¤šç»†èŠ‚ï¼ŒåŒ…æ‹¬é”™è¯¯ç­”æ¡ˆï¼Œè€ƒè¯•æ—¶é‡åˆ°æ–°é¢˜ç›®å°±ä¸ä¼šäº†
- **æ­£åˆ™åŒ–**ï¼šç»™å­¦ä¹ è¿‡ç¨‹åŠ çº¦æŸï¼Œé˜²æ­¢å­¦å¾—å¤ªå¤æ‚
- **ç›®æ ‡**ï¼šæ‰¾åˆ°ç®€å•ä½†æœ‰æ•ˆçš„è§„å¾‹ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›

#### L1 vs L2 æ­£åˆ™åŒ– = "ä¸åŒçš„æƒ©ç½šæ–¹å¼"
- **L1æ­£åˆ™åŒ– (Lasso)**ï¼šåƒ"ä¸€åˆ€åˆ‡"ï¼Œç›´æ¥åˆ é™¤ä¸é‡è¦çš„ç‰¹å¾
  - æ¯”å–»ï¼šè€å¸ˆè¦æ±‚åˆ é™¤æ‰€æœ‰ä¸ç›¸å…³çš„çŸ¥è¯†ç‚¹
  - ç»“æœï¼šäº§ç”Ÿç¨€ç–è§£ï¼Œå¾ˆå¤šæƒé‡å˜ä¸º0
  
- **L2æ­£åˆ™åŒ– (Ridge)**ï¼šåƒ"æ¸©å’Œæé†’"ï¼Œè®©æ‰€æœ‰æƒé‡éƒ½å˜å°
  - æ¯”å–»ï¼šè€å¸ˆè¦æ±‚æ‰€æœ‰çŸ¥è¯†ç‚¹éƒ½è¦å­¦ï¼Œä½†ä¸è¦é’»ç‰›è§’å°–
  - ç»“æœï¼šæ‰€æœ‰æƒé‡éƒ½ä¿ç•™ï¼Œä½†æ•°å€¼å˜å°

#### Lasso vs Ridge = "ç‰¹å¾é€‰æ‹© vs æƒé‡æ”¶ç¼©"
- **Lasso**ï¼šè‡ªåŠ¨è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œåˆ é™¤æ— å…³ç‰¹å¾
- **Ridge**ï¼šä¿ç•™æ‰€æœ‰ç‰¹å¾ï¼Œä½†å‡å°‘è¿‡æ‹Ÿåˆ

### ğŸ¤ ç›´æ¥é¢è¯•å›ç­” (Direct Interview Answer)

**Regularization prevents overfitting by adding penalty terms to the loss function. L1 regularization (Lasso) uses absolute values of weights, creating sparse solutions that perform automatic feature selection. L2 regularization (Ridge) uses squared weights, shrinking all parameters toward zero while keeping all features.**

**The mathematical difference is:** L1 penalty = Î»âˆ‘|w_i| vs L2 penalty = Î»âˆ‘w_iÂ². L1's absolute value creates sharp corners at zero, forcing some weights to exactly zero, while L2's smooth penalty curve keeps all weights small but non-zero.

**Lasso corresponds to Laplace prior distribution** which has sharp peaks at zero, encouraging sparsity. **Ridge corresponds to Gaussian prior** which is smooth and centered at zero, encouraging small but non-zero values.

**L1 is more sparse than L2 because** the L1 penalty function has sharp corners at zero where gradients are discontinuous, making it easier for optimization algorithms to push weights to exactly zero. L2's smooth penalty makes it harder to reach exact zero.

**We use L1/L2 instead of L3/L4 because** higher-order penalties (L3, L4) create computational challenges - non-differentiable at zero and harder to optimize. L1/L2 provide the right balance of sparsity (L1) and smoothness (L2) for most practical applications.

---

### ğŸ“ è‹±æ–‡æ ‡å‡†é¢è¯•ç­”æ¡ˆ (English Interview Answer)

#### 1. L1 vs L2 Regularization Comparison

**L1 Regularization (Lasso):**
```python
# Loss function with L1 penalty
Loss = MSE + Î»âˆ‘|w_i|
```

**L2 Regularization (Ridge):**
```python
# Loss function with L2 penalty  
Loss = MSE + Î»âˆ‘w_iÂ²
```

**Key Differences:**

| Aspect | L1 (Lasso) | L2 (Ridge) |
|--------|------------|------------|
| **Penalty Function** | |w_i| | w_iÂ² |
| **Sparsity** | Creates sparse solutions | No sparsity |
| **Feature Selection** | Automatic | Manual required |
| **Computational** | Non-differentiable at 0 | Everywhere differentiable |
| **Prior Distribution** | Laplace | Gaussian |
| **Geometric Shape** | Diamond (L1 ball) | Circle (L2 ball) |

#### 2. Lasso/Ridge Explanation and Priors

**Lasso (Least Absolute Shrinkage and Selection Operator):**

**Prior Distribution:** Laplace (Double Exponential)
```python
# Laplace prior: P(w) âˆ exp(-Î»|w|)
# Mean = 0, Variance = 2/Î»Â²
```

**Characteristics:**
- Sharp peak at zero
- Heavy tails
- Encourages sparsity
- Automatic feature selection

**Ridge Regression:**

**Prior Distribution:** Gaussian (Normal)
```python
# Gaussian prior: P(w) âˆ exp(-Î»wÂ²/2)
# Mean = 0, Variance = 1/Î»
```

**Characteristics:**
- Smooth, bell-shaped curve
- Light tails
- No sparsity
- Shrinks all coefficients

#### 3. Mathematical Derivation

**Lasso Derivation:**

**Step 1: Objective Function**
```
L(w) = (1/2n)||y - Xw||Â² + Î»||w||â‚
```

**Step 2: Subgradient**
```
âˆ‚L/âˆ‚w_j = -(1/n)X_j^T(y - Xw) + Î» sign(w_j)
```

**Step 3: Soft Thresholding Solution**
```python
def lasso_coordinate_descent(X, y, lambda_reg, max_iter=1000):
    """
    Lasso coordinate descent algorithm
    """
    n, p = X.shape
    w = np.zeros(p)
    
    for iteration in range(max_iter):
        for j in range(p):
            # Calculate partial residual
            r = y - X @ w + w[j] * X[:, j]
            
            # Calculate coefficient
            rho_j = X[:, j].T @ r / n
            
            # Soft thresholding
            if rho_j > lambda_reg:
                w[j] = rho_j - lambda_reg
            elif rho_j < -lambda_reg:
                w[j] = rho_j + lambda_reg
            else:
                w[j] = 0
                
    return w
```

**Ridge Derivation:**

**Step 1: Objective Function**
```
L(w) = (1/2n)||y - Xw||Â² + Î»||w||â‚‚Â²
```

**Step 2: Gradient**
```
âˆ‚L/âˆ‚w = -(1/n)X^T(y - Xw) + Î»w
```

**Step 3: Closed-form Solution**
```python
def ridge_closed_form(X, y, lambda_reg):
    """
    Ridge regression closed-form solution
    """
    n, p = X.shape
    # Add regularization term to diagonal
    XTX = X.T @ X + lambda_reg * np.eye(p)
    XTy = X.T @ y
    
    # Solve: (X^T X + Î»I)w = X^T y
    w = np.linalg.solve(XTX, XTy)
    return w
```

#### 4. Why L1 is More Sparse Than L2

**Mathematical Explanation:**

**L1 Penalty Function:**
```
f(w) = |w|
f'(w) = sign(w) = {1 if w > 0, -1 if w < 0, undefined at w = 0}
```

**L2 Penalty Function:**
```
f(w) = wÂ²
f'(w) = 2w
```

**Key Insight:**
- L1 has **discontinuous gradient** at zero
- L2 has **continuous gradient** everywhere
- Optimization algorithms can "jump" to zero with L1
- L2 gradient approaches zero smoothly but rarely reaches exactly zero

**Geometric Visualization:**
```python
import numpy as np
import matplotlib.pyplot as plt

# Create visualization
w = np.linspace(-3, 3, 1000)

# L1 and L2 penalty functions
l1_penalty = np.abs(w)
l2_penalty = w**2

# Gradients
l1_grad = np.sign(w)
l2_grad = 2 * w

# Plot
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Penalty functions
ax1.plot(w, l1_penalty, 'b-', label='L1: |w|', linewidth=2)
ax1.plot(w, l2_penalty, 'r-', label='L2: wÂ²', linewidth=2)
ax1.set_title('Penalty Functions')
ax1.legend()
ax1.grid(True)

# Gradients
ax2.plot(w, l1_grad, 'b-', label='L1 gradient: sign(w)', linewidth=2)
ax2.plot(w, l2_grad, 'r-', label='L2 gradient: 2w', linewidth=2)
ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)
ax2.axvline(x=0, color='k', linestyle='--', alpha=0.5)
ax2.set_title('Gradient Functions')
ax2.legend()
ax2.grid(True)
```

#### 5. Why Regularization Works

**Statistical Perspective:**

**Bias-Variance Tradeoff:**
```
Total Error = BiasÂ² + Variance + Irreducible Error
```

**Without Regularization:**
- Low bias, high variance (overfitting)
- Model memorizes training data

**With Regularization:**
- Slightly higher bias, much lower variance
- Better generalization to new data

**Bayesian Interpretation:**

**Maximum Likelihood (No Regularization):**
```
w_ML = argmax P(y|X, w)
```

**Maximum A Posteriori (With Regularization):**
```
w_MAP = argmax P(y|X, w) Ã— P(w)
```

**Regularization term P(w) acts as prior knowledge:**
- L1: "Most weights should be zero"
- L2: "All weights should be small"

#### 6. Why L1/L2 Instead of L3/L4

**Computational Challenges:**

**L3 Penalty:**
```
f(w) = |w|Â³
f'(w) = 3wÂ² Ã— sign(w)  # Discontinuous at zero
```

**L4 Penalty:**
```
f(w) = wâ´
f'(w) = 4wÂ³  # Continuous but very steep
```

**Problems with Higher Orders:**

1. **Computational Complexity:**
   - L3: Non-differentiable at zero
   - L4: Very steep gradients, optimization difficulties

2. **Optimization Issues:**
   - L3: Coordinate descent fails at zero
   - L4: Gradient descent becomes unstable

3. **Practical Benefits Diminish:**
   - L1: Perfect sparsity
   - L2: Good shrinkage
   - L3/L4: No additional practical benefits

**Mathematical Proof:**
```python
def compare_penalties():
    """Compare different penalty functions"""
    w = np.linspace(-2, 2, 1000)
    
    penalties = {
        'L1': np.abs(w),
        'L2': w**2,
        'L3': np.abs(w)**3,
        'L4': w**4
    }
    
    gradients = {
        'L1': np.sign(w),
        'L2': 2 * w,
        'L3': 3 * w**2 * np.sign(w),
        'L4': 4 * w**3
    }
    
    # L1 and L2 are optimal for most applications
    # L3/L4 provide diminishing returns with increased complexity
```

### ğŸ’» å®é™…ä»£ç ç¤ºä¾‹

#### Complete Regularization Implementation
```python
import numpy as np
from sklearn.linear_model import Lasso, Ridge
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

class RegularizedRegression:
    def __init__(self, alpha=1.0, regularization='l2'):
        self.alpha = alpha
        self.regularization = regularization
        self.weights = None
        self.scaler = StandardScaler()
    
    def fit(self, X, y):
        """Fit regularized regression model"""
        # Standardize features
        X_scaled = self.scaler.fit_transform(X)
        
        if self.regularization == 'l1':
            self.weights = self._lasso_fit(X_scaled, y)
        elif self.regularization == 'l2':
            self.weights = self._ridge_fit(X_scaled, y)
        else:
            raise ValueError("Regularization must be 'l1' or 'l2'")
    
    def _lasso_fit(self, X, y, max_iter=1000, tol=1e-4):
        """Lasso using coordinate descent"""
        n, p = X.shape
        w = np.zeros(p)
        
        for iteration in range(max_iter):
            w_old = w.copy()
            
            for j in range(p):
                # Calculate partial residual
                r = y - X @ w + w[j] * X[:, j]
                
                # Calculate coefficient
                rho_j = X[:, j].T @ r / n
                
                # Soft thresholding
                if rho_j > self.alpha:
                    w[j] = rho_j - self.alpha
                elif rho_j < -self.alpha:
                    w[j] = rho_j + self.alpha
                else:
                    w[j] = 0
            
            # Check convergence
            if np.max(np.abs(w - w_old)) < tol:
                break
                
        return w
    
    def _ridge_fit(self, X, y):
        """Ridge regression closed-form solution"""
        n, p = X.shape
        # Add regularization term to diagonal
        XTX = X.T @ X + self.alpha * np.eye(p)
        XTy = X.T @ y
        
        # Solve: (X^T X + Î»I)w = X^T y
        w = np.linalg.solve(XTX, XTy)
        return w
    
    def predict(self, X):
        """Make predictions"""
        X_scaled = self.scaler.transform(X)
        return X_scaled @ self.weights

# Example usage
def regularization_comparison_example():
    """Compare L1 and L2 regularization"""
    # Generate synthetic data
    np.random.seed(42)
    n, p = 100, 20
    X = np.random.randn(n, p)
    
    # Create sparse true weights (only first 5 are non-zero)
    true_weights = np.zeros(p)
    true_weights[:5] = [2, -1.5, 1, -0.5, 0.8]
    
    # Generate target with noise
    y = X @ true_weights + 0.1 * np.random.randn(n)
    
    # Fit models
    lasso_model = RegularizedRegression(alpha=0.1, regularization='l1')
    ridge_model = RegularizedRegression(alpha=0.1, regularization='l2')
    
    lasso_model.fit(X, y)
    ridge_model.fit(X, y)
    
    # Compare results
    print("True weights (first 10):", true_weights[:10])
    print("Lasso weights (first 10):", lasso_model.weights[:10])
    print("Ridge weights (first 10):", ridge_model.weights[:10])
    
    # Count non-zero weights
    lasso_sparsity = np.sum(np.abs(lasso_model.weights) > 1e-6)
    ridge_sparsity = np.sum(np.abs(ridge_model.weights) > 1e-6)
    
    print(f"Lasso non-zero weights: {lasso_sparsity}/{p}")
    print(f"Ridge non-zero weights: {ridge_sparsity}/{p}")

if __name__ == "__main__":
    regularization_comparison_example()
```

### ğŸ” é¢è¯•å¸¸è§é—®é¢˜åŠå›ç­”

#### Q1: "What's the difference between L1 and L2 regularization?"

**English Answer:**
L1 regularization uses absolute values of weights (Î»âˆ‘|w_i|) and creates sparse solutions by setting some weights to exactly zero, performing automatic feature selection. L2 regularization uses squared weights (Î»âˆ‘w_iÂ²) and shrinks all weights toward zero while keeping them non-zero, preventing overfitting without feature selection.

#### Q2: "Why does L1 create sparse solutions while L2 doesn't?"

**English Answer:**
L1's penalty function |w| has a sharp corner at zero where the gradient is discontinuous. During optimization, when a weight approaches zero, the L1 penalty can "push" it to exactly zero. L2's penalty wÂ² is smooth everywhere with gradient 2w, making it difficult to reach exactly zero during optimization.

#### Q3: "What are the prior distributions for Lasso and Ridge?"

**English Answer:**
Lasso corresponds to a Laplace (double exponential) prior: P(w) âˆ exp(-Î»|w|), which has sharp peaks at zero encouraging sparsity. Ridge corresponds to a Gaussian prior: P(w) âˆ exp(-Î»wÂ²/2), which is smooth and centered at zero, encouraging small but non-zero weights.

#### Q4: "How do you choose the regularization parameter Î»?"

**English Answer:**
I use cross-validation to find the optimal Î». For Lasso, I often use L1 path algorithms to efficiently compute solutions for multiple Î» values. I also consider the bias-variance tradeoff - larger Î» reduces variance but increases bias. The optimal Î» balances this tradeoff based on validation performance.

### ğŸ’¡ å®æˆ˜æŠ€å·§

#### 1. å›ç­”ç»“æ„ (Answer Structure)
1. **å®šä¹‰å’ŒåŒºåˆ«** (Definitions and Differences): L1 vs L2çš„åŸºæœ¬æ¦‚å¿µ
2. **æ•°å­¦åŸç†** (Mathematical Principles): æƒ©ç½šå‡½æ•°å’Œæ¢¯åº¦
3. **å…ˆéªŒåˆ†å¸ƒ** (Prior Distributions): è´å¶æ–¯è§£é‡Š
4. **ç¨€ç–æ€§åŸç†** (Sparsity Principles): ä¸ºä»€ä¹ˆL1æ›´ç¨€ç–
5. **å®é™…åº”ç”¨** (Practical Applications): ä½•æ—¶ä½¿ç”¨å“ªç§æ­£åˆ™åŒ–

#### 2. å…³é”®è¯ (Key Terms)
- **L1 Regularization**: L1æ­£åˆ™åŒ–
- **L2 Regularization**: L2æ­£åˆ™åŒ–
- **Sparsity**: ç¨€ç–æ€§
- **Feature Selection**: ç‰¹å¾é€‰æ‹©
- **Prior Distribution**: å…ˆéªŒåˆ†å¸ƒ
- **Soft Thresholding**: è½¯é˜ˆå€¼
- **Coordinate Descent**: åæ ‡ä¸‹é™

#### 3. å¸¸è§é™·é˜± (Common Pitfalls)
- âŒ æ··æ·†L1å’ŒL2çš„å‡ ä½•å½¢çŠ¶
- âŒ ä¸ç†è§£ä¸ºä»€ä¹ˆL1äº§ç”Ÿç¨€ç–è§£
- âŒ å¿½ç•¥æ­£åˆ™åŒ–å‚æ•°çš„è°ƒä¼˜
- âŒ ä¸è€ƒè™‘æ•°æ®çš„æ ‡å‡†åŒ–
- âŒ ä¸ç†è§£è´å¶æ–¯è§£é‡Š

### ğŸ“Š å¯è§†åŒ–ç†è§£

#### L1 vs L2 æ­£åˆ™åŒ–å¯¹æ¯”
![L1 vs L2 æ­£åˆ™åŒ–å¯¹æ¯”](../../images/basic_ml/l1_l2_regularization_comparison.png)

#### ç¨€ç–æ€§å¯è§†åŒ–
![ç¨€ç–æ€§å¯è§†åŒ–](../../images/basic_ml/sparsity_visualization.png)

#### å…ˆéªŒåˆ†å¸ƒå¯¹æ¯”
![å…ˆéªŒåˆ†å¸ƒå¯¹æ¯”](../../images/basic_ml/prior_distributions.png)

#### æ­£åˆ™åŒ–è·¯å¾„
![æ­£åˆ™åŒ–è·¯å¾„](../../images/basic_ml/regularization_paths.png)

### ğŸ“Š é¢è¯•å‡†å¤‡æ£€æŸ¥æ¸…å•

- [ ] ç†è§£L1å’ŒL2æ­£åˆ™åŒ–çš„æ•°å­¦å®šä¹‰
- [ ] æŒæ¡Lassoå’ŒRidgeçš„æ¨å¯¼è¿‡ç¨‹
- [ ] ç†è§£ä¸ºä»€ä¹ˆL1æ¯”L2ç¨€ç–
- [ ] çŸ¥é“Lassoå’ŒRidgeçš„å…ˆéªŒåˆ†å¸ƒ
- [ ] èƒ½å¤Ÿè§£é‡Šæ­£åˆ™åŒ–ä¸ºä»€ä¹ˆæœ‰æ•ˆ
- [ ] ç†è§£ä¸ºä»€ä¹ˆä½¿ç”¨L1/L2è€Œä¸æ˜¯L3/L4
- [ ] æŒæ¡æ­£åˆ™åŒ–å‚æ•°çš„é€‰æ‹©æ–¹æ³•
- [ ] èƒ½å¤Ÿå®ç°ç®€å•çš„æ­£åˆ™åŒ–ç®—æ³•
- [ ] ç†è§£è´å¶æ–¯è§£é‡Š
- [ ] çŸ¥é“å®é™…åº”ç”¨ä¸­çš„æ³¨æ„äº‹é¡¹

### ğŸ¯ ç»ƒä¹ å»ºè®®

1. **ç†è®ºç»ƒä¹ **: æ¨å¯¼Lassoå’ŒRidgeçš„æ•°å­¦å…¬å¼
2. **ä»£ç ç»ƒä¹ **: å®ç°åæ ‡ä¸‹é™ç®—æ³•
3. **å¯è§†åŒ–ç»ƒä¹ **: ç»˜åˆ¶ä¸åŒæ­£åˆ™åŒ–å‚æ•°çš„è·¯å¾„
4. **åº”ç”¨ç»ƒä¹ **: åœ¨çœŸå®æ•°æ®ä¸Šæ¯”è¾ƒL1å’ŒL2æ•ˆæœ
5. **å‚æ•°è°ƒä¼˜**: ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ‹©æœ€ä¼˜Î»

**è®°ä½**: æ­£åˆ™åŒ–æ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆçš„é‡è¦æŠ€æœ¯ï¼ŒL1ç”¨äºç‰¹å¾é€‰æ‹©ï¼ŒL2ç”¨äºæƒé‡æ”¶ç¼©ï¼Œé€‰æ‹©å“ªç§å–å†³äºå…·ä½“é—®é¢˜éœ€æ±‚ï¼
