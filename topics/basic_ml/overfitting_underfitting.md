# æœºå™¨å­¦ä¹ åŸºç¡€é—®é¢˜ - è¯¦ç»†ç­”æ¡ˆ

## é—®é¢˜: ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆ(Overfitting)å’Œæ¬ æ‹Ÿåˆ(Underfitting)ï¼Ÿ

### ä¸­æ–‡ç†è§£ (ä¾¿äºè®°å¿†)

#### è¿‡æ‹Ÿåˆ = "æ­»è®°ç¡¬èƒŒ"
- **ç°è±¡**ï¼šæ¨¡å‹æŠŠè®­ç»ƒæ•°æ®"èƒŒ"å¾—å¤ªç†Ÿï¼ŒåŒ…æ‹¬é”™è¯¯å’Œå™ªå£°
- **ç»“æœ**ï¼šé‡åˆ°æ–°é¢˜ç›®(æµ‹è¯•æ•°æ®)å°±ä¸ä¼šåšäº†
- **æ¯”å–»**ï¼šåƒå­¦ç”ŸèƒŒç­”æ¡ˆè€Œä¸æ˜¯ç†è§£åŸç†

#### æ¬ æ‹Ÿåˆ = "å­¦ä¹ ä¸å¤Ÿ"  
- **ç°è±¡**ï¼šæ¨¡å‹å­¦å¾—å¤ªæµ…ï¼Œæ²¡æœ‰æŒæ¡æ•°æ®ä¸­çš„è§„å¾‹
- **ç»“æœ**ï¼šæ— è®ºè®­ç»ƒè¿˜æ˜¯æµ‹è¯•éƒ½åšä¸å¥½
- **æ¯”å–»**ï¼šåƒåªå­¦äº†çš®æ¯›ï¼Œæ²¡æœ‰æ·±å…¥ç†è§£

### è‹±æ–‡æ ‡å‡†é¢è¯•ç­”æ¡ˆ

#### 1. åŸºç¡€å®šä¹‰ (Basic Definitions)

**Overfitting** occurs when a model learns the training data too well, including its noise and outliers, resulting in poor generalization to unseen data.

**Underfitting** occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test sets.

#### 2. æ•°å­¦è¡¨ç¤º (Mathematical Representation)

```
Total Error = BiasÂ² + Variance + Irreducible Error

Where:
- Bias: Error from oversimplified assumptions
- Variance: Error from sensitivity to small fluctuations  
- Irreducible Error: Noise inherent in the data
```

#### 3. æ£€æµ‹æ–¹æ³• (Detection Methods)

**Overfitting Indicators:**
- Training accuracy >> Test accuracy
- Large gap between training and validation loss
- Model performs well on training data but poorly on new data

**Underfitting Indicators:**
- Training accuracy â‰ˆ Test accuracy (both low)
- Small gap between training and validation loss
- Model fails to learn patterns in training data

#### 4. è§£å†³æ–¹æ¡ˆ (Solutions)

**For Overfitting:**
1. **Regularization**: Add penalty terms to prevent overfitting
2. **Cross-validation**: Better estimate of model performance
3. **Early Stopping**: Stop training when validation loss increases
4. **Dropout**: Randomly disable neurons during training
5. **Data Augmentation**: Increase effective training data size

**For Underfitting:**
1. **Increase Model Complexity**: Add more layers/parameters
2. **Feature Engineering**: Create more informative features
3. **Reduce Regularization**: Lower penalty strength
4. **Longer Training**: Train for more epochs
5. **Ensemble Methods**: Combine multiple models

### é¢è¯•å¸¸è§é—®é¢˜åŠå›ç­”

#### Q1: "How do you detect overfitting in practice?"

**Answer:**
```python
# Monitor training vs validation metrics
if train_accuracy > val_accuracy + threshold:
    print("Overfitting detected")

# Use learning curves
def plot_learning_curves(model, X, y):
    train_sizes = np.linspace(0.1, 1.0, 10)
    train_scores, val_scores = [], []
    
    for size in train_sizes:
        # Train on subset
        X_subset = X[:int(size * len(X))]
        y_subset = y[:int(size * len(y))]
        
        model.fit(X_subset, y_subset)
        train_scores.append(model.score(X_subset, y_subset))
        val_scores.append(model.score(X_val, y_val))
    
    # Plot curves - large gap indicates overfitting
```

#### Q2: "What's the difference between L1 and L2 regularization?"

**Answer:**
- **L1 (Lasso)**: 
  - Penalty: `Î» * Î£|w|`
  - Effect: Feature selection, sparse solutions
  - Use case: When you want to remove irrelevant features

- **L2 (Ridge)**:
  - Penalty: `Î» * Î£wÂ²`  
  - Effect: Shrinks weights, prevents overfitting
  - Use case: When you want to keep all features but reduce overfitting

- **Elastic Net**: Combines L1 + L2 for both benefits

#### Q3: "How do you choose the right model complexity?"

**Answer:**
1. **Cross-validation**: Use k-fold CV to estimate performance
2. **Validation curves**: Plot complexity vs performance
3. **Occam's Razor**: Choose simplest model that performs well
4. **Domain knowledge**: Consider interpretability requirements
5. **Computational constraints**: Balance performance vs efficiency

#### Q4: "Explain the bias-variance tradeoff with examples"

**Answer:**
```python
# High Bias (Underfitting) Example
# Simple linear model on complex data
linear_model = LinearRegression()
# Result: High bias, low variance

# High Variance (Overfitting) Example  
# Complex polynomial model on simple data
poly_model = PolynomialFeatures(degree=15)
# Result: Low bias, high variance

# Optimal Model
# Right complexity for the data
optimal_model = RandomForestClassifier(n_estimators=100)
# Result: Balanced bias and variance
```

### å®æˆ˜æŠ€å·§

#### 1. å›ç­”ç»“æ„ (Answer Structure)
1. **å®šä¹‰** (Definition): ç®€æ´æ˜ç¡®å®šä¹‰æ¦‚å¿µ
2. **åŸå› ** (Causes): è§£é‡Šä¸ºä»€ä¹ˆä¼šå‘ç”Ÿ
3. **æ£€æµ‹** (Detection): å¦‚ä½•è¯†åˆ«é—®é¢˜
4. **è§£å†³** (Solutions): å…·ä½“çš„è§£å†³æ–¹æ³•
5. **ä¾‹å­** (Examples): æä¾›å®é™…æ¡ˆä¾‹

#### 2. å…³é”®è¯ (Key Terms)
- **Generalization**: æ³›åŒ–èƒ½åŠ›
- **Cross-validation**: äº¤å‰éªŒè¯
- **Regularization**: æ­£åˆ™åŒ–
- **Bias-variance tradeoff**: åå·®-æ–¹å·®æƒè¡¡
- **Learning curves**: å­¦ä¹ æ›²çº¿
- **Early stopping**: æ—©åœæ³•

#### 3. å¸¸è§é™·é˜± (Common Pitfalls)
- âŒ åªè°ˆç†è®ºï¼Œæ²¡æœ‰å®é™…ä¾‹å­
- âŒ æ··æ·†è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆçš„è§£å†³æ–¹æ¡ˆ
- âŒ å¿½ç•¥åå·®-æ–¹å·®æƒè¡¡
- âŒ æ²¡æœ‰æåˆ°äº¤å‰éªŒè¯çš„é‡è¦æ€§

### ğŸ“Š å¯è§†åŒ–ç†è§£

#### è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆåˆ†æå›¾
![è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆåˆ†æ](../../images/basic_ml/overfitting_underfitting_analysis.png)

#### å­¦ä¹ æ›²çº¿æ¨¡å¼
```
æ¬ æ‹Ÿåˆ: è®­ç»ƒè¯¯å·® â‰ˆ æµ‹è¯•è¯¯å·® (éƒ½é«˜)
è¿‡æ‹Ÿåˆ: è®­ç»ƒè¯¯å·® << æµ‹è¯•è¯¯å·®  
ç†æƒ³: è®­ç»ƒè¯¯å·® â‰ˆ æµ‹è¯•è¯¯å·® (éƒ½ä½)
```

#### æ¨¡å‹å¤æ‚åº¦ vs æ€§èƒ½
```
ä½å¤æ‚åº¦ â†’ é«˜åå·®ï¼Œä½æ–¹å·® (æ¬ æ‹Ÿåˆ)
é«˜å¤æ‚åº¦ â†’ ä½åå·®ï¼Œé«˜æ–¹å·® (è¿‡æ‹Ÿåˆ)
æœ€ä¼˜å¤æ‚åº¦ â†’ å¹³è¡¡åå·®å’Œæ–¹å·®
```

### é¢è¯•å‡†å¤‡æ£€æŸ¥æ¸…å•

- [ ] èƒ½æ¸…æ™°å®šä¹‰è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ
- [ ] ç†è§£åå·®-æ–¹å·®æƒè¡¡
- [ ] çŸ¥é“å¦‚ä½•æ£€æµ‹è¿™ä¸¤ç§é—®é¢˜
- [ ] æŒæ¡å„ç§è§£å†³æ–¹æ¡ˆ
- [ ] èƒ½æä¾›å®é™…ä»£ç ä¾‹å­
- [ ] ç†è§£æ­£åˆ™åŒ–çš„ä½œç”¨æœºåˆ¶
- [ ] çŸ¥é“äº¤å‰éªŒè¯çš„é‡è¦æ€§
- [ ] èƒ½è§£é‡Šå­¦ä¹ æ›²çº¿çš„å«ä¹‰

### ç»ƒä¹ å»ºè®®

1. **ç†è®ºç»ƒä¹ **: ç”¨è‡ªå·±çš„è¯è§£é‡Šæ¦‚å¿µ
2. **ä»£ç ç»ƒä¹ **: å®ç°æ£€æµ‹å’Œè§£å†³æ–¹æ¡ˆ
3. **æ¡ˆä¾‹åˆ†æ**: åˆ†æçœŸå®æ•°æ®é›†çš„é—®é¢˜
4. **å¯è§†åŒ–**: ç»˜åˆ¶å­¦ä¹ æ›²çº¿å’Œåå·®-æ–¹å·®å›¾
5. **æ¨¡æ‹Ÿé¢è¯•**: ç»ƒä¹ å›ç­”å¸¸è§é—®é¢˜

è®°ä½ï¼šé¢è¯•å®˜æ›´çœ‹é‡ä½ çš„ç†è§£æ·±åº¦å’Œå®é™…åº”ç”¨èƒ½åŠ›ï¼Œè€Œä¸æ˜¯æ­»è®°ç¡¬èƒŒï¼
